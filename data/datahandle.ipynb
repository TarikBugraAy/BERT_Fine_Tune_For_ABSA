{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 0: 1018\n",
      "Total -1: 44824\n",
      "Total 1: 864\n",
      "Total 2: 1224\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "input_csv = 'C:/Users/emink/OneDrive/Masa端st端/BERT_Fine_Tune_For_ABSA/data/ABSA4restaurants_train.csv'\n",
    "\n",
    "with open(input_csv, 'r', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    \n",
    "    count_net0 = 0\n",
    "    count_neg1 = 0\n",
    "    count_pos1 = 0\n",
    "    count_pos2 = 0\n",
    "    \n",
    "    for row in reader:\n",
    "        # Convert the string representation of a list into an actual Python list\n",
    "        polarities_list = ast.literal_eval(row['Polarities'])\n",
    "        \n",
    "        # Count occurrences in this row, then add to running totals\n",
    "        count_net0 += polarities_list.count(-0)\n",
    "        count_neg1 += polarities_list.count(-1)\n",
    "        count_pos1 += polarities_list.count(1)\n",
    "        count_pos2 += polarities_list.count(2)\n",
    "\n",
    "print(f\"Total 0: {count_net0}\")\n",
    "print(f\"Total -1: {count_neg1}\")\n",
    "print(f\"Total 1: {count_pos1}\")\n",
    "print(f\"Total 2: {count_pos2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Test\n",
    "+ Total 0: 248\n",
    "+ Total -1: 19636\n",
    "+ Total 1: 310\n",
    "+ Total 2: 1065"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ABSA5\n",
    "+ Total 0: 882\n",
    "+ Total -1: 38477\n",
    "+ Total 1: 864\n",
    "+ Total 2: 861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('C:/Users/emink/OneDrive/Masa端st端/BERT_Fine_Tune_For_ABSA/data/restaurants_test.csv')\n",
    "\n",
    "# Convert the 'Polarities' column from string representation of a list to an actual Python list\n",
    "df['Polarities'] = df['Polarities'].apply(ast.literal_eval)\n",
    "\n",
    "# Filter rows that contain a '2' in the polarity list\n",
    "df_with_2 = df[df['Polarities'].apply(lambda x: 2 in x)]\n",
    "\n",
    "# Randomly select 1000 rows to delete from those with polarity 2 \n",
    "# (make sure there are at least 1000 rows)\n",
    "rows_to_delete = df_with_2.sample(n=500, random_state=42)\n",
    "\n",
    "# Drop these rows from the dataframe\n",
    "df.drop(rows_to_delete.index, inplace=True)\n",
    "\n",
    "# # Filter rows that contain a '2' in the polarity list\n",
    "# df_with_0 = df[df['Polarities'].apply(lambda x: 0 in x)]\n",
    "\n",
    "# # Randomly select 1000 rows to delete from those with polarity 2 \n",
    "# # (make sure there are at least 1000 rows)\n",
    "# rows_to_delete = df_with_0.sample(n=100, random_state=42)\n",
    "# df.drop(rows_to_delete.index, inplace=True)\n",
    "\n",
    "# Save the updated dataframe\n",
    "df.to_csv('V1restaurants_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Tokens        Tags Polarities\n",
      "0  All the appetizers and salads were fabulous, t...  appetizers   positive\n",
      "1  All the appetizers and salads were fabulous, t...      salads   positive\n",
      "2  All the appetizers and salads were fabulous, t...       steak   positive\n",
      "3  All the appetizers and salads were fabulous, t...       pasta   positive\n",
      "4                         And really large portions.    portions   positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"restaurants-trial.csv\"  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_remove = ['id', 'from', 'to']\n",
    "df = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "# Ensure columns are named properly\n",
    "df.columns = ['Tokens', 'Tags', 'Polarities']\n",
    "\n",
    "# Check the first few rows\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df.to_csv('testing_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure columns are stringified for processing\n",
    "df['Tokens'] = df['Tokens'].apply(lambda x: str(x))\n",
    "df['Tags'] = df['Tags'].apply(lambda x: str(x))\n",
    "df['Polarities'] = df['Polarities'].apply(lambda x: str(x))\n",
    "\n",
    "# Save processed data to a new file (optional)\n",
    "df.to_csv(\"Processed_Restaurants_Train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_csv = 'Processed_Restaurants_Train.csv'\n",
    "output_csv = 'output.csv'\n",
    "\n",
    "with open(input_csv, 'r', encoding='utf-8', newline='') as infile, \\\n",
    "     open(output_csv, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    \n",
    "    reader = csv.DictReader(infile)\n",
    "    \n",
    "    # Write the header line exactly as is\n",
    "    outfile.write(','.join(reader.fieldnames) + '\\n')\n",
    "    \n",
    "    for row in reader:\n",
    "        tokens = row['Tokens'].strip()\n",
    "        \n",
    "        # Strip *all* leading/trailing quotes. \n",
    "        # (Loop until there's no symmetrical pair of quotes left.)\n",
    "        while tokens.startswith('\"') and tokens.endswith('\"') and len(tokens) > 1:\n",
    "            tokens = tokens[1:-1].strip()\n",
    "        \n",
    "        # Now wrap once in quotes\n",
    "        tokens = f'\"{tokens}\"'\n",
    "        \n",
    "        # Build the CSV line manually for 3 columns: Tokens,Tags,Polarities\n",
    "        # Make sure we keep the exact order of columns the same as the header\n",
    "        out_line = f'{tokens},{row[\"Tags\"]},{row[\"Polarities\"]}\\n'\n",
    "        \n",
    "        outfile.write(out_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\emink\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[22], line 37\u001b[0m\n    df['Tokens'], df['Tags'], df['Polarities'] = zip(*df.apply(tokenize_data, axis=1))\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\emink\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m in \u001b[0;35mapply\u001b[0m\n    return op.apply().__finalize__(self, method=\"apply\")\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\emink\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m in \u001b[0;35mapply\u001b[0m\n    return self.apply_standard()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\emink\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m in \u001b[0;35mapply_standard\u001b[0m\n    results, res_index = self.apply_series_generator()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\emink\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m in \u001b[0;35mapply_series_generator\u001b[0m\n    results[i] = self.func(v, *self.args, **self.kwargs)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 20\u001b[1;36m in \u001b[1;35mtokenize_data\u001b[1;36m\n\u001b[1;33m    tokens = eval(row['Tokens'])  # Convert string representation to list\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    But the staff was so horrible to us.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"output.csv\"  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_remove = ['id', 'from', 'to']\n",
    "df = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "# Ensure the correct column names\n",
    "df.columns = ['Tokens', 'Tags', 'Polarities']\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# Tokenize and save\n",
    "def tokenize_data(row):\n",
    "    tokens = eval(row['Tokens'])  # Convert string representation to list\n",
    "    tags = eval(row['Tags'])\n",
    "    polarities = eval(row['Polarities'])\n",
    "    \n",
    "    tokenized_tokens = []\n",
    "    tokenized_tags = []\n",
    "    tokenized_polarities = []\n",
    "\n",
    "    for token, tag, polarity in zip(tokens, tags, polarities):\n",
    "        subwords = tokenizer.tokenize(token)\n",
    "        tokenized_tokens.extend(subwords)\n",
    "        tokenized_tags.extend([tag] * len(subwords))\n",
    "        tokenized_polarities.extend([polarity] * len(subwords))\n",
    "\n",
    "    return tokenized_tokens, tokenized_tags, tokenized_polarities\n",
    "\n",
    "# Apply tokenization\n",
    "df['Tokens'], df['Tags'], df['Polarities'] = zip(*df.apply(tokenize_data, axis=1))\n",
    "\n",
    "# Save the processed data to a new CSV\n",
    "output_path = \"Processed_Restaurants_Train_Tokenized.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Tokenized data saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
