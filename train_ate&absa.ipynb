{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory (notebooks do not have __file__)\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "sys.path.append(data_path)\n",
    "\n",
    "\n",
    "#Use this one if you are running the code from a script not a notebook\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # Add the 'data' directory to the Python path\n",
    "# data_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')\n",
    "# sys.path.append(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.bert import bert_ATE, bert_ABSA\n",
    "\n",
    "from dataset import dataset_ATM, dataset_ABSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#pretrain_model_name = \"bert-base-uncased\"      # I will try absa \n",
    "pretrain_model_name = \"bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)\n",
    "lr = 2e-5      #Learning rate\n",
    "model_ATE = bert_ATE(pretrain_model_name).to(DEVICE)\n",
    "optimizer_ATE = torch.optim.Adam(model_ATE.parameters(), lr=lr)\n",
    "#model_ABSA = bert_ABSA(pretrain_model_name).to(DEVICE)\n",
    "model_ABSA = bert_ABSA(pretrain_model_name, DEVICE).to(DEVICE)\n",
    "#optimizer_ABSA = torch.optim.Adam(model_ABSA.parameters(), lr=lr)\n",
    "optimizer_ABSA = AdamW(\n",
    "    model_ABSA.parameters(), \n",
    "    lr=lr, \n",
    "    weight_decay=1e-4  # or 1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evl_time(t):\n",
    "    min, sec= divmod(t, 60)\n",
    "    hr, min = divmod(min, 60)\n",
    "    return int(hr), int(min), int(sec)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path), strict=False)\n",
    "    return model\n",
    "    \n",
    "def save_model(model, name):\n",
    "    torch.save(model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acpect Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "restaurants_train_ds = dataset_ATM(pd.read_csv(\"data/restaurants_train.csv\"), tokenizer)\n",
    "restaurants_test_ds = dataset_ATM(pd.read_csv(\"data/restaurants_test.csv\"), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = ConcatDataset([restaurants_train_ds])\n",
    "# test_ds = ConcatDataset([restaurants_test_ds])\n",
    "\n",
    "train_ds =restaurants_train_ds\n",
    "test_ds = restaurants_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    ids_tensors = [s[1] for s in samples]\n",
    "    ids_tensors = pad_sequence(ids_tensors, batch_first=True)\n",
    "\n",
    "    tags_tensors = [s[2] for s in samples]\n",
    "    tags_tensors = pad_sequence(tags_tensors, batch_first=True)\n",
    "\n",
    "    pols_tensors = [s[3] for s in samples]\n",
    "    pols_tensors = pad_sequence(pols_tensors, batch_first=True)\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "    \n",
    "    return ids_tensors, tags_tensors, pols_tensors, masks_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=5, collate_fn=create_mini_batch, shuffle = True)\n",
    "test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     w,x,y,z = batch\n",
    "#     print(w)\n",
    "#     print(w.size())\n",
    "#     print(x)\n",
    "#     print(x.size())\n",
    "#     print(y)\n",
    "#     print(y.size())\n",
    "#     print(z)\n",
    "#     print(z.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ATE(loader, epochs):\n",
    "    all_data = len(loader)\n",
    "    for epoch in range(epochs):\n",
    "        finish_data = 0\n",
    "        losses = []\n",
    "        current_times = []\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for data in loader:\n",
    "            t0 = time.time()\n",
    "            ids_tensors, tags_tensors, _, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            tags_tensors = tags_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            loss = model_ATE(ids_tensors=ids_tensors, tags_tensors=tags_tensors, masks_tensors=masks_tensors)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer_ATE.step()\n",
    "            optimizer_ATE.zero_grad()\n",
    "\n",
    "            finish_data += 1\n",
    "            current_times.append(round(time.time()-t0,3))\n",
    "            current = np.mean(current_times)\n",
    "            hr, min, sec = evl_time(current*(all_data-finish_data) + current*all_data*(epochs-epoch-1))\n",
    "            print('epoch:', epoch, \" batch:\", finish_data, \"/\" , all_data, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min,\" sec:\", sec)         \n",
    "\n",
    "        save_model(model_ATE, 'bert_ATE2.pkl')\n",
    "        \n",
    "def test_model_ATE(loader):\n",
    "    pred = []\n",
    "    trueth = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            ids_tensors, tags_tensors, _, masks_tensors = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            tags_tensors = tags_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            outputs = model_ATE(ids_tensors=ids_tensors, tags_tensors=None, masks_tensors=masks_tensors)\n",
    "\n",
    "            _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "            pred += list([int(j) for i in predictions for j in i ])\n",
    "            trueth += list([int(j) for i in tags_tensors for j in i ])\n",
    "\n",
    "    return trueth, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  batch: 1 / 721  loss: 0.9688948392868042  hr: 0  min: 41  sec: 24\n",
      "epoch: 0  batch: 2 / 721  loss: 0.8510468900203705  hr: 0  min: 33  sec: 40\n",
      "epoch: 0  batch: 3 / 721  loss: 0.7461363077163696  hr: 0  min: 32  sec: 57\n",
      "epoch: 0  batch: 4 / 721  loss: 0.7034807950258255  hr: 0  min: 31  sec: 27\n",
      "epoch: 0  batch: 5 / 721  loss: 0.6527902364730835  hr: 0  min: 30  sec: 27\n",
      "epoch: 0  batch: 6 / 721  loss: 0.5977026174465815  hr: 0  min: 30  sec: 14\n",
      "epoch: 0  batch: 7 / 721  loss: 0.6084430856364114  hr: 0  min: 29  sec: 30\n",
      "epoch: 0  batch: 8 / 721  loss: 0.5584185644984245  hr: 0  min: 30  sec: 7\n",
      "epoch: 0  batch: 9 / 721  loss: 0.5282848808500502  hr: 0  min: 30  sec: 8\n",
      "epoch: 0  batch: 10 / 721  loss: 0.5164529800415039  hr: 0  min: 29  sec: 43\n",
      "epoch: 0  batch: 11 / 721  loss: 0.5134764422069896  hr: 0  min: 29  sec: 25\n",
      "epoch: 0  batch: 12 / 721  loss: 0.49250348657369614  hr: 0  min: 29  sec: 51\n",
      "epoch: 0  batch: 13 / 721  loss: 0.4958679423882411  hr: 0  min: 29  sec: 24\n",
      "epoch: 0  batch: 14 / 721  loss: 0.49136132427624296  hr: 0  min: 28  sec: 57\n",
      "epoch: 0  batch: 15 / 721  loss: 0.4780740181605021  hr: 0  min: 28  sec: 40\n",
      "epoch: 0  batch: 16 / 721  loss: 0.4747024793177843  hr: 0  min: 28  sec: 28\n",
      "epoch: 0  batch: 17 / 721  loss: 0.46011365511838126  hr: 0  min: 28  sec: 20\n",
      "epoch: 0  batch: 18 / 721  loss: 0.45394982066419387  hr: 0  min: 28  sec: 11\n",
      "epoch: 0  batch: 19 / 721  loss: 0.44785651721452413  hr: 0  min: 28  sec: 5\n",
      "epoch: 0  batch: 20 / 721  loss: 0.44126827269792557  hr: 0  min: 28  sec: 1\n",
      "epoch: 0  batch: 21 / 721  loss: 0.43523772841408137  hr: 0  min: 27  sec: 45\n",
      "epoch: 0  batch: 22 / 721  loss: 0.42820568789135327  hr: 0  min: 27  sec: 31\n",
      "epoch: 0  batch: 23 / 721  loss: 0.4222985402397487  hr: 0  min: 27  sec: 37\n",
      "epoch: 0  batch: 24 / 721  loss: 0.42252709592382115  hr: 0  min: 27  sec: 42\n",
      "epoch: 0  batch: 25 / 721  loss: 0.4246692252159119  hr: 0  min: 27  sec: 34\n",
      "epoch: 0  batch: 26 / 721  loss: 0.4237752350477072  hr: 0  min: 27  sec: 25\n",
      "epoch: 0  batch: 27 / 721  loss: 0.42179227206442094  hr: 0  min: 27  sec: 18\n",
      "epoch: 0  batch: 28 / 721  loss: 0.41692015528678894  hr: 0  min: 27  sec: 18\n",
      "epoch: 0  batch: 29 / 721  loss: 0.4120400835727823  hr: 0  min: 27  sec: 18\n",
      "epoch: 0  batch: 30 / 721  loss: 0.40963906546433765  hr: 0  min: 27  sec: 19\n",
      "epoch: 0  batch: 31 / 721  loss: 0.4158637321764423  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 32 / 721  loss: 0.41268899757415056  hr: 0  min: 27  sec: 30\n",
      "epoch: 0  batch: 33 / 721  loss: 0.40435189776348346  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 34 / 721  loss: 0.40235545661519556  hr: 0  min: 27  sec: 30\n",
      "epoch: 0  batch: 35 / 721  loss: 0.3973098248243332  hr: 0  min: 27  sec: 33\n",
      "epoch: 0  batch: 36 / 721  loss: 0.3933633760445648  hr: 0  min: 27  sec: 37\n",
      "epoch: 0  batch: 37 / 721  loss: 0.38725043672162135  hr: 0  min: 27  sec: 50\n",
      "epoch: 0  batch: 38 / 721  loss: 0.3848866079198687  hr: 0  min: 27  sec: 51\n",
      "epoch: 0  batch: 39 / 721  loss: 0.37920825259807783  hr: 0  min: 27  sec: 56\n",
      "epoch: 0  batch: 40 / 721  loss: 0.37378620766103265  hr: 0  min: 28  sec: 2\n",
      "epoch: 0  batch: 41 / 721  loss: 0.3722225642058907  hr: 0  min: 28  sec: 7\n",
      "epoch: 0  batch: 42 / 721  loss: 0.3684105993736358  hr: 0  min: 28  sec: 12\n",
      "epoch: 0  batch: 43 / 721  loss: 0.3639330846625705  hr: 0  min: 28  sec: 16\n",
      "epoch: 0  batch: 44 / 721  loss: 0.3606204011223533  hr: 0  min: 28  sec: 14\n",
      "epoch: 0  batch: 45 / 721  loss: 0.3562623745865292  hr: 0  min: 28  sec: 17\n",
      "epoch: 0  batch: 46 / 721  loss: 0.35208166649808054  hr: 0  min: 28  sec: 17\n",
      "epoch: 0  batch: 47 / 721  loss: 0.3482618439704814  hr: 0  min: 28  sec: 23\n",
      "epoch: 0  batch: 48 / 721  loss: 0.3438035883009434  hr: 0  min: 28  sec: 19\n",
      "epoch: 0  batch: 49 / 721  loss: 0.340107473791862  hr: 0  min: 28  sec: 16\n",
      "epoch: 0  batch: 50 / 721  loss: 0.33750263214111326  hr: 0  min: 28  sec: 16\n",
      "epoch: 0  batch: 51 / 721  loss: 0.3332832232117653  hr: 0  min: 28  sec: 14\n",
      "epoch: 0  batch: 52 / 721  loss: 0.3300646547801219  hr: 0  min: 28  sec: 16\n",
      "epoch: 0  batch: 53 / 721  loss: 0.3270000032377693  hr: 0  min: 28  sec: 14\n",
      "epoch: 0  batch: 54 / 721  loss: 0.3240854940204709  hr: 0  min: 28  sec: 12\n",
      "epoch: 0  batch: 55 / 721  loss: 0.3218939660625024  hr: 0  min: 28  sec: 10\n",
      "epoch: 0  batch: 56 / 721  loss: 0.31986107930008856  hr: 0  min: 28  sec: 7\n",
      "epoch: 0  batch: 57 / 721  loss: 0.3165276109388  hr: 0  min: 28  sec: 6\n",
      "epoch: 0  batch: 58 / 721  loss: 0.3143728184032029  hr: 0  min: 28  sec: 5\n",
      "epoch: 0  batch: 59 / 721  loss: 0.311050883920516  hr: 0  min: 28  sec: 7\n",
      "epoch: 0  batch: 60 / 721  loss: 0.307920894275109  hr: 0  min: 28  sec: 9\n",
      "epoch: 0  batch: 61 / 721  loss: 0.3058448450975731  hr: 0  min: 28  sec: 9\n",
      "epoch: 0  batch: 62 / 721  loss: 0.3026973710906121  hr: 0  min: 28  sec: 21\n",
      "epoch: 0  batch: 63 / 721  loss: 0.2996212656772326  hr: 0  min: 28  sec: 17\n",
      "epoch: 0  batch: 64 / 721  loss: 0.2994747041957453  hr: 0  min: 28  sec: 12\n",
      "epoch: 0  batch: 65 / 721  loss: 0.2983157347028072  hr: 0  min: 28  sec: 10\n",
      "epoch: 0  batch: 66 / 721  loss: 0.29528892491803027  hr: 0  min: 28  sec: 8\n",
      "epoch: 0  batch: 67 / 721  loss: 0.29206495465182547  hr: 0  min: 28  sec: 6\n",
      "epoch: 0  batch: 68 / 721  loss: 0.2891533313428654  hr: 0  min: 28  sec: 4\n",
      "epoch: 0  batch: 69 / 721  loss: 0.2868376857992532  hr: 0  min: 28  sec: 2\n",
      "epoch: 0  batch: 70 / 721  loss: 0.2837113104760647  hr: 0  min: 27  sec: 58\n",
      "epoch: 0  batch: 71 / 721  loss: 0.2814379839200369  hr: 0  min: 27  sec: 58\n",
      "epoch: 0  batch: 72 / 721  loss: 0.2802244382393029  hr: 0  min: 27  sec: 55\n",
      "epoch: 0  batch: 73 / 721  loss: 0.2781133060790088  hr: 0  min: 27  sec: 53\n",
      "epoch: 0  batch: 74 / 721  loss: 0.2758706453684214  hr: 0  min: 27  sec: 55\n",
      "epoch: 0  batch: 75 / 721  loss: 0.27431478480497995  hr: 0  min: 27  sec: 57\n",
      "epoch: 0  batch: 76 / 721  loss: 0.2717713897949771  hr: 0  min: 27  sec: 58\n",
      "epoch: 0  batch: 77 / 721  loss: 0.268973525081362  hr: 0  min: 27  sec: 58\n",
      "epoch: 0  batch: 78 / 721  loss: 0.2688478437753824  hr: 0  min: 27  sec: 56\n",
      "epoch: 0  batch: 79 / 721  loss: 0.26659532645835154  hr: 0  min: 27  sec: 54\n",
      "epoch: 0  batch: 80 / 721  loss: 0.26457990510389207  hr: 0  min: 27  sec: 51\n",
      "epoch: 0  batch: 81 / 721  loss: 0.26234471016092065  hr: 0  min: 27  sec: 50\n",
      "epoch: 0  batch: 82 / 721  loss: 0.2599181958996668  hr: 0  min: 27  sec: 58\n",
      "epoch: 0  batch: 83 / 721  loss: 0.25926552703940725  hr: 0  min: 27  sec: 55\n",
      "epoch: 0  batch: 84 / 721  loss: 0.2580031780082555  hr: 0  min: 27  sec: 55\n",
      "epoch: 0  batch: 85 / 721  loss: 0.25547573947731184  hr: 0  min: 27  sec: 52\n",
      "epoch: 0  batch: 86 / 721  loss: 0.25929450187398945  hr: 0  min: 27  sec: 49\n",
      "epoch: 0  batch: 87 / 721  loss: 0.25825152944388063  hr: 0  min: 27  sec: 47\n",
      "epoch: 0  batch: 88 / 721  loss: 0.2568561071953313  hr: 0  min: 27  sec: 44\n",
      "epoch: 0  batch: 89 / 721  loss: 0.25521574315897533  hr: 0  min: 27  sec: 44\n",
      "epoch: 0  batch: 90 / 721  loss: 0.2537954855710268  hr: 0  min: 27  sec: 45\n",
      "epoch: 0  batch: 91 / 721  loss: 0.2558855498311939  hr: 0  min: 27  sec: 43\n",
      "epoch: 0  batch: 92 / 721  loss: 0.2543919143550422  hr: 0  min: 27  sec: 41\n",
      "epoch: 0  batch: 93 / 721  loss: 0.25323689949288164  hr: 0  min: 27  sec: 39\n",
      "epoch: 0  batch: 94 / 721  loss: 0.2516518463876019  hr: 0  min: 27  sec: 38\n",
      "epoch: 0  batch: 95 / 721  loss: 0.25128384032531786  hr: 0  min: 27  sec: 37\n",
      "epoch: 0  batch: 96 / 721  loss: 0.2499961844102169  hr: 0  min: 27  sec: 35\n",
      "epoch: 0  batch: 97 / 721  loss: 0.24814947068537632  hr: 0  min: 27  sec: 32\n",
      "epoch: 0  batch: 98 / 721  loss: 0.24749711258526969  hr: 0  min: 27  sec: 30\n",
      "epoch: 0  batch: 99 / 721  loss: 0.2468807546403071  hr: 0  min: 27  sec: 27\n",
      "epoch: 0  batch: 100 / 721  loss: 0.244933846257627  hr: 0  min: 27  sec: 27\n",
      "epoch: 0  batch: 101 / 721  loss: 0.24383943000494843  hr: 0  min: 27  sec: 34\n",
      "epoch: 0  batch: 102 / 721  loss: 0.24365767102469416  hr: 0  min: 27  sec: 34\n",
      "epoch: 0  batch: 103 / 721  loss: 0.24354382810517422  hr: 0  min: 27  sec: 35\n",
      "epoch: 0  batch: 104 / 721  loss: 0.24260740322419083  hr: 0  min: 27  sec: 33\n",
      "epoch: 0  batch: 105 / 721  loss: 0.24088531083294323  hr: 0  min: 27  sec: 30\n",
      "epoch: 0  batch: 106 / 721  loss: 0.23986766800143808  hr: 0  min: 27  sec: 28\n",
      "epoch: 0  batch: 107 / 721  loss: 0.23967158018010798  hr: 0  min: 27  sec: 25\n",
      "epoch: 0  batch: 108 / 721  loss: 0.23835175660335356  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 109 / 721  loss: 0.23890233350866433  hr: 0  min: 27  sec: 25\n",
      "epoch: 0  batch: 110 / 721  loss: 0.23888474076308988  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 111 / 721  loss: 0.2381332494855464  hr: 0  min: 27  sec: 23\n",
      "epoch: 0  batch: 112 / 721  loss: 0.23667075328661927  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 113 / 721  loss: 0.23525898451958083  hr: 0  min: 27  sec: 25\n",
      "epoch: 0  batch: 114 / 721  loss: 0.23439033188971511  hr: 0  min: 27  sec: 26\n",
      "epoch: 0  batch: 115 / 721  loss: 0.23294277505382247  hr: 0  min: 27  sec: 30\n",
      "epoch: 0  batch: 116 / 721  loss: 0.23290005779086514  hr: 0  min: 27  sec: 28\n",
      "epoch: 0  batch: 117 / 721  loss: 0.2326961889799334  hr: 0  min: 27  sec: 26\n",
      "epoch: 0  batch: 118 / 721  loss: 0.23125694284878545  hr: 0  min: 27  sec: 26\n",
      "epoch: 0  batch: 119 / 721  loss: 0.22987008141745038  hr: 0  min: 27  sec: 26\n",
      "epoch: 0  batch: 120 / 721  loss: 0.2287442616187036  hr: 0  min: 27  sec: 27\n",
      "epoch: 0  batch: 121 / 721  loss: 0.22745800390839577  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 122 / 721  loss: 0.22665849941797922  hr: 0  min: 27  sec: 32\n",
      "epoch: 0  batch: 123 / 721  loss: 0.22722967711042583  hr: 0  min: 27  sec: 30\n",
      "epoch: 0  batch: 124 / 721  loss: 0.22589914936331973  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 125 / 721  loss: 0.22522742381691932  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 126 / 721  loss: 0.2254784262428681  hr: 0  min: 27  sec: 27\n",
      "epoch: 0  batch: 127 / 721  loss: 0.2243240894823093  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 128 / 721  loss: 0.22372414314304478  hr: 0  min: 27  sec: 31\n",
      "epoch: 0  batch: 129 / 721  loss: 0.2226912588631922  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 130 / 721  loss: 0.22198831980618147  hr: 0  min: 27  sec: 26\n",
      "epoch: 0  batch: 131 / 721  loss: 0.22182308254009894  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 132 / 721  loss: 0.22094335917834984  hr: 0  min: 27  sec: 23\n",
      "epoch: 0  batch: 133 / 721  loss: 0.21971827781850234  hr: 0  min: 27  sec: 23\n",
      "epoch: 0  batch: 134 / 721  loss: 0.2192121431954316  hr: 0  min: 27  sec: 23\n",
      "epoch: 0  batch: 135 / 721  loss: 0.2184988223568157  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 136 / 721  loss: 0.2177719483973787  hr: 0  min: 27  sec: 26\n",
      "epoch: 0  batch: 137 / 721  loss: 0.21681853712801516  hr: 0  min: 27  sec: 30\n",
      "epoch: 0  batch: 138 / 721  loss: 0.21673704821454443  hr: 0  min: 27  sec: 28\n",
      "epoch: 0  batch: 139 / 721  loss: 0.21696520031248923  hr: 0  min: 27  sec: 30\n",
      "epoch: 0  batch: 140 / 721  loss: 0.21616213356277772  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 141 / 721  loss: 0.21505848421378337  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 142 / 721  loss: 0.21436656573393814  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 143 / 721  loss: 0.21345097266814925  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 144 / 721  loss: 0.2129636632744223  hr: 0  min: 27  sec: 28\n",
      "epoch: 0  batch: 145 / 721  loss: 0.21228529356163123  hr: 0  min: 27  sec: 30\n",
      "epoch: 0  batch: 146 / 721  loss: 0.21143412684434898  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 147 / 721  loss: 0.21083307978348667  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 148 / 721  loss: 0.2105373541094564  hr: 0  min: 27  sec: 29\n",
      "epoch: 0  batch: 149 / 721  loss: 0.20994116718437048  hr: 0  min: 27  sec: 28\n",
      "epoch: 0  batch: 150 / 721  loss: 0.2091681361446778  hr: 0  min: 27  sec: 27\n",
      "epoch: 0  batch: 151 / 721  loss: 0.20804904667746943  hr: 0  min: 27  sec: 27\n",
      "epoch: 0  batch: 152 / 721  loss: 0.20765260459953233  hr: 0  min: 27  sec: 26\n",
      "epoch: 0  batch: 153 / 721  loss: 0.2066003641515386  hr: 0  min: 27  sec: 25\n",
      "epoch: 0  batch: 154 / 721  loss: 0.20594461651688273  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 155 / 721  loss: 0.2053689072930044  hr: 0  min: 27  sec: 23\n",
      "epoch: 0  batch: 156 / 721  loss: 0.20417030292969102  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 157 / 721  loss: 0.2042162368418115  hr: 0  min: 27  sec: 23\n",
      "epoch: 0  batch: 158 / 721  loss: 0.20348051415544144  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 159 / 721  loss: 0.20332253624761254  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 160 / 721  loss: 0.20292069184361025  hr: 0  min: 27  sec: 24\n",
      "epoch: 0  batch: 161 / 721  loss: 0.20217219040214274  hr: 0  min: 27  sec: 22\n",
      "epoch: 0  batch: 162 / 721  loss: 0.2014070649826784  hr: 0  min: 27  sec: 20\n",
      "epoch: 0  batch: 163 / 721  loss: 0.20060481276203151  hr: 0  min: 27  sec: 19\n",
      "epoch: 0  batch: 164 / 721  loss: 0.19991118727797053  hr: 0  min: 27  sec: 20\n",
      "epoch: 0  batch: 165 / 721  loss: 0.19943591309993555  hr: 0  min: 27  sec: 18\n",
      "epoch: 0  batch: 166 / 721  loss: 0.19956812297319432  hr: 0  min: 27  sec: 16\n",
      "epoch: 0  batch: 167 / 721  loss: 0.198714258104652  hr: 0  min: 27  sec: 20\n",
      "epoch: 0  batch: 168 / 721  loss: 0.1979811638926289  hr: 0  min: 27  sec: 18\n",
      "epoch: 0  batch: 169 / 721  loss: 0.19742839469315385  hr: 0  min: 27  sec: 18\n",
      "epoch: 0  batch: 170 / 721  loss: 0.1967787653536481  hr: 0  min: 27  sec: 18\n",
      "epoch: 0  batch: 171 / 721  loss: 0.19612444774928497  hr: 0  min: 27  sec: 17\n",
      "epoch: 0  batch: 172 / 721  loss: 0.19585081262427362  hr: 0  min: 27  sec: 18\n",
      "epoch: 0  batch: 173 / 721  loss: 0.19513923613451464  hr: 0  min: 27  sec: 16\n",
      "epoch: 0  batch: 174 / 721  loss: 0.1946940802615778  hr: 0  min: 27  sec: 14\n",
      "epoch: 0  batch: 175 / 721  loss: 0.1946321493706533  hr: 0  min: 27  sec: 12\n",
      "epoch: 0  batch: 176 / 721  loss: 0.1943524020448835  hr: 0  min: 27  sec: 9\n",
      "epoch: 0  batch: 177 / 721  loss: 0.1937667297588735  hr: 0  min: 27  sec: 8\n",
      "epoch: 0  batch: 178 / 721  loss: 0.1928824001630203  hr: 0  min: 27  sec: 7\n",
      "epoch: 0  batch: 179 / 721  loss: 0.19219581935783672  hr: 0  min: 27  sec: 7\n",
      "epoch: 0  batch: 180 / 721  loss: 0.19144317405298353  hr: 0  min: 27  sec: 6\n",
      "epoch: 0  batch: 181 / 721  loss: 0.19069673535697038  hr: 0  min: 27  sec: 5\n",
      "epoch: 0  batch: 182 / 721  loss: 0.19027664179780654  hr: 0  min: 27  sec: 4\n",
      "epoch: 0  batch: 183 / 721  loss: 0.18969240417205246  hr: 0  min: 27  sec: 3\n",
      "epoch: 0  batch: 184 / 721  loss: 0.18909114322868054  hr: 0  min: 27  sec: 1\n",
      "epoch: 0  batch: 185 / 721  loss: 0.18840305149756573  hr: 0  min: 27  sec: 1\n",
      "epoch: 0  batch: 186 / 721  loss: 0.18794765464601018  hr: 0  min: 26  sec: 59\n",
      "epoch: 0  batch: 187 / 721  loss: 0.1871469517582081  hr: 0  min: 27  sec: 0\n",
      "epoch: 0  batch: 188 / 721  loss: 0.1864244290825693  hr: 0  min: 26  sec: 58\n",
      "epoch: 0  batch: 189 / 721  loss: 0.18562781541711754  hr: 0  min: 26  sec: 58\n",
      "epoch: 0  batch: 190 / 721  loss: 0.1851306228163211  hr: 0  min: 26  sec: 57\n",
      "epoch: 0  batch: 191 / 721  loss: 0.18427868128915106  hr: 0  min: 26  sec: 57\n",
      "epoch: 0  batch: 192 / 721  loss: 0.18349254814287028  hr: 0  min: 26  sec: 56\n",
      "epoch: 0  batch: 193 / 721  loss: 0.18320465659230484  hr: 0  min: 26  sec: 55\n",
      "epoch: 0  batch: 194 / 721  loss: 0.18263391048177002  hr: 0  min: 26  sec: 53\n",
      "epoch: 0  batch: 195 / 721  loss: 0.1821068233404404  hr: 0  min: 26  sec: 51\n",
      "epoch: 0  batch: 196 / 721  loss: 0.18128036980384163  hr: 0  min: 26  sec: 50\n",
      "epoch: 0  batch: 197 / 721  loss: 0.18112236963878126  hr: 0  min: 26  sec: 48\n",
      "epoch: 0  batch: 198 / 721  loss: 0.18069752650993942  hr: 0  min: 26  sec: 47\n",
      "epoch: 0  batch: 199 / 721  loss: 0.18008726961611204  hr: 0  min: 26  sec: 45\n",
      "epoch: 0  batch: 200 / 721  loss: 0.18098378173075616  hr: 0  min: 26  sec: 43\n",
      "epoch: 0  batch: 201 / 721  loss: 0.18038660779928983  hr: 0  min: 26  sec: 43\n",
      "epoch: 0  batch: 202 / 721  loss: 0.18036677270648208  hr: 0  min: 26  sec: 41\n",
      "epoch: 0  batch: 203 / 721  loss: 0.17976666184078002  hr: 0  min: 26  sec: 42\n",
      "epoch: 0  batch: 204 / 721  loss: 0.1795194171387337  hr: 0  min: 26  sec: 40\n",
      "epoch: 0  batch: 205 / 721  loss: 0.1794260362753781  hr: 0  min: 26  sec: 39\n",
      "epoch: 0  batch: 206 / 721  loss: 0.17896042331076653  hr: 0  min: 26  sec: 37\n",
      "epoch: 0  batch: 207 / 721  loss: 0.17905922558896495  hr: 0  min: 26  sec: 35\n",
      "epoch: 0  batch: 208 / 721  loss: 0.17882709234702185  hr: 0  min: 26  sec: 35\n",
      "epoch: 0  batch: 209 / 721  loss: 0.17825207282659825  hr: 0  min: 26  sec: 34\n",
      "epoch: 0  batch: 210 / 721  loss: 0.17790388898657902  hr: 0  min: 26  sec: 33\n",
      "epoch: 0  batch: 211 / 721  loss: 0.1775128387744534  hr: 0  min: 26  sec: 33\n",
      "epoch: 0  batch: 212 / 721  loss: 0.17692045701386513  hr: 0  min: 26  sec: 31\n",
      "epoch: 0  batch: 213 / 721  loss: 0.17640600034293713  hr: 0  min: 26  sec: 29\n",
      "epoch: 0  batch: 214 / 721  loss: 0.17603777845990715  hr: 0  min: 26  sec: 29\n",
      "epoch: 0  batch: 215 / 721  loss: 0.17585511910187643  hr: 0  min: 26  sec: 28\n",
      "epoch: 0  batch: 216 / 721  loss: 0.17684553379917312  hr: 0  min: 26  sec: 26\n",
      "epoch: 0  batch: 217 / 721  loss: 0.17630306012447802  hr: 0  min: 26  sec: 25\n",
      "epoch: 0  batch: 218 / 721  loss: 0.1757738834178639  hr: 0  min: 26  sec: 25\n",
      "epoch: 0  batch: 219 / 721  loss: 0.17517705883439547  hr: 0  min: 26  sec: 23\n",
      "epoch: 0  batch: 220 / 721  loss: 0.17514920474284074  hr: 0  min: 26  sec: 21\n",
      "epoch: 0  batch: 221 / 721  loss: 0.17474327390651087  hr: 0  min: 26  sec: 20\n",
      "epoch: 0  batch: 222 / 721  loss: 0.1744137576084819  hr: 0  min: 26  sec: 20\n",
      "epoch: 0  batch: 223 / 721  loss: 0.1738939480981351  hr: 0  min: 26  sec: 18\n",
      "epoch: 0  batch: 224 / 721  loss: 0.17326093178209184  hr: 0  min: 26  sec: 18\n",
      "epoch: 0  batch: 225 / 721  loss: 0.17316755558881494  hr: 0  min: 26  sec: 19\n",
      "epoch: 0  batch: 226 / 721  loss: 0.17273865823955398  hr: 0  min: 26  sec: 18\n",
      "epoch: 0  batch: 227 / 721  loss: 0.17219734026964278  hr: 0  min: 26  sec: 16\n",
      "epoch: 0  batch: 228 / 721  loss: 0.17165468856202146  hr: 0  min: 26  sec: 18\n",
      "epoch: 0  batch: 229 / 721  loss: 0.17125802852343525  hr: 0  min: 26  sec: 16\n",
      "epoch: 0  batch: 230 / 721  loss: 0.17078068837037552  hr: 0  min: 26  sec: 15\n",
      "epoch: 0  batch: 231 / 721  loss: 0.1702168674099368  hr: 0  min: 26  sec: 14\n",
      "epoch: 0  batch: 232 / 721  loss: 0.16979926993974068  hr: 0  min: 26  sec: 13\n",
      "epoch: 0  batch: 233 / 721  loss: 0.1691771510652718  hr: 0  min: 26  sec: 12\n",
      "epoch: 0  batch: 234 / 721  loss: 0.1687469182488246  hr: 0  min: 26  sec: 11\n",
      "epoch: 0  batch: 235 / 721  loss: 0.1681786982778539  hr: 0  min: 26  sec: 9\n",
      "epoch: 0  batch: 236 / 721  loss: 0.16761755944876852  hr: 0  min: 26  sec: 8\n",
      "epoch: 0  batch: 237 / 721  loss: 0.16716949412214102  hr: 0  min: 26  sec: 6\n",
      "epoch: 0  batch: 238 / 721  loss: 0.1667407449929654  hr: 0  min: 26  sec: 5\n",
      "epoch: 0  batch: 239 / 721  loss: 0.16646509679915017  hr: 0  min: 26  sec: 4\n",
      "epoch: 0  batch: 240 / 721  loss: 0.1664324816626807  hr: 0  min: 26  sec: 3\n",
      "epoch: 0  batch: 241 / 721  loss: 0.16593999698510803  hr: 0  min: 26  sec: 2\n",
      "epoch: 0  batch: 242 / 721  loss: 0.1657666964071595  hr: 0  min: 26  sec: 0\n",
      "epoch: 0  batch: 243 / 721  loss: 0.1654375649130148  hr: 0  min: 25  sec: 59\n",
      "epoch: 0  batch: 244 / 721  loss: 0.16494543022918898  hr: 0  min: 25  sec: 58\n",
      "epoch: 0  batch: 245 / 721  loss: 0.1644076571020545  hr: 0  min: 25  sec: 57\n",
      "epoch: 0  batch: 246 / 721  loss: 0.1643649457037691  hr: 0  min: 25  sec: 57\n",
      "epoch: 0  batch: 247 / 721  loss: 0.16405096627500376  hr: 0  min: 25  sec: 56\n",
      "epoch: 0  batch: 248 / 721  loss: 0.16442620154890802  hr: 0  min: 25  sec: 55\n",
      "epoch: 0  batch: 249 / 721  loss: 0.1639615681366509  hr: 0  min: 25  sec: 54\n",
      "epoch: 0  batch: 250 / 721  loss: 0.16394663374125956  hr: 0  min: 25  sec: 53\n",
      "epoch: 0  batch: 251 / 721  loss: 0.16369504204012483  hr: 0  min: 25  sec: 52\n",
      "epoch: 0  batch: 252 / 721  loss: 0.16325755208908094  hr: 0  min: 25  sec: 51\n",
      "epoch: 0  batch: 253 / 721  loss: 0.16284007724860441  hr: 0  min: 25  sec: 50\n",
      "epoch: 0  batch: 254 / 721  loss: 0.16265164015037337  hr: 0  min: 25  sec: 48\n",
      "epoch: 0  batch: 255 / 721  loss: 0.1622351908654559  hr: 0  min: 25  sec: 48\n",
      "epoch: 0  batch: 256 / 721  loss: 0.1617972176609328  hr: 0  min: 25  sec: 46\n",
      "epoch: 0  batch: 257 / 721  loss: 0.1612921402815019  hr: 0  min: 25  sec: 45\n",
      "epoch: 0  batch: 258 / 721  loss: 0.16080419190747794  hr: 0  min: 25  sec: 47\n",
      "epoch: 0  batch: 259 / 721  loss: 0.16031150010676917  hr: 0  min: 25  sec: 46\n",
      "epoch: 0  batch: 260 / 721  loss: 0.15993287335508144  hr: 0  min: 25  sec: 45\n",
      "epoch: 0  batch: 261 / 721  loss: 0.15965297654042757  hr: 0  min: 25  sec: 43\n",
      "epoch: 0  batch: 262 / 721  loss: 0.15996594751211068  hr: 0  min: 25  sec: 42\n",
      "epoch: 0  batch: 263 / 721  loss: 0.15954530668292663  hr: 0  min: 25  sec: 41\n",
      "epoch: 0  batch: 264 / 721  loss: 0.15987785644547053  hr: 0  min: 25  sec: 39\n",
      "epoch: 0  batch: 265 / 721  loss: 0.1601085496258061  hr: 0  min: 25  sec: 37\n",
      "epoch: 0  batch: 266 / 721  loss: 0.15981155820190907  hr: 0  min: 25  sec: 36\n",
      "epoch: 0  batch: 267 / 721  loss: 0.15936437536603057  hr: 0  min: 25  sec: 35\n",
      "epoch: 0  batch: 268 / 721  loss: 0.15885611786159562  hr: 0  min: 25  sec: 35\n",
      "epoch: 0  batch: 269 / 721  loss: 0.15843824563084039  hr: 0  min: 25  sec: 33\n",
      "epoch: 0  batch: 270 / 721  loss: 0.15808163710214473  hr: 0  min: 25  sec: 32\n",
      "epoch: 0  batch: 271 / 721  loss: 0.1579339256255829  hr: 0  min: 25  sec: 31\n",
      "epoch: 0  batch: 272 / 721  loss: 0.1576507633058902  hr: 0  min: 25  sec: 30\n",
      "epoch: 0  batch: 273 / 721  loss: 0.15725701034833223  hr: 0  min: 25  sec: 28\n",
      "epoch: 0  batch: 274 / 721  loss: 0.15691181780756824  hr: 0  min: 25  sec: 26\n",
      "epoch: 0  batch: 275 / 721  loss: 0.15706182244149122  hr: 0  min: 25  sec: 24\n",
      "epoch: 0  batch: 276 / 721  loss: 0.15683859243881013  hr: 0  min: 25  sec: 23\n",
      "epoch: 0  batch: 277 / 721  loss: 0.1566308508980145  hr: 0  min: 25  sec: 22\n",
      "epoch: 0  batch: 278 / 721  loss: 0.15613639066598828  hr: 0  min: 25  sec: 21\n",
      "epoch: 0  batch: 279 / 721  loss: 0.1557123564771213  hr: 0  min: 25  sec: 21\n",
      "epoch: 0  batch: 280 / 721  loss: 0.15535432354414036  hr: 0  min: 25  sec: 21\n",
      "epoch: 0  batch: 281 / 721  loss: 0.1553621288269652  hr: 0  min: 25  sec: 19\n",
      "epoch: 0  batch: 282 / 721  loss: 0.15506190855505195  hr: 0  min: 25  sec: 19\n",
      "epoch: 0  batch: 283 / 721  loss: 0.1547060045199765  hr: 0  min: 25  sec: 18\n",
      "epoch: 0  batch: 284 / 721  loss: 0.1544309016317129  hr: 0  min: 25  sec: 16\n",
      "epoch: 0  batch: 285 / 721  loss: 0.1542100499857936  hr: 0  min: 25  sec: 15\n",
      "epoch: 0  batch: 286 / 721  loss: 0.15373716612144575  hr: 0  min: 25  sec: 13\n",
      "epoch: 0  batch: 287 / 721  loss: 0.1534958787535022  hr: 0  min: 25  sec: 12\n",
      "epoch: 0  batch: 288 / 721  loss: 0.15316151591509375  hr: 0  min: 25  sec: 11\n",
      "epoch: 0  batch: 289 / 721  loss: 0.15278051100722853  hr: 0  min: 25  sec: 9\n",
      "epoch: 0  batch: 290 / 721  loss: 0.1524410848794826  hr: 0  min: 25  sec: 8\n",
      "epoch: 0  batch: 291 / 721  loss: 0.15202783204973563  hr: 0  min: 25  sec: 7\n",
      "epoch: 0  batch: 292 / 721  loss: 0.151760793641873  hr: 0  min: 25  sec: 6\n",
      "epoch: 0  batch: 293 / 721  loss: 0.15170079879717005  hr: 0  min: 25  sec: 6\n",
      "epoch: 0  batch: 294 / 721  loss: 0.15137024507002564  hr: 0  min: 25  sec: 5\n",
      "epoch: 0  batch: 295 / 721  loss: 0.151390261866026  hr: 0  min: 25  sec: 3\n",
      "epoch: 0  batch: 296 / 721  loss: 0.15097568662384073  hr: 0  min: 25  sec: 2\n",
      "epoch: 0  batch: 297 / 721  loss: 0.15060638641367857  hr: 0  min: 25  sec: 1\n",
      "epoch: 0  batch: 298 / 721  loss: 0.15037777817389308  hr: 0  min: 25  sec: 0\n",
      "epoch: 0  batch: 299 / 721  loss: 0.15000601742030387  hr: 0  min: 24  sec: 59\n",
      "epoch: 0  batch: 300 / 721  loss: 0.1495609517892202  hr: 0  min: 24  sec: 57\n",
      "epoch: 0  batch: 301 / 721  loss: 0.14921803277394305  hr: 0  min: 24  sec: 56\n",
      "epoch: 0  batch: 302 / 721  loss: 0.14918900488425565  hr: 0  min: 24  sec: 55\n",
      "epoch: 0  batch: 303 / 721  loss: 0.14879106116103064  hr: 0  min: 24  sec: 54\n",
      "epoch: 0  batch: 304 / 721  loss: 0.14839855175293787  hr: 0  min: 24  sec: 52\n",
      "epoch: 0  batch: 305 / 721  loss: 0.14811961358321496  hr: 0  min: 24  sec: 51\n",
      "epoch: 0  batch: 306 / 721  loss: 0.14779393913626088  hr: 0  min: 24  sec: 51\n",
      "epoch: 0  batch: 307 / 721  loss: 0.14751704370281402  hr: 0  min: 24  sec: 50\n",
      "epoch: 0  batch: 308 / 721  loss: 0.1471442209568794  hr: 0  min: 24  sec: 49\n",
      "epoch: 0  batch: 309 / 721  loss: 0.14757576439541714  hr: 0  min: 24  sec: 48\n",
      "epoch: 0  batch: 310 / 721  loss: 0.14714590840702577  hr: 0  min: 24  sec: 48\n",
      "epoch: 0  batch: 311 / 721  loss: 0.14705113049333024  hr: 0  min: 24  sec: 46\n",
      "epoch: 0  batch: 312 / 721  loss: 0.14665665871833858  hr: 0  min: 24  sec: 45\n",
      "epoch: 0  batch: 313 / 721  loss: 0.14631503864753836  hr: 0  min: 24  sec: 47\n",
      "epoch: 0  batch: 314 / 721  loss: 0.1460947476618087  hr: 0  min: 24  sec: 46\n",
      "epoch: 0  batch: 315 / 721  loss: 0.14598181033950478  hr: 0  min: 24  sec: 45\n",
      "epoch: 0  batch: 316 / 721  loss: 0.1457972335717582  hr: 0  min: 24  sec: 44\n",
      "epoch: 0  batch: 317 / 721  loss: 0.14680929840703963  hr: 0  min: 24  sec: 44\n",
      "epoch: 0  batch: 318 / 721  loss: 0.1464496410053528  hr: 0  min: 24  sec: 43\n",
      "epoch: 0  batch: 319 / 721  loss: 0.14620593431052462  hr: 0  min: 24  sec: 42\n",
      "epoch: 0  batch: 320 / 721  loss: 0.14598420569964218  hr: 0  min: 24  sec: 41\n",
      "epoch: 0  batch: 321 / 721  loss: 0.14558715389234905  hr: 0  min: 24  sec: 40\n",
      "epoch: 0  batch: 322 / 721  loss: 0.1452580792742625  hr: 0  min: 24  sec: 41\n",
      "epoch: 0  batch: 323 / 721  loss: 0.1452003172670863  hr: 0  min: 24  sec: 39\n",
      "epoch: 0  batch: 324 / 721  loss: 0.1449446373210967  hr: 0  min: 24  sec: 38\n",
      "epoch: 0  batch: 325 / 721  loss: 0.14479273087416705  hr: 0  min: 24  sec: 36\n",
      "epoch: 0  batch: 326 / 721  loss: 0.1446077653801103  hr: 0  min: 24  sec: 35\n",
      "epoch: 0  batch: 327 / 721  loss: 0.1443311292934445  hr: 0  min: 24  sec: 34\n",
      "epoch: 0  batch: 328 / 721  loss: 0.1439408718568568  hr: 0  min: 24  sec: 34\n",
      "epoch: 0  batch: 329 / 721  loss: 0.1436557590672718  hr: 0  min: 24  sec: 34\n",
      "epoch: 0  batch: 330 / 721  loss: 0.14339137709784236  hr: 0  min: 24  sec: 33\n",
      "epoch: 0  batch: 331 / 721  loss: 0.1430354357536691  hr: 0  min: 24  sec: 33\n",
      "epoch: 0  batch: 332 / 721  loss: 0.14269124479481884  hr: 0  min: 24  sec: 32\n",
      "epoch: 0  batch: 333 / 721  loss: 0.14265735765958243  hr: 0  min: 24  sec: 31\n",
      "epoch: 0  batch: 334 / 721  loss: 0.1423677183815953  hr: 0  min: 24  sec: 29\n",
      "epoch: 0  batch: 335 / 721  loss: 0.1420679366938881  hr: 0  min: 24  sec: 29\n",
      "epoch: 0  batch: 336 / 721  loss: 0.1417574094957672  hr: 0  min: 24  sec: 28\n",
      "epoch: 0  batch: 337 / 721  loss: 0.14179677837625013  hr: 0  min: 24  sec: 27\n",
      "epoch: 0  batch: 338 / 721  loss: 0.141636386808537  hr: 0  min: 24  sec: 26\n",
      "epoch: 0  batch: 339 / 721  loss: 0.14130302518331458  hr: 0  min: 24  sec: 25\n",
      "epoch: 0  batch: 340 / 721  loss: 0.14124773920184988  hr: 0  min: 24  sec: 24\n",
      "epoch: 0  batch: 341 / 721  loss: 0.1410042806405607  hr: 0  min: 24  sec: 22\n",
      "epoch: 0  batch: 342 / 721  loss: 0.14094520095267404  hr: 0  min: 24  sec: 21\n",
      "epoch: 0  batch: 343 / 721  loss: 0.14075283737413302  hr: 0  min: 24  sec: 21\n",
      "epoch: 0  batch: 344 / 721  loss: 0.1406317801014412  hr: 0  min: 24  sec: 20\n",
      "epoch: 0  batch: 345 / 721  loss: 0.14035229672934266  hr: 0  min: 24  sec: 19\n",
      "epoch: 0  batch: 346 / 721  loss: 0.13997203119185275  hr: 0  min: 24  sec: 18\n",
      "epoch: 0  batch: 347 / 721  loss: 0.13965749295605534  hr: 0  min: 24  sec: 17\n",
      "epoch: 0  batch: 348 / 721  loss: 0.13929736361325043  hr: 0  min: 24  sec: 16\n",
      "epoch: 0  batch: 349 / 721  loss: 0.1393137711225702  hr: 0  min: 24  sec: 15\n",
      "epoch: 0  batch: 350 / 721  loss: 0.13897217423522046  hr: 0  min: 24  sec: 16\n",
      "epoch: 0  batch: 351 / 721  loss: 0.13883424478920012  hr: 0  min: 24  sec: 15\n",
      "epoch: 0  batch: 352 / 721  loss: 0.13862844546780584  hr: 0  min: 24  sec: 13\n",
      "epoch: 0  batch: 353 / 721  loss: 0.13829021973973377  hr: 0  min: 24  sec: 13\n",
      "epoch: 0  batch: 354 / 721  loss: 0.1380836816878179  hr: 0  min: 24  sec: 11\n",
      "epoch: 0  batch: 355 / 721  loss: 0.137837591748947  hr: 0  min: 24  sec: 10\n",
      "epoch: 0  batch: 356 / 721  loss: 0.13761302916950474  hr: 0  min: 24  sec: 10\n",
      "epoch: 0  batch: 357 / 721  loss: 0.13739222672400103  hr: 0  min: 24  sec: 8\n",
      "epoch: 0  batch: 358 / 721  loss: 0.13721076647007016  hr: 0  min: 24  sec: 7\n",
      "epoch: 0  batch: 359 / 721  loss: 0.13701726100083264  hr: 0  min: 24  sec: 7\n",
      "epoch: 0  batch: 360 / 721  loss: 0.13673642016171167  hr: 0  min: 24  sec: 5\n",
      "epoch: 0  batch: 361 / 721  loss: 0.13648722353043072  hr: 0  min: 24  sec: 4\n",
      "epoch: 0  batch: 362 / 721  loss: 0.13619696361511863  hr: 0  min: 24  sec: 3\n",
      "epoch: 0  batch: 363 / 721  loss: 0.1360599709592616  hr: 0  min: 24  sec: 2\n",
      "epoch: 0  batch: 364 / 721  loss: 0.13579078760908936  hr: 0  min: 24  sec: 1\n",
      "epoch: 0  batch: 365 / 721  loss: 0.13562822465670027  hr: 0  min: 24  sec: 1\n",
      "epoch: 0  batch: 366 / 721  loss: 0.13536402738088224  hr: 0  min: 24  sec: 0\n",
      "epoch: 0  batch: 367 / 721  loss: 0.13583607756728128  hr: 0  min: 23  sec: 59\n",
      "epoch: 0  batch: 368 / 721  loss: 0.13558228230154465  hr: 0  min: 23  sec: 58\n",
      "epoch: 0  batch: 369 / 721  loss: 0.1353790153418655  hr: 0  min: 23  sec: 57\n",
      "epoch: 0  batch: 370 / 721  loss: 0.13512060255869418  hr: 0  min: 23  sec: 56\n",
      "epoch: 0  batch: 371 / 721  loss: 0.13497652244224462  hr: 0  min: 23  sec: 55\n",
      "epoch: 0  batch: 372 / 721  loss: 0.13484226718234518  hr: 0  min: 23  sec: 54\n",
      "epoch: 0  batch: 373 / 721  loss: 0.1345479240960594  hr: 0  min: 23  sec: 53\n",
      "epoch: 0  batch: 374 / 721  loss: 0.1343499858524989  hr: 0  min: 23  sec: 53\n",
      "epoch: 0  batch: 375 / 721  loss: 0.13411964831501247  hr: 0  min: 23  sec: 52\n",
      "epoch: 0  batch: 376 / 721  loss: 0.1338293475638877  hr: 0  min: 23  sec: 51\n",
      "epoch: 0  batch: 377 / 721  loss: 0.13373138723198313  hr: 0  min: 23  sec: 49\n",
      "epoch: 0  batch: 378 / 721  loss: 0.13345714021602242  hr: 0  min: 23  sec: 49\n",
      "epoch: 0  batch: 379 / 721  loss: 0.13330957401386587  hr: 0  min: 23  sec: 49\n",
      "epoch: 0  batch: 380 / 721  loss: 0.13297560492677515  hr: 0  min: 23  sec: 49\n",
      "epoch: 0  batch: 381 / 721  loss: 0.13295337965765727  hr: 0  min: 23  sec: 47\n",
      "epoch: 0  batch: 382 / 721  loss: 0.1326392100104838  hr: 0  min: 23  sec: 46\n",
      "epoch: 0  batch: 383 / 721  loss: 0.1323603298344866  hr: 0  min: 23  sec: 45\n",
      "epoch: 0  batch: 384 / 721  loss: 0.1321241303400408  hr: 0  min: 23  sec: 45\n",
      "epoch: 0  batch: 385 / 721  loss: 0.1319297711643112  hr: 0  min: 23  sec: 43\n",
      "epoch: 0  batch: 386 / 721  loss: 0.13184957600734318  hr: 0  min: 23  sec: 42\n",
      "epoch: 0  batch: 387 / 721  loss: 0.13165074730056123  hr: 0  min: 23  sec: 41\n",
      "epoch: 0  batch: 388 / 721  loss: 0.13202435059290493  hr: 0  min: 23  sec: 40\n",
      "epoch: 0  batch: 389 / 721  loss: 0.13179921539581757  hr: 0  min: 23  sec: 40\n",
      "epoch: 0  batch: 390 / 721  loss: 0.13169803734725485  hr: 0  min: 23  sec: 39\n",
      "epoch: 0  batch: 391 / 721  loss: 0.13155393487990588  hr: 0  min: 23  sec: 38\n",
      "epoch: 0  batch: 392 / 721  loss: 0.13138644512011005  hr: 0  min: 23  sec: 37\n",
      "epoch: 0  batch: 393 / 721  loss: 0.13113163156392238  hr: 0  min: 23  sec: 36\n",
      "epoch: 0  batch: 394 / 721  loss: 0.1308991591502832  hr: 0  min: 23  sec: 35\n",
      "epoch: 0  batch: 395 / 721  loss: 0.13060070806053242  hr: 0  min: 23  sec: 34\n",
      "epoch: 0  batch: 396 / 721  loss: 0.13095575234043705  hr: 0  min: 23  sec: 33\n",
      "epoch: 0  batch: 397 / 721  loss: 0.13097111675982137  hr: 0  min: 23  sec: 32\n",
      "epoch: 0  batch: 398 / 721  loss: 0.13078982575595305  hr: 0  min: 23  sec: 31\n",
      "epoch: 0  batch: 399 / 721  loss: 0.13057176374543206  hr: 0  min: 23  sec: 30\n",
      "epoch: 0  batch: 400 / 721  loss: 0.1303460887703113  hr: 0  min: 23  sec: 29\n",
      "epoch: 0  batch: 401 / 721  loss: 0.13016459545320333  hr: 0  min: 23  sec: 28\n",
      "epoch: 0  batch: 402 / 721  loss: 0.1299756581857986  hr: 0  min: 23  sec: 26\n",
      "epoch: 0  batch: 403 / 721  loss: 0.12977825762200224  hr: 0  min: 23  sec: 25\n",
      "epoch: 0  batch: 404 / 721  loss: 0.12961014821931793  hr: 0  min: 23  sec: 24\n",
      "epoch: 0  batch: 405 / 721  loss: 0.12969014148607297  hr: 0  min: 23  sec: 23\n",
      "epoch: 0  batch: 406 / 721  loss: 0.1295165773916971  hr: 0  min: 23  sec: 22\n",
      "epoch: 0  batch: 407 / 721  loss: 0.12925579827461914  hr: 0  min: 23  sec: 21\n",
      "epoch: 0  batch: 408 / 721  loss: 0.1290032418568016  hr: 0  min: 23  sec: 20\n",
      "epoch: 0  batch: 409 / 721  loss: 0.12906654490372008  hr: 0  min: 23  sec: 19\n",
      "epoch: 0  batch: 410 / 721  loss: 0.12883415063843132  hr: 0  min: 23  sec: 19\n",
      "epoch: 0  batch: 411 / 721  loss: 0.12861411982961451  hr: 0  min: 23  sec: 17\n",
      "epoch: 0  batch: 412 / 721  loss: 0.12839643180469792  hr: 0  min: 23  sec: 16\n",
      "epoch: 0  batch: 413 / 721  loss: 0.12821233831571538  hr: 0  min: 23  sec: 15\n",
      "epoch: 0  batch: 414 / 721  loss: 0.12796781289701661  hr: 0  min: 23  sec: 14\n",
      "epoch: 0  batch: 415 / 721  loss: 0.12768826232662042  hr: 0  min: 23  sec: 13\n",
      "epoch: 0  batch: 416 / 721  loss: 0.12742627645583035  hr: 0  min: 23  sec: 13\n",
      "epoch: 0  batch: 417 / 721  loss: 0.12736692744791864  hr: 0  min: 23  sec: 12\n",
      "epoch: 0  batch: 418 / 721  loss: 0.12714698422103052  hr: 0  min: 23  sec: 11\n",
      "epoch: 0  batch: 419 / 721  loss: 0.1272916960594442  hr: 0  min: 23  sec: 10\n",
      "epoch: 0  batch: 420 / 721  loss: 0.1270599275140003  hr: 0  min: 23  sec: 10\n",
      "epoch: 0  batch: 421 / 721  loss: 0.12687016259259468  hr: 0  min: 23  sec: 8\n",
      "epoch: 0  batch: 422 / 721  loss: 0.12700265130663724  hr: 0  min: 23  sec: 7\n",
      "epoch: 0  batch: 423 / 721  loss: 0.12676911264103144  hr: 0  min: 23  sec: 7\n",
      "epoch: 0  batch: 424 / 721  loss: 0.12699475719479247  hr: 0  min: 23  sec: 5\n",
      "epoch: 0  batch: 425 / 721  loss: 0.1268522133218015  hr: 0  min: 23  sec: 5\n",
      "epoch: 0  batch: 426 / 721  loss: 0.1269958591189218  hr: 0  min: 23  sec: 4\n",
      "epoch: 0  batch: 427 / 721  loss: 0.12691471762562595  hr: 0  min: 23  sec: 4\n",
      "epoch: 0  batch: 428 / 721  loss: 0.12668325284835405  hr: 0  min: 23  sec: 4\n",
      "epoch: 0  batch: 429 / 721  loss: 0.12662823026943387  hr: 0  min: 23  sec: 3\n",
      "epoch: 0  batch: 430 / 721  loss: 0.12639842240226476  hr: 0  min: 23  sec: 2\n",
      "epoch: 0  batch: 431 / 721  loss: 0.1261363282168755  hr: 0  min: 23  sec: 1\n",
      "epoch: 0  batch: 432 / 721  loss: 0.12604492081380966  hr: 0  min: 23  sec: 0\n",
      "epoch: 0  batch: 433 / 721  loss: 0.12587931840390174  hr: 0  min: 22  sec: 59\n",
      "epoch: 0  batch: 434 / 721  loss: 0.1256629533696628  hr: 0  min: 22  sec: 58\n",
      "epoch: 0  batch: 435 / 721  loss: 0.12544900606675394  hr: 0  min: 22  sec: 57\n",
      "epoch: 0  batch: 436 / 721  loss: 0.12528425121440664  hr: 0  min: 22  sec: 56\n",
      "epoch: 0  batch: 437 / 721  loss: 0.12526833182051608  hr: 0  min: 22  sec: 55\n",
      "epoch: 0  batch: 438 / 721  loss: 0.12511634272544486  hr: 0  min: 22  sec: 55\n",
      "epoch: 0  batch: 439 / 721  loss: 0.1252207938700507  hr: 0  min: 22  sec: 53\n",
      "epoch: 0  batch: 440 / 721  loss: 0.12504688171585174  hr: 0  min: 22  sec: 52\n",
      "epoch: 0  batch: 441 / 721  loss: 0.12488732684209368  hr: 0  min: 22  sec: 51\n",
      "epoch: 0  batch: 442 / 721  loss: 0.12468667068345919  hr: 0  min: 22  sec: 50\n",
      "epoch: 0  batch: 443 / 721  loss: 0.12452623710187644  hr: 0  min: 22  sec: 48\n",
      "epoch: 0  batch: 444 / 721  loss: 0.12440946508148634  hr: 0  min: 22  sec: 48\n",
      "epoch: 0  batch: 445 / 721  loss: 0.12417059554980042  hr: 0  min: 22  sec: 47\n",
      "epoch: 0  batch: 446 / 721  loss: 0.12407318424984747  hr: 0  min: 22  sec: 46\n",
      "epoch: 0  batch: 447 / 721  loss: 0.12407788666802765  hr: 0  min: 22  sec: 46\n",
      "epoch: 0  batch: 448 / 721  loss: 0.12382934833086827  hr: 0  min: 22  sec: 44\n",
      "epoch: 0  batch: 449 / 721  loss: 0.12363826937533766  hr: 0  min: 22  sec: 43\n",
      "epoch: 0  batch: 450 / 721  loss: 0.12362592992890212  hr: 0  min: 22  sec: 42\n",
      "epoch: 0  batch: 451 / 721  loss: 0.12342073674419039  hr: 0  min: 22  sec: 41\n",
      "epoch: 0  batch: 452 / 721  loss: 0.12322610852605276  hr: 0  min: 22  sec: 41\n",
      "epoch: 0  batch: 453 / 721  loss: 0.12305343024654697  hr: 0  min: 22  sec: 40\n",
      "epoch: 0  batch: 454 / 721  loss: 0.12286564050701812  hr: 0  min: 22  sec: 40\n",
      "epoch: 0  batch: 455 / 721  loss: 0.12265111616311165  hr: 0  min: 22  sec: 38\n",
      "epoch: 0  batch: 456 / 721  loss: 0.12244601662703708  hr: 0  min: 22  sec: 38\n",
      "epoch: 0  batch: 457 / 721  loss: 0.12222466972214323  hr: 0  min: 22  sec: 37\n",
      "epoch: 0  batch: 458 / 721  loss: 0.12205958819244561  hr: 0  min: 22  sec: 36\n",
      "epoch: 0  batch: 459 / 721  loss: 0.1218340829408409  hr: 0  min: 22  sec: 35\n",
      "epoch: 0  batch: 460 / 721  loss: 0.12161645122560794  hr: 0  min: 22  sec: 34\n",
      "epoch: 0  batch: 461 / 721  loss: 0.12141049588038515  hr: 0  min: 22  sec: 33\n",
      "epoch: 0  batch: 462 / 721  loss: 0.12124713733026282  hr: 0  min: 22  sec: 32\n",
      "epoch: 0  batch: 463 / 721  loss: 0.12101246040016735  hr: 0  min: 22  sec: 31\n",
      "epoch: 0  batch: 464 / 721  loss: 0.12090185191482306  hr: 0  min: 22  sec: 30\n",
      "epoch: 0  batch: 465 / 721  loss: 0.1207229436565471  hr: 0  min: 22  sec: 29\n",
      "epoch: 0  batch: 466 / 721  loss: 0.1205178606503767  hr: 0  min: 22  sec: 28\n",
      "epoch: 0  batch: 467 / 721  loss: 0.12090273649776773  hr: 0  min: 22  sec: 28\n",
      "epoch: 0  batch: 468 / 721  loss: 0.12066488979686783  hr: 0  min: 22  sec: 27\n",
      "epoch: 0  batch: 469 / 721  loss: 0.12041750086793132  hr: 0  min: 22  sec: 26\n",
      "epoch: 0  batch: 470 / 721  loss: 0.12026004031538329  hr: 0  min: 22  sec: 25\n",
      "epoch: 0  batch: 471 / 721  loss: 0.12039432917858005  hr: 0  min: 22  sec: 24\n",
      "epoch: 0  batch: 472 / 721  loss: 0.12028979306046109  hr: 0  min: 22  sec: 23\n",
      "epoch: 0  batch: 473 / 721  loss: 0.12009977760781823  hr: 0  min: 22  sec: 22\n",
      "epoch: 0  batch: 474 / 721  loss: 0.11991323459547658  hr: 0  min: 22  sec: 22\n",
      "epoch: 0  batch: 475 / 721  loss: 0.11972371509200648  hr: 0  min: 22  sec: 21\n",
      "epoch: 0  batch: 476 / 721  loss: 0.11960661248499606  hr: 0  min: 22  sec: 20\n",
      "epoch: 0  batch: 477 / 721  loss: 0.11969355723393038  hr: 0  min: 22  sec: 19\n",
      "epoch: 0  batch: 478 / 721  loss: 0.11950973756455477  hr: 0  min: 22  sec: 18\n",
      "epoch: 0  batch: 479 / 721  loss: 0.11932921208176832  hr: 0  min: 22  sec: 17\n",
      "epoch: 0  batch: 480 / 721  loss: 0.11913652902003377  hr: 0  min: 22  sec: 16\n",
      "epoch: 0  batch: 481 / 721  loss: 0.11895349107125074  hr: 0  min: 22  sec: 15\n",
      "epoch: 0  batch: 482 / 721  loss: 0.11875168488387992  hr: 0  min: 22  sec: 14\n",
      "epoch: 0  batch: 483 / 721  loss: 0.1187835448724266  hr: 0  min: 22  sec: 13\n",
      "epoch: 0  batch: 484 / 721  loss: 0.11864120089872317  hr: 0  min: 22  sec: 12\n",
      "epoch: 0  batch: 485 / 721  loss: 0.11847024891794343  hr: 0  min: 22  sec: 11\n",
      "epoch: 0  batch: 486 / 721  loss: 0.11844155006877188  hr: 0  min: 22  sec: 10\n",
      "epoch: 0  batch: 487 / 721  loss: 0.11824792695042412  hr: 0  min: 22  sec: 10\n",
      "epoch: 0  batch: 488 / 721  loss: 0.11808112958736229  hr: 0  min: 22  sec: 9\n",
      "epoch: 0  batch: 489 / 721  loss: 0.1178680525269885  hr: 0  min: 22  sec: 8\n",
      "epoch: 0  batch: 490 / 721  loss: 0.11778761923351154  hr: 0  min: 22  sec: 7\n",
      "epoch: 0  batch: 491 / 721  loss: 0.11759767746799407  hr: 0  min: 22  sec: 7\n",
      "epoch: 0  batch: 492 / 721  loss: 0.1173732276614074  hr: 0  min: 22  sec: 5\n",
      "epoch: 0  batch: 493 / 721  loss: 0.11726890921188447  hr: 0  min: 22  sec: 4\n",
      "epoch: 0  batch: 494 / 721  loss: 0.11772468151913028  hr: 0  min: 22  sec: 3\n",
      "epoch: 0  batch: 495 / 721  loss: 0.11750727570098307  hr: 0  min: 22  sec: 3\n",
      "epoch: 0  batch: 496 / 721  loss: 0.11728900604066439  hr: 0  min: 22  sec: 2\n",
      "epoch: 0  batch: 497 / 721  loss: 0.11714712073052043  hr: 0  min: 22  sec: 1\n",
      "epoch: 0  batch: 498 / 721  loss: 0.11719110377607904  hr: 0  min: 22  sec: 0\n",
      "epoch: 0  batch: 499 / 721  loss: 0.11699201948175539  hr: 0  min: 22  sec: 0\n",
      "epoch: 0  batch: 500 / 721  loss: 0.11679416549112648  hr: 0  min: 21  sec: 59\n",
      "epoch: 0  batch: 501 / 721  loss: 0.11661681504882947  hr: 0  min: 21  sec: 58\n",
      "epoch: 0  batch: 502 / 721  loss: 0.11641497762254124  hr: 0  min: 21  sec: 56\n",
      "epoch: 0  batch: 503 / 721  loss: 0.1162499206253581  hr: 0  min: 21  sec: 56\n",
      "epoch: 0  batch: 504 / 721  loss: 0.11611465357949897  hr: 0  min: 21  sec: 55\n",
      "epoch: 0  batch: 505 / 721  loss: 0.11592883856708903  hr: 0  min: 21  sec: 54\n",
      "epoch: 0  batch: 506 / 721  loss: 0.11588213305180459  hr: 0  min: 21  sec: 53\n",
      "epoch: 0  batch: 507 / 721  loss: 0.11568558910361967  hr: 0  min: 21  sec: 52\n",
      "epoch: 0  batch: 508 / 721  loss: 0.11547356377095425  hr: 0  min: 21  sec: 51\n",
      "epoch: 0  batch: 509 / 721  loss: 0.1153025345168715  hr: 0  min: 21  sec: 50\n",
      "epoch: 0  batch: 510 / 721  loss: 0.11510350511628477  hr: 0  min: 21  sec: 50\n",
      "epoch: 0  batch: 511 / 721  loss: 0.11499272003576684  hr: 0  min: 21  sec: 49\n",
      "epoch: 0  batch: 512 / 721  loss: 0.1149228899694208  hr: 0  min: 21  sec: 48\n",
      "epoch: 0  batch: 513 / 721  loss: 0.11493436425296158  hr: 0  min: 21  sec: 48\n",
      "epoch: 0  batch: 514 / 721  loss: 0.11484142586657713  hr: 0  min: 21  sec: 47\n",
      "epoch: 0  batch: 515 / 721  loss: 0.11470112924563508  hr: 0  min: 21  sec: 46\n",
      "epoch: 0  batch: 516 / 721  loss: 0.1145082962430306  hr: 0  min: 21  sec: 45\n",
      "epoch: 0  batch: 517 / 721  loss: 0.11433623721318704  hr: 0  min: 21  sec: 44\n",
      "epoch: 0  batch: 518 / 721  loss: 0.11413584632752283  hr: 0  min: 21  sec: 43\n",
      "epoch: 0  batch: 519 / 721  loss: 0.11392708286653795  hr: 0  min: 21  sec: 42\n",
      "epoch: 0  batch: 520 / 721  loss: 0.11374620530808058  hr: 0  min: 21  sec: 42\n",
      "epoch: 0  batch: 521 / 721  loss: 0.1135755701807476  hr: 0  min: 21  sec: 41\n",
      "epoch: 0  batch: 522 / 721  loss: 0.11353828409290605  hr: 0  min: 21  sec: 40\n",
      "epoch: 0  batch: 523 / 721  loss: 0.11349737740622168  hr: 0  min: 21  sec: 39\n",
      "epoch: 0  batch: 524 / 721  loss: 0.11336230759185432  hr: 0  min: 21  sec: 38\n",
      "epoch: 0  batch: 525 / 721  loss: 0.11317034691483492  hr: 0  min: 21  sec: 38\n",
      "epoch: 0  batch: 526 / 721  loss: 0.11302215202194514  hr: 0  min: 21  sec: 37\n",
      "epoch: 0  batch: 527 / 721  loss: 0.11293348227406673  hr: 0  min: 21  sec: 36\n",
      "epoch: 0  batch: 528 / 721  loss: 0.11280622169513001  hr: 0  min: 21  sec: 36\n",
      "epoch: 0  batch: 529 / 721  loss: 0.11282654489621181  hr: 0  min: 21  sec: 35\n",
      "epoch: 0  batch: 530 / 721  loss: 0.11279350718711767  hr: 0  min: 21  sec: 34\n",
      "epoch: 0  batch: 531 / 721  loss: 0.11276969066584262  hr: 0  min: 21  sec: 33\n",
      "epoch: 0  batch: 532 / 721  loss: 0.11285791728081868  hr: 0  min: 21  sec: 32\n",
      "epoch: 0  batch: 533 / 721  loss: 0.11275682029337054  hr: 0  min: 21  sec: 31\n",
      "epoch: 0  batch: 534 / 721  loss: 0.11268202889246748  hr: 0  min: 21  sec: 30\n",
      "epoch: 0  batch: 535 / 721  loss: 0.11250846737508323  hr: 0  min: 21  sec: 29\n",
      "epoch: 0  batch: 536 / 721  loss: 0.11237470216878842  hr: 0  min: 21  sec: 28\n",
      "epoch: 0  batch: 537 / 721  loss: 0.11228457719583612  hr: 0  min: 21  sec: 27\n",
      "epoch: 0  batch: 538 / 721  loss: 0.11211370048286276  hr: 0  min: 21  sec: 27\n",
      "epoch: 0  batch: 539 / 721  loss: 0.11197265776233654  hr: 0  min: 21  sec: 26\n",
      "epoch: 0  batch: 540 / 721  loss: 0.11179794653198095  hr: 0  min: 21  sec: 25\n",
      "epoch: 0  batch: 541 / 721  loss: 0.11162618094497462  hr: 0  min: 21  sec: 25\n",
      "epoch: 0  batch: 542 / 721  loss: 0.11145130602893767  hr: 0  min: 21  sec: 24\n",
      "epoch: 0  batch: 543 / 721  loss: 0.1112731274767928  hr: 0  min: 21  sec: 23\n",
      "epoch: 0  batch: 544 / 721  loss: 0.11113548926026423  hr: 0  min: 21  sec: 22\n",
      "epoch: 0  batch: 545 / 721  loss: 0.11094600941832049  hr: 0  min: 21  sec: 21\n",
      "epoch: 0  batch: 546 / 721  loss: 0.11168819115388005  hr: 0  min: 21  sec: 20\n",
      "epoch: 0  batch: 547 / 721  loss: 0.11150734824889512  hr: 0  min: 21  sec: 19\n",
      "epoch: 0  batch: 548 / 721  loss: 0.11134551743463525  hr: 0  min: 21  sec: 19\n",
      "epoch: 0  batch: 549 / 721  loss: 0.11117714902263928  hr: 0  min: 21  sec: 18\n",
      "epoch: 0  batch: 550 / 721  loss: 0.11105120969343592  hr: 0  min: 21  sec: 17\n",
      "epoch: 0  batch: 551 / 721  loss: 0.11091201770256279  hr: 0  min: 21  sec: 16\n",
      "epoch: 0  batch: 552 / 721  loss: 0.11076012733847955  hr: 0  min: 21  sec: 15\n",
      "epoch: 0  batch: 553 / 721  loss: 0.11063058727132881  hr: 0  min: 21  sec: 14\n",
      "epoch: 0  batch: 554 / 721  loss: 0.11064262103393593  hr: 0  min: 21  sec: 13\n",
      "epoch: 0  batch: 555 / 721  loss: 0.11046653912226494  hr: 0  min: 21  sec: 12\n",
      "epoch: 0  batch: 556 / 721  loss: 0.11037313753300139  hr: 0  min: 21  sec: 12\n",
      "epoch: 0  batch: 557 / 721  loss: 0.11024627206626363  hr: 0  min: 21  sec: 11\n",
      "epoch: 0  batch: 558 / 721  loss: 0.11021237173551647  hr: 0  min: 21  sec: 10\n",
      "epoch: 0  batch: 559 / 721  loss: 0.11009405525434555  hr: 0  min: 21  sec: 9\n",
      "epoch: 0  batch: 560 / 721  loss: 0.1099425247878701  hr: 0  min: 21  sec: 10\n",
      "epoch: 0  batch: 561 / 721  loss: 0.1098362781886018  hr: 0  min: 21  sec: 8\n",
      "epoch: 0  batch: 562 / 721  loss: 0.10983359787921385  hr: 0  min: 21  sec: 8\n",
      "epoch: 0  batch: 563 / 721  loss: 0.10966441424920838  hr: 0  min: 21  sec: 7\n",
      "epoch: 0  batch: 564 / 721  loss: 0.10952330927499933  hr: 0  min: 21  sec: 6\n",
      "epoch: 0  batch: 565 / 721  loss: 0.1094298978550442  hr: 0  min: 21  sec: 5\n",
      "epoch: 0  batch: 566 / 721  loss: 0.10930686479503479  hr: 0  min: 21  sec: 4\n",
      "epoch: 0  batch: 567 / 721  loss: 0.10915496430197169  hr: 0  min: 21  sec: 4\n",
      "epoch: 0  batch: 568 / 721  loss: 0.10902382445793835  hr: 0  min: 21  sec: 3\n",
      "epoch: 0  batch: 569 / 721  loss: 0.10897037426749595  hr: 0  min: 21  sec: 2\n",
      "epoch: 0  batch: 570 / 721  loss: 0.10883524534782689  hr: 0  min: 21  sec: 1\n",
      "epoch: 0  batch: 571 / 721  loss: 0.10881748565125252  hr: 0  min: 21  sec: 0\n",
      "epoch: 0  batch: 572 / 721  loss: 0.1086753087266433  hr: 0  min: 20  sec: 59\n",
      "epoch: 0  batch: 573 / 721  loss: 0.10849977208197403  hr: 0  min: 20  sec: 59\n",
      "epoch: 0  batch: 574 / 721  loss: 0.10849369194225542  hr: 0  min: 20  sec: 57\n",
      "epoch: 0  batch: 575 / 721  loss: 0.10836455623983689  hr: 0  min: 20  sec: 57\n",
      "epoch: 0  batch: 576 / 721  loss: 0.10822549397239022  hr: 0  min: 20  sec: 56\n",
      "epoch: 0  batch: 577 / 721  loss: 0.10804523041139338  hr: 0  min: 20  sec: 56\n",
      "epoch: 0  batch: 578 / 721  loss: 0.10788781396322802  hr: 0  min: 20  sec: 55\n",
      "epoch: 0  batch: 579 / 721  loss: 0.1080181608121321  hr: 0  min: 20  sec: 54\n",
      "epoch: 0  batch: 580 / 721  loss: 0.10798824114146932  hr: 0  min: 20  sec: 53\n",
      "epoch: 0  batch: 581 / 721  loss: 0.10783588775015227  hr: 0  min: 20  sec: 52\n",
      "epoch: 0  batch: 582 / 721  loss: 0.10766411760399483  hr: 0  min: 20  sec: 51\n",
      "epoch: 0  batch: 583 / 721  loss: 0.10753701629331192  hr: 0  min: 20  sec: 50\n",
      "epoch: 0  batch: 584 / 721  loss: 0.10740195633684069  hr: 0  min: 20  sec: 50\n",
      "epoch: 0  batch: 585 / 721  loss: 0.10729824056705603  hr: 0  min: 20  sec: 50\n",
      "epoch: 0  batch: 586 / 721  loss: 0.10716260990140636  hr: 0  min: 20  sec: 49\n",
      "epoch: 0  batch: 587 / 721  loss: 0.10698880647504273  hr: 0  min: 20  sec: 48\n",
      "epoch: 0  batch: 588 / 721  loss: 0.10681656640114859  hr: 0  min: 20  sec: 48\n",
      "epoch: 0  batch: 589 / 721  loss: 0.10680150280901071  hr: 0  min: 20  sec: 47\n",
      "epoch: 0  batch: 590 / 721  loss: 0.10686284931045088  hr: 0  min: 20  sec: 46\n",
      "epoch: 0  batch: 591 / 721  loss: 0.10669796129959665  hr: 0  min: 20  sec: 46\n",
      "epoch: 0  batch: 592 / 721  loss: 0.1065662210560215  hr: 0  min: 20  sec: 45\n",
      "epoch: 0  batch: 593 / 721  loss: 0.10640225506624801  hr: 0  min: 20  sec: 44\n",
      "epoch: 0  batch: 594 / 721  loss: 0.10631966009431892  hr: 0  min: 20  sec: 43\n",
      "epoch: 0  batch: 595 / 721  loss: 0.10619658024418129  hr: 0  min: 20  sec: 42\n",
      "epoch: 0  batch: 596 / 721  loss: 0.10606621413557296  hr: 0  min: 20  sec: 41\n",
      "epoch: 0  batch: 597 / 721  loss: 0.10610699821091951  hr: 0  min: 20  sec: 41\n",
      "epoch: 0  batch: 598 / 721  loss: 0.10601446527386564  hr: 0  min: 20  sec: 40\n",
      "epoch: 0  batch: 599 / 721  loss: 0.10586960152808408  hr: 0  min: 20  sec: 39\n",
      "epoch: 0  batch: 600 / 721  loss: 0.1057519656039464  hr: 0  min: 20  sec: 38\n",
      "epoch: 0  batch: 601 / 721  loss: 0.1055895308065174  hr: 0  min: 20  sec: 37\n",
      "epoch: 0  batch: 602 / 721  loss: 0.10545819367442985  hr: 0  min: 20  sec: 36\n",
      "epoch: 0  batch: 603 / 721  loss: 0.1053139493016103  hr: 0  min: 20  sec: 35\n",
      "epoch: 0  batch: 604 / 721  loss: 0.10517909545806045  hr: 0  min: 20  sec: 34\n",
      "epoch: 0  batch: 605 / 721  loss: 0.10517949858711155  hr: 0  min: 20  sec: 33\n",
      "epoch: 0  batch: 606 / 721  loss: 0.10503322795792652  hr: 0  min: 20  sec: 33\n",
      "epoch: 0  batch: 607 / 721  loss: 0.10501636765955577  hr: 0  min: 20  sec: 32\n",
      "epoch: 0  batch: 608 / 721  loss: 0.10488105979732752  hr: 0  min: 20  sec: 31\n",
      "epoch: 0  batch: 609 / 721  loss: 0.10474052911086011  hr: 0  min: 20  sec: 30\n",
      "epoch: 0  batch: 610 / 721  loss: 0.10499627964769596  hr: 0  min: 20  sec: 29\n",
      "epoch: 0  batch: 611 / 721  loss: 0.10487682919464579  hr: 0  min: 20  sec: 28\n",
      "epoch: 0  batch: 612 / 721  loss: 0.10475249336408513  hr: 0  min: 20  sec: 27\n",
      "epoch: 0  batch: 613 / 721  loss: 0.10463477341081419  hr: 0  min: 20  sec: 27\n",
      "epoch: 0  batch: 614 / 721  loss: 0.10448039148278923  hr: 0  min: 20  sec: 26\n",
      "epoch: 0  batch: 615 / 721  loss: 0.10439369840153713  hr: 0  min: 20  sec: 25\n",
      "epoch: 0  batch: 616 / 721  loss: 0.10426555143883935  hr: 0  min: 20  sec: 24\n",
      "epoch: 0  batch: 617 / 721  loss: 0.10410663670961363  hr: 0  min: 20  sec: 24\n",
      "epoch: 0  batch: 618 / 721  loss: 0.10396518064647745  hr: 0  min: 20  sec: 23\n",
      "epoch: 0  batch: 619 / 721  loss: 0.10381193392306735  hr: 0  min: 20  sec: 22\n",
      "epoch: 0  batch: 620 / 721  loss: 0.10371830411210296  hr: 0  min: 20  sec: 21\n",
      "epoch: 0  batch: 621 / 721  loss: 0.10369554073917236  hr: 0  min: 20  sec: 21\n",
      "epoch: 0  batch: 622 / 721  loss: 0.10353861911330145  hr: 0  min: 20  sec: 20\n",
      "epoch: 0  batch: 623 / 721  loss: 0.10344271317429327  hr: 0  min: 20  sec: 19\n",
      "epoch: 0  batch: 624 / 721  loss: 0.10333059042041452  hr: 0  min: 20  sec: 18\n",
      "epoch: 0  batch: 625 / 721  loss: 0.10324915076345205  hr: 0  min: 20  sec: 17\n",
      "epoch: 0  batch: 626 / 721  loss: 0.10315429268571467  hr: 0  min: 20  sec: 16\n",
      "epoch: 0  batch: 627 / 721  loss: 0.1030513951733138  hr: 0  min: 20  sec: 16\n",
      "epoch: 0  batch: 628 / 721  loss: 0.10303991528637232  hr: 0  min: 20  sec: 15\n",
      "epoch: 0  batch: 629 / 721  loss: 0.10288392503587425  hr: 0  min: 20  sec: 14\n",
      "epoch: 0  batch: 630 / 721  loss: 0.10279545455151015  hr: 0  min: 20  sec: 13\n",
      "epoch: 0  batch: 631 / 721  loss: 0.10270614292629052  hr: 0  min: 20  sec: 13\n",
      "epoch: 0  batch: 632 / 721  loss: 0.10273324693420524  hr: 0  min: 20  sec: 12\n",
      "epoch: 0  batch: 633 / 721  loss: 0.10262344769859837  hr: 0  min: 20  sec: 11\n",
      "epoch: 0  batch: 634 / 721  loss: 0.10247372653209003  hr: 0  min: 20  sec: 10\n",
      "epoch: 0  batch: 635 / 721  loss: 0.10240865676612483  hr: 0  min: 20  sec: 9\n",
      "epoch: 0  batch: 636 / 721  loss: 0.10226672795475256  hr: 0  min: 20  sec: 9\n",
      "epoch: 0  batch: 637 / 721  loss: 0.10215023726011441  hr: 0  min: 20  sec: 8\n",
      "epoch: 0  batch: 638 / 721  loss: 0.10205464036974671  hr: 0  min: 20  sec: 7\n",
      "epoch: 0  batch: 639 / 721  loss: 0.10195941550814559  hr: 0  min: 20  sec: 6\n",
      "epoch: 0  batch: 640 / 721  loss: 0.10201956372984569  hr: 0  min: 20  sec: 5\n",
      "epoch: 0  batch: 641 / 721  loss: 0.10192044807435267  hr: 0  min: 20  sec: 4\n",
      "epoch: 0  batch: 642 / 721  loss: 0.10179172218284016  hr: 0  min: 20  sec: 3\n",
      "epoch: 0  batch: 643 / 721  loss: 0.1016870729826553  hr: 0  min: 20  sec: 2\n",
      "epoch: 0  batch: 644 / 721  loss: 0.10166758280869295  hr: 0  min: 20  sec: 1\n",
      "epoch: 0  batch: 645 / 721  loss: 0.1015281045989877  hr: 0  min: 20  sec: 1\n",
      "epoch: 0  batch: 646 / 721  loss: 0.1014902420571195  hr: 0  min: 20  sec: 0\n",
      "epoch: 0  batch: 647 / 721  loss: 0.10152972272166859  hr: 0  min: 19  sec: 59\n",
      "epoch: 0  batch: 648 / 721  loss: 0.10137950777461362  hr: 0  min: 19  sec: 58\n",
      "epoch: 0  batch: 649 / 721  loss: 0.10138645817673492  hr: 0  min: 19  sec: 57\n",
      "epoch: 0  batch: 650 / 721  loss: 0.10128978889865371  hr: 0  min: 19  sec: 56\n",
      "epoch: 0  batch: 651 / 721  loss: 0.10115906249834783  hr: 0  min: 19  sec: 56\n",
      "epoch: 0  batch: 652 / 721  loss: 0.10104956702569191  hr: 0  min: 19  sec: 55\n",
      "epoch: 0  batch: 653 / 721  loss: 0.1009142679842631  hr: 0  min: 19  sec: 54\n",
      "epoch: 0  batch: 654 / 721  loss: 0.10078945854644573  hr: 0  min: 19  sec: 54\n",
      "epoch: 0  batch: 655 / 721  loss: 0.10065914370148009  hr: 0  min: 19  sec: 53\n",
      "epoch: 0  batch: 656 / 721  loss: 0.10053777670425294  hr: 0  min: 19  sec: 52\n",
      "epoch: 0  batch: 657 / 721  loss: 0.10047153630853833  hr: 0  min: 19  sec: 52\n",
      "epoch: 0  batch: 658 / 721  loss: 0.10032904608172327  hr: 0  min: 19  sec: 51\n",
      "epoch: 0  batch: 659 / 721  loss: 0.10020216550799553  hr: 0  min: 19  sec: 51\n",
      "epoch: 0  batch: 660 / 721  loss: 0.1000699472112694  hr: 0  min: 19  sec: 50\n",
      "epoch: 0  batch: 661 / 721  loss: 0.09994132171558293  hr: 0  min: 19  sec: 50\n",
      "epoch: 0  batch: 662 / 721  loss: 0.09981768466940212  hr: 0  min: 19  sec: 50\n",
      "epoch: 0  batch: 663 / 721  loss: 0.0997952280030902  hr: 0  min: 19  sec: 49\n",
      "epoch: 0  batch: 664 / 721  loss: 0.09968265347219218  hr: 0  min: 19  sec: 48\n",
      "epoch: 0  batch: 665 / 721  loss: 0.09959235762533053  hr: 0  min: 19  sec: 48\n",
      "epoch: 0  batch: 666 / 721  loss: 0.09945473472962531  hr: 0  min: 19  sec: 47\n",
      "epoch: 0  batch: 667 / 721  loss: 0.09938883277443775  hr: 0  min: 19  sec: 46\n",
      "epoch: 0  batch: 668 / 721  loss: 0.09926892748774548  hr: 0  min: 19  sec: 45\n",
      "epoch: 0  batch: 669 / 721  loss: 0.09914235508219744  hr: 0  min: 19  sec: 45\n",
      "epoch: 0  batch: 670 / 721  loss: 0.09900570269465557  hr: 0  min: 19  sec: 44\n",
      "epoch: 0  batch: 671 / 721  loss: 0.09886522420672297  hr: 0  min: 19  sec: 43\n",
      "epoch: 0  batch: 672 / 721  loss: 0.09877411244503621  hr: 0  min: 19  sec: 42\n",
      "epoch: 0  batch: 673 / 721  loss: 0.09873353398196057  hr: 0  min: 19  sec: 42\n",
      "epoch: 0  batch: 674 / 721  loss: 0.09861946347216559  hr: 0  min: 19  sec: 41\n",
      "epoch: 0  batch: 675 / 721  loss: 0.09851382466378035  hr: 0  min: 19  sec: 40\n",
      "epoch: 0  batch: 676 / 721  loss: 0.09842383694013901  hr: 0  min: 19  sec: 39\n",
      "epoch: 0  batch: 677 / 721  loss: 0.09833646505696911  hr: 0  min: 19  sec: 39\n",
      "epoch: 0  batch: 678 / 721  loss: 0.09832097864146605  hr: 0  min: 19  sec: 38\n",
      "epoch: 0  batch: 679 / 721  loss: 0.09820803337139304  hr: 0  min: 19  sec: 37\n",
      "epoch: 0  batch: 680 / 721  loss: 0.09832947116126033  hr: 0  min: 19  sec: 36\n",
      "epoch: 0  batch: 681 / 721  loss: 0.09821275670611998  hr: 0  min: 19  sec: 35\n",
      "epoch: 0  batch: 682 / 721  loss: 0.09811219046454975  hr: 0  min: 19  sec: 34\n",
      "epoch: 0  batch: 683 / 721  loss: 0.09803955170760915  hr: 0  min: 19  sec: 33\n",
      "epoch: 0  batch: 684 / 721  loss: 0.0979666250937602  hr: 0  min: 19  sec: 33\n",
      "epoch: 0  batch: 685 / 721  loss: 0.09786692405541013  hr: 0  min: 19  sec: 32\n",
      "epoch: 0  batch: 686 / 721  loss: 0.09773446435282555  hr: 0  min: 19  sec: 31\n",
      "epoch: 0  batch: 687 / 721  loss: 0.0976125045857098  hr: 0  min: 19  sec: 30\n",
      "epoch: 0  batch: 688 / 721  loss: 0.09748773066175365  hr: 0  min: 19  sec: 29\n",
      "epoch: 0  batch: 689 / 721  loss: 0.09735644542975744  hr: 0  min: 19  sec: 29\n",
      "epoch: 0  batch: 690 / 721  loss: 0.09777519176174225  hr: 0  min: 19  sec: 28\n",
      "epoch: 0  batch: 691 / 721  loss: 0.09767054065768155  hr: 0  min: 19  sec: 27\n",
      "epoch: 0  batch: 692 / 721  loss: 0.09757217327388532  hr: 0  min: 19  sec: 26\n",
      "epoch: 0  batch: 693 / 721  loss: 0.09743806720183139  hr: 0  min: 19  sec: 25\n",
      "epoch: 0  batch: 694 / 721  loss: 0.09735062058548033  hr: 0  min: 19  sec: 25\n",
      "epoch: 0  batch: 695 / 721  loss: 0.09728948663544955  hr: 0  min: 19  sec: 24\n",
      "epoch: 0  batch: 696 / 721  loss: 0.09730741307647878  hr: 0  min: 19  sec: 23\n",
      "epoch: 0  batch: 697 / 721  loss: 0.09719378020817486  hr: 0  min: 19  sec: 22\n",
      "epoch: 0  batch: 698 / 721  loss: 0.0970899695223651  hr: 0  min: 19  sec: 21\n",
      "epoch: 0  batch: 699 / 721  loss: 0.09699135731628566  hr: 0  min: 19  sec: 20\n",
      "epoch: 0  batch: 700 / 721  loss: 0.09688531662337482  hr: 0  min: 19  sec: 19\n",
      "epoch: 0  batch: 701 / 721  loss: 0.09676435214072764  hr: 0  min: 19  sec: 18\n",
      "epoch: 0  batch: 702 / 721  loss: 0.09665040378631265  hr: 0  min: 19  sec: 17\n",
      "epoch: 0  batch: 703 / 721  loss: 0.09653776124183348  hr: 0  min: 19  sec: 16\n",
      "epoch: 0  batch: 704 / 721  loss: 0.09643606574073518  hr: 0  min: 19  sec: 15\n",
      "epoch: 0  batch: 705 / 721  loss: 0.09636522346830115  hr: 0  min: 19  sec: 14\n",
      "epoch: 0  batch: 706 / 721  loss: 0.09626144895768031  hr: 0  min: 19  sec: 14\n",
      "epoch: 0  batch: 707 / 721  loss: 0.09614399117924789  hr: 0  min: 19  sec: 13\n",
      "epoch: 0  batch: 708 / 721  loss: 0.0960317713255852  hr: 0  min: 19  sec: 12\n",
      "epoch: 0  batch: 709 / 721  loss: 0.09600948508203618  hr: 0  min: 19  sec: 11\n",
      "epoch: 0  batch: 710 / 721  loss: 0.09605731165010324  hr: 0  min: 19  sec: 10\n",
      "epoch: 0  batch: 711 / 721  loss: 0.09593907090456719  hr: 0  min: 19  sec: 10\n",
      "epoch: 0  batch: 712 / 721  loss: 0.09581091285753848  hr: 0  min: 19  sec: 9\n",
      "epoch: 0  batch: 713 / 721  loss: 0.09569061452230285  hr: 0  min: 19  sec: 8\n",
      "epoch: 0  batch: 714 / 721  loss: 0.095561106475031  hr: 0  min: 19  sec: 7\n",
      "epoch: 0  batch: 715 / 721  loss: 0.09543785690134586  hr: 0  min: 19  sec: 6\n",
      "epoch: 0  batch: 716 / 721  loss: 0.09534545085885869  hr: 0  min: 19  sec: 6\n",
      "epoch: 0  batch: 717 / 721  loss: 0.09523292628800135  hr: 0  min: 19  sec: 5\n",
      "epoch: 0  batch: 718 / 721  loss: 0.09511679548914168  hr: 0  min: 19  sec: 4\n",
      "epoch: 0  batch: 719 / 721  loss: 0.09500803344530702  hr: 0  min: 19  sec: 3\n",
      "epoch: 0  batch: 720 / 721  loss: 0.09492905678505648  hr: 0  min: 19  sec: 2\n",
      "epoch: 0  batch: 721 / 721  loss: 0.09490225431522233  hr: 0  min: 19  sec: 1\n",
      "epoch: 1  batch: 1 / 721  loss: 0.03740692138671875  hr: 0  min: 18  sec: 18\n",
      "epoch: 1  batch: 2 / 721  loss: 0.024871534202247858  hr: 0  min: 18  sec: 17\n",
      "epoch: 1  batch: 3 / 721  loss: 0.02232531923800707  hr: 0  min: 17  sec: 42\n",
      "epoch: 1  batch: 4 / 721  loss: 0.01791053090710193  hr: 0  min: 17  sec: 41\n",
      "epoch: 1  batch: 5 / 721  loss: 0.016032261680811642  hr: 0  min: 17  sec: 45\n",
      "epoch: 1  batch: 6 / 721  loss: 0.01430588262155652  hr: 0  min: 17  sec: 39\n",
      "epoch: 1  batch: 7 / 721  loss: 0.016423979374979223  hr: 0  min: 17  sec: 39\n",
      "epoch: 1  batch: 8 / 721  loss: 0.02259706088807434  hr: 0  min: 17  sec: 20\n",
      "epoch: 1  batch: 9 / 721  loss: 0.021723618420461815  hr: 0  min: 17  sec: 26\n",
      "epoch: 1  batch: 10 / 721  loss: 0.023532964568585158  hr: 0  min: 17  sec: 32\n",
      "epoch: 1  batch: 11 / 721  loss: 0.021899975920942696  hr: 0  min: 17  sec: 25\n",
      "epoch: 1  batch: 12 / 721  loss: 0.026567629305645823  hr: 0  min: 17  sec: 38\n",
      "epoch: 1  batch: 13 / 721  loss: 0.025566988481351964  hr: 0  min: 17  sec: 31\n",
      "epoch: 1  batch: 14 / 721  loss: 0.03084048968074577  hr: 0  min: 17  sec: 32\n",
      "epoch: 1  batch: 15 / 721  loss: 0.02913181611026327  hr: 0  min: 17  sec: 23\n",
      "epoch: 1  batch: 16 / 721  loss: 0.02782032868708484  hr: 0  min: 17  sec: 23\n",
      "epoch: 1  batch: 17 / 721  loss: 0.026872706298223314  hr: 0  min: 17  sec: 33\n",
      "epoch: 1  batch: 18 / 721  loss: 0.025538527859478362  hr: 0  min: 17  sec: 28\n",
      "epoch: 1  batch: 19 / 721  loss: 0.02463890596194879  hr: 0  min: 17  sec: 32\n",
      "epoch: 1  batch: 20 / 721  loss: 0.023722787818405776  hr: 0  min: 17  sec: 25\n",
      "epoch: 1  batch: 21 / 721  loss: 0.025480399407180294  hr: 0  min: 17  sec: 25\n",
      "epoch: 1  batch: 22 / 721  loss: 0.024455798197198998  hr: 0  min: 17  sec: 25\n",
      "epoch: 1  batch: 23 / 721  loss: 0.023705228231847286  hr: 0  min: 17  sec: 25\n",
      "epoch: 1  batch: 24 / 721  loss: 0.022849572754542653  hr: 0  min: 17  sec: 31\n",
      "epoch: 1  batch: 25 / 721  loss: 0.022069282038137315  hr: 0  min: 17  sec: 29\n",
      "epoch: 1  batch: 26 / 721  loss: 0.021885143763099153  hr: 0  min: 17  sec: 36\n",
      "epoch: 1  batch: 27 / 721  loss: 0.021610032447993203  hr: 0  min: 17  sec: 38\n",
      "epoch: 1  batch: 28 / 721  loss: 0.021381773545207188  hr: 0  min: 17  sec: 48\n",
      "epoch: 1  batch: 29 / 721  loss: 0.020707083898114747  hr: 0  min: 17  sec: 43\n",
      "epoch: 1  batch: 30 / 721  loss: 0.020132563496008515  hr: 0  min: 17  sec: 47\n",
      "epoch: 1  batch: 31 / 721  loss: 0.019578493853670456  hr: 0  min: 17  sec: 50\n",
      "epoch: 1  batch: 32 / 721  loss: 0.019103360296867322  hr: 0  min: 17  sec: 48\n",
      "epoch: 1  batch: 33 / 721  loss: 0.018596606110363748  hr: 0  min: 18  sec: 2\n",
      "epoch: 1  batch: 34 / 721  loss: 0.018677714178064728  hr: 0  min: 17  sec: 59\n",
      "epoch: 1  batch: 35 / 721  loss: 0.02297999055923096  hr: 0  min: 18  sec: 4\n",
      "epoch: 1  batch: 36 / 721  loss: 0.02244488210029279  hr: 0  min: 18  sec: 2\n",
      "epoch: 1  batch: 37 / 721  loss: 0.022181820709610712  hr: 0  min: 18  sec: 3\n",
      "epoch: 1  batch: 38 / 721  loss: 0.021659058144953298  hr: 0  min: 18  sec: 2\n",
      "epoch: 1  batch: 39 / 721  loss: 0.021307213560272105  hr: 0  min: 17  sec: 57\n",
      "epoch: 1  batch: 40 / 721  loss: 0.020984124479582533  hr: 0  min: 17  sec: 53\n",
      "epoch: 1  batch: 41 / 721  loss: 0.021000591896065488  hr: 0  min: 17  sec: 50\n",
      "epoch: 1  batch: 42 / 721  loss: 0.02067874842655978  hr: 0  min: 17  sec: 49\n",
      "epoch: 1  batch: 43 / 721  loss: 0.02041755913930057  hr: 0  min: 17  sec: 51\n",
      "epoch: 1  batch: 44 / 721  loss: 0.020158952297854492  hr: 0  min: 17  sec: 48\n",
      "epoch: 1  batch: 45 / 721  loss: 0.019791546903757586  hr: 0  min: 17  sec: 49\n",
      "epoch: 1  batch: 46 / 721  loss: 0.01986871022508358  hr: 0  min: 17  sec: 52\n",
      "epoch: 1  batch: 47 / 721  loss: 0.019523220325007716  hr: 0  min: 17  sec: 52\n",
      "epoch: 1  batch: 48 / 721  loss: 0.019322650477988645  hr: 0  min: 17  sec: 50\n",
      "epoch: 1  batch: 49 / 721  loss: 0.019281925947158312  hr: 0  min: 17  sec: 50\n",
      "epoch: 1  batch: 50 / 721  loss: 0.018951395507901907  hr: 0  min: 17  sec: 49\n",
      "epoch: 1  batch: 51 / 721  loss: 0.018641495912828866  hr: 0  min: 17  sec: 47\n",
      "epoch: 1  batch: 52 / 721  loss: 0.018442093233506266  hr: 0  min: 17  sec: 47\n",
      "epoch: 1  batch: 53 / 721  loss: 0.018298830749150716  hr: 0  min: 17  sec: 44\n",
      "epoch: 1  batch: 54 / 721  loss: 0.018126529365502023  hr: 0  min: 17  sec: 44\n",
      "epoch: 1  batch: 55 / 721  loss: 0.018332983113147995  hr: 0  min: 17  sec: 45\n",
      "epoch: 1  batch: 56 / 721  loss: 0.018063543780174638  hr: 0  min: 17  sec: 42\n",
      "epoch: 1  batch: 57 / 721  loss: 0.01785963975513975  hr: 0  min: 17  sec: 45\n",
      "epoch: 1  batch: 58 / 721  loss: 0.017698169228267568  hr: 0  min: 17  sec: 42\n",
      "epoch: 1  batch: 59 / 721  loss: 0.01847314805392239  hr: 0  min: 17  sec: 39\n",
      "epoch: 1  batch: 60 / 721  loss: 0.018675033492036162  hr: 0  min: 17  sec: 37\n",
      "epoch: 1  batch: 61 / 721  loss: 0.018468860345968945  hr: 0  min: 17  sec: 35\n",
      "epoch: 1  batch: 62 / 721  loss: 0.01820457319263369  hr: 0  min: 17  sec: 34\n",
      "epoch: 1  batch: 63 / 721  loss: 0.01922515863626604  hr: 0  min: 17  sec: 31\n",
      "epoch: 1  batch: 64 / 721  loss: 0.01976521779943141  hr: 0  min: 17  sec: 32\n",
      "epoch: 1  batch: 65 / 721  loss: 0.019541792302893903  hr: 0  min: 17  sec: 31\n",
      "epoch: 1  batch: 66 / 721  loss: 0.019269085681475135  hr: 0  min: 17  sec: 29\n",
      "epoch: 1  batch: 67 / 721  loss: 0.021749406547369017  hr: 0  min: 17  sec: 28\n",
      "epoch: 1  batch: 68 / 721  loss: 0.021557721386403394  hr: 0  min: 17  sec: 27\n",
      "epoch: 1  batch: 69 / 721  loss: 0.021448330408758553  hr: 0  min: 17  sec: 26\n",
      "epoch: 1  batch: 70 / 721  loss: 0.022224095911120196  hr: 0  min: 17  sec: 24\n",
      "epoch: 1  batch: 71 / 721  loss: 0.023037266899490545  hr: 0  min: 17  sec: 23\n",
      "epoch: 1  batch: 72 / 721  loss: 0.022982199309303217  hr: 0  min: 17  sec: 30\n",
      "epoch: 1  batch: 73 / 721  loss: 0.02286615377859761  hr: 0  min: 17  sec: 28\n",
      "epoch: 1  batch: 74 / 721  loss: 0.022838241139087022  hr: 0  min: 17  sec: 29\n",
      "epoch: 1  batch: 75 / 721  loss: 0.022567580377993484  hr: 0  min: 17  sec: 30\n",
      "epoch: 1  batch: 76 / 721  loss: 0.02240967978325084  hr: 0  min: 17  sec: 33\n",
      "epoch: 1  batch: 77 / 721  loss: 0.022459912221189347  hr: 0  min: 17  sec: 33\n",
      "epoch: 1  batch: 78 / 721  loss: 0.02220042553007937  hr: 0  min: 17  sec: 33\n",
      "epoch: 1  batch: 79 / 721  loss: 0.022232601906856688  hr: 0  min: 17  sec: 31\n",
      "epoch: 1  batch: 80 / 721  loss: 0.021986240449768955  hr: 0  min: 17  sec: 30\n",
      "epoch: 1  batch: 81 / 721  loss: 0.021745841005920537  hr: 0  min: 17  sec: 29\n",
      "epoch: 1  batch: 82 / 721  loss: 0.021595403050901595  hr: 0  min: 17  sec: 29\n",
      "epoch: 1  batch: 83 / 721  loss: 0.02139964931874525  hr: 0  min: 17  sec: 27\n",
      "epoch: 1  batch: 84 / 721  loss: 0.022984229370541426  hr: 0  min: 17  sec: 25\n",
      "epoch: 1  batch: 85 / 721  loss: 0.022770322978441768  hr: 0  min: 17  sec: 24\n",
      "epoch: 1  batch: 86 / 721  loss: 0.022858333220261387  hr: 0  min: 17  sec: 22\n",
      "epoch: 1  batch: 87 / 721  loss: 0.023157315851917125  hr: 0  min: 17  sec: 23\n",
      "epoch: 1  batch: 88 / 721  loss: 0.02296134197571188  hr: 0  min: 17  sec: 24\n",
      "epoch: 1  batch: 89 / 721  loss: 0.025304609751381147  hr: 0  min: 17  sec: 22\n",
      "epoch: 1  batch: 90 / 721  loss: 0.025138393426055297  hr: 0  min: 17  sec: 22\n",
      "epoch: 1  batch: 91 / 721  loss: 0.02507672521188487  hr: 0  min: 17  sec: 21\n",
      "epoch: 1  batch: 92 / 721  loss: 0.025020297767300886  hr: 0  min: 17  sec: 20\n",
      "epoch: 1  batch: 93 / 721  loss: 0.024867226428512523  hr: 0  min: 17  sec: 21\n",
      "epoch: 1  batch: 94 / 721  loss: 0.024761127802156942  hr: 0  min: 17  sec: 19\n",
      "epoch: 1  batch: 95 / 721  loss: 0.024670877796597778  hr: 0  min: 17  sec: 17\n",
      "epoch: 1  batch: 96 / 721  loss: 0.02459717479723622  hr: 0  min: 17  sec: 18\n",
      "epoch: 1  batch: 97 / 721  loss: 0.024479016948481733  hr: 0  min: 17  sec: 19\n",
      "epoch: 1  batch: 98 / 721  loss: 0.024273567494451618  hr: 0  min: 17  sec: 18\n",
      "epoch: 1  batch: 99 / 721  loss: 0.02406144854871349  hr: 0  min: 17  sec: 17\n",
      "epoch: 1  batch: 100 / 721  loss: 0.023934485303470865  hr: 0  min: 17  sec: 15\n",
      "epoch: 1  batch: 101 / 721  loss: 0.023755067199846675  hr: 0  min: 17  sec: 15\n",
      "epoch: 1  batch: 102 / 721  loss: 0.026329546725130912  hr: 0  min: 17  sec: 13\n",
      "epoch: 1  batch: 103 / 721  loss: 0.026171653889779687  hr: 0  min: 17  sec: 11\n",
      "epoch: 1  batch: 104 / 721  loss: 0.025945424916034635  hr: 0  min: 17  sec: 16\n",
      "epoch: 1  batch: 105 / 721  loss: 0.025805009962503043  hr: 0  min: 17  sec: 16\n",
      "epoch: 1  batch: 106 / 721  loss: 0.026065162320419232  hr: 0  min: 17  sec: 16\n",
      "epoch: 1  batch: 107 / 721  loss: 0.026247136772470984  hr: 0  min: 17  sec: 15\n",
      "epoch: 1  batch: 108 / 721  loss: 0.026372358848608134  hr: 0  min: 17  sec: 14\n",
      "epoch: 1  batch: 109 / 721  loss: 0.02634016218394417  hr: 0  min: 17  sec: 14\n",
      "epoch: 1  batch: 110 / 721  loss: 0.026142324696675958  hr: 0  min: 17  sec: 12\n",
      "epoch: 1  batch: 111 / 721  loss: 0.02595169235939613  hr: 0  min: 17  sec: 13\n",
      "epoch: 1  batch: 112 / 721  loss: 0.026672062242661405  hr: 0  min: 17  sec: 11\n",
      "epoch: 1  batch: 113 / 721  loss: 0.026462298631462045  hr: 0  min: 17  sec: 10\n",
      "epoch: 1  batch: 114 / 721  loss: 0.026501494201447554  hr: 0  min: 17  sec: 8\n",
      "epoch: 1  batch: 115 / 721  loss: 0.026427190672889674  hr: 0  min: 17  sec: 7\n",
      "epoch: 1  batch: 116 / 721  loss: 0.026669033372509775  hr: 0  min: 17  sec: 9\n",
      "epoch: 1  batch: 117 / 721  loss: 0.02662859223763116  hr: 0  min: 17  sec: 8\n",
      "epoch: 1  batch: 118 / 721  loss: 0.026978727085792097  hr: 0  min: 17  sec: 6\n",
      "epoch: 1  batch: 119 / 721  loss: 0.026789020832056695  hr: 0  min: 17  sec: 5\n",
      "epoch: 1  batch: 120 / 721  loss: 0.02658688176209883  hr: 0  min: 17  sec: 3\n",
      "epoch: 1  batch: 121 / 721  loss: 0.02658700270381995  hr: 0  min: 17  sec: 3\n",
      "epoch: 1  batch: 122 / 721  loss: 0.0265067678534609  hr: 0  min: 17  sec: 2\n",
      "epoch: 1  batch: 123 / 721  loss: 0.026313997262793525  hr: 0  min: 17  sec: 1\n",
      "epoch: 1  batch: 124 / 721  loss: 0.02613723804073919  hr: 0  min: 17  sec: 0\n",
      "epoch: 1  batch: 125 / 721  loss: 0.026008252154104412  hr: 0  min: 17  sec: 0\n",
      "epoch: 1  batch: 126 / 721  loss: 0.02598274240015252  hr: 0  min: 16  sec: 59\n",
      "epoch: 1  batch: 127 / 721  loss: 0.02579882539585086  hr: 0  min: 16  sec: 59\n",
      "epoch: 1  batch: 128 / 721  loss: 0.02562535036213376  hr: 0  min: 16  sec: 58\n",
      "epoch: 1  batch: 129 / 721  loss: 0.02552530764468563  hr: 0  min: 16  sec: 57\n",
      "epoch: 1  batch: 130 / 721  loss: 0.025502335051826846  hr: 0  min: 16  sec: 56\n",
      "epoch: 1  batch: 131 / 721  loss: 0.02537089405058819  hr: 0  min: 16  sec: 55\n",
      "epoch: 1  batch: 132 / 721  loss: 0.02521731850610002  hr: 0  min: 16  sec: 54\n",
      "epoch: 1  batch: 133 / 721  loss: 0.0250597592571022  hr: 0  min: 16  sec: 54\n",
      "epoch: 1  batch: 134 / 721  loss: 0.024945987941898794  hr: 0  min: 16  sec: 52\n",
      "epoch: 1  batch: 135 / 721  loss: 0.02491514107160684  hr: 0  min: 16  sec: 52\n",
      "epoch: 1  batch: 136 / 721  loss: 0.024835069503751583  hr: 0  min: 16  sec: 51\n",
      "epoch: 1  batch: 137 / 721  loss: 0.0246869540394345  hr: 0  min: 16  sec: 52\n",
      "epoch: 1  batch: 138 / 721  loss: 0.024557098058774474  hr: 0  min: 16  sec: 52\n",
      "epoch: 1  batch: 139 / 721  loss: 0.024393804282579812  hr: 0  min: 16  sec: 52\n",
      "epoch: 1  batch: 140 / 721  loss: 0.024246111764971698  hr: 0  min: 16  sec: 52\n",
      "epoch: 1  batch: 141 / 721  loss: 0.02410133692752966  hr: 0  min: 16  sec: 51\n",
      "epoch: 1  batch: 142 / 721  loss: 0.024216593831816926  hr: 0  min: 16  sec: 51\n",
      "epoch: 1  batch: 143 / 721  loss: 0.024257264117925212  hr: 0  min: 16  sec: 49\n",
      "epoch: 1  batch: 144 / 721  loss: 0.024303358175934084  hr: 0  min: 16  sec: 48\n",
      "epoch: 1  batch: 145 / 721  loss: 0.024163035189347534  hr: 0  min: 16  sec: 47\n",
      "epoch: 1  batch: 146 / 721  loss: 0.024028147122669607  hr: 0  min: 16  sec: 46\n",
      "epoch: 1  batch: 147 / 721  loss: 0.023874527811972412  hr: 0  min: 16  sec: 46\n",
      "epoch: 1  batch: 148 / 721  loss: 0.023721979942009155  hr: 0  min: 16  sec: 45\n",
      "epoch: 1  batch: 149 / 721  loss: 0.0235844066041528  hr: 0  min: 16  sec: 44\n",
      "epoch: 1  batch: 150 / 721  loss: 0.023453738018870355  hr: 0  min: 16  sec: 43\n",
      "epoch: 1  batch: 151 / 721  loss: 0.02461446232057565  hr: 0  min: 16  sec: 41\n",
      "epoch: 1  batch: 152 / 721  loss: 0.02447332181863634  hr: 0  min: 16  sec: 40\n",
      "epoch: 1  batch: 153 / 721  loss: 0.024325657426683973  hr: 0  min: 16  sec: 39\n",
      "epoch: 1  batch: 154 / 721  loss: 0.024271726460867355  hr: 0  min: 16  sec: 37\n",
      "epoch: 1  batch: 155 / 721  loss: 0.024156945301670462  hr: 0  min: 16  sec: 38\n",
      "epoch: 1  batch: 156 / 721  loss: 0.02406298119100169  hr: 0  min: 16  sec: 36\n",
      "epoch: 1  batch: 157 / 721  loss: 0.024434217205122826  hr: 0  min: 16  sec: 35\n",
      "epoch: 1  batch: 158 / 721  loss: 0.02429264127291574  hr: 0  min: 16  sec: 35\n",
      "epoch: 1  batch: 159 / 721  loss: 0.024432674671945005  hr: 0  min: 16  sec: 34\n",
      "epoch: 1  batch: 160 / 721  loss: 0.024295161605550675  hr: 0  min: 16  sec: 33\n",
      "epoch: 1  batch: 161 / 721  loss: 0.024166293221710957  hr: 0  min: 16  sec: 32\n",
      "epoch: 1  batch: 162 / 721  loss: 0.024033334993694068  hr: 0  min: 16  sec: 31\n",
      "epoch: 1  batch: 163 / 721  loss: 0.024212200642403422  hr: 0  min: 16  sec: 30\n",
      "epoch: 1  batch: 164 / 721  loss: 0.024431238759439665  hr: 0  min: 16  sec: 30\n",
      "epoch: 1  batch: 165 / 721  loss: 0.024295995142451968  hr: 0  min: 16  sec: 29\n",
      "epoch: 1  batch: 166 / 721  loss: 0.024206754527789687  hr: 0  min: 16  sec: 28\n",
      "epoch: 1  batch: 167 / 721  loss: 0.024342809642754315  hr: 0  min: 16  sec: 26\n",
      "epoch: 1  batch: 168 / 721  loss: 0.024236749545144404  hr: 0  min: 16  sec: 26\n",
      "epoch: 1  batch: 169 / 721  loss: 0.02410457705866722  hr: 0  min: 16  sec: 26\n",
      "epoch: 1  batch: 170 / 721  loss: 0.0241122559966136  hr: 0  min: 16  sec: 25\n",
      "epoch: 1  batch: 171 / 721  loss: 0.023995658420216923  hr: 0  min: 16  sec: 24\n",
      "epoch: 1  batch: 172 / 721  loss: 0.023889049984444324  hr: 0  min: 16  sec: 23\n",
      "epoch: 1  batch: 173 / 721  loss: 0.02375991168940666  hr: 0  min: 16  sec: 23\n",
      "epoch: 1  batch: 174 / 721  loss: 0.02366695509910391  hr: 0  min: 16  sec: 22\n",
      "epoch: 1  batch: 175 / 721  loss: 0.02375613190965461  hr: 0  min: 16  sec: 22\n",
      "epoch: 1  batch: 176 / 721  loss: 0.023682004207909235  hr: 0  min: 16  sec: 24\n",
      "epoch: 1  batch: 177 / 721  loss: 0.023612662420429205  hr: 0  min: 16  sec: 23\n",
      "epoch: 1  batch: 178 / 721  loss: 0.023519073204226332  hr: 0  min: 16  sec: 22\n",
      "epoch: 1  batch: 179 / 721  loss: 0.023396580998379966  hr: 0  min: 16  sec: 21\n",
      "epoch: 1  batch: 180 / 721  loss: 0.02327404412208125  hr: 0  min: 16  sec: 23\n",
      "epoch: 1  batch: 181 / 721  loss: 0.02322512678158835  hr: 0  min: 16  sec: 22\n",
      "epoch: 1  batch: 182 / 721  loss: 0.023119951361677722  hr: 0  min: 16  sec: 22\n",
      "epoch: 1  batch: 183 / 721  loss: 0.02299973711401957  hr: 0  min: 16  sec: 21\n",
      "epoch: 1  batch: 184 / 721  loss: 0.02289421993191354  hr: 0  min: 16  sec: 21\n",
      "epoch: 1  batch: 185 / 721  loss: 0.02289002703981021  hr: 0  min: 16  sec: 20\n",
      "epoch: 1  batch: 186 / 721  loss: 0.022834535824605616  hr: 0  min: 16  sec: 19\n",
      "epoch: 1  batch: 187 / 721  loss: 0.02295607523270509  hr: 0  min: 16  sec: 18\n",
      "epoch: 1  batch: 188 / 721  loss: 0.022846783178194646  hr: 0  min: 16  sec: 18\n",
      "epoch: 1  batch: 189 / 721  loss: 0.023144417736807435  hr: 0  min: 16  sec: 17\n",
      "epoch: 1  batch: 190 / 721  loss: 0.02317828611887403  hr: 0  min: 16  sec: 16\n",
      "epoch: 1  batch: 191 / 721  loss: 0.023077160766084938  hr: 0  min: 16  sec: 15\n",
      "epoch: 1  batch: 192 / 721  loss: 0.023037761529849377  hr: 0  min: 16  sec: 15\n",
      "epoch: 1  batch: 193 / 721  loss: 0.022976573486691774  hr: 0  min: 16  sec: 14\n",
      "epoch: 1  batch: 194 / 721  loss: 0.022890714777287904  hr: 0  min: 16  sec: 13\n",
      "epoch: 1  batch: 195 / 721  loss: 0.023561459151693644  hr: 0  min: 16  sec: 12\n",
      "epoch: 1  batch: 196 / 721  loss: 0.0234529606377877  hr: 0  min: 16  sec: 11\n",
      "epoch: 1  batch: 197 / 721  loss: 0.023359911009281645  hr: 0  min: 16  sec: 10\n",
      "epoch: 1  batch: 198 / 721  loss: 0.023273423336436876  hr: 0  min: 16  sec: 9\n",
      "epoch: 1  batch: 199 / 721  loss: 0.02322193592126907  hr: 0  min: 16  sec: 8\n",
      "epoch: 1  batch: 200 / 721  loss: 0.023144689778564497  hr: 0  min: 16  sec: 6\n",
      "epoch: 1  batch: 201 / 721  loss: 0.023077603697137378  hr: 0  min: 16  sec: 5\n",
      "epoch: 1  batch: 202 / 721  loss: 0.023007290459717486  hr: 0  min: 16  sec: 7\n",
      "epoch: 1  batch: 203 / 721  loss: 0.02301685118777081  hr: 0  min: 16  sec: 6\n",
      "epoch: 1  batch: 204 / 721  loss: 0.022978423559335152  hr: 0  min: 16  sec: 5\n",
      "epoch: 1  batch: 205 / 721  loss: 0.02287967756913021  hr: 0  min: 16  sec: 5\n",
      "epoch: 1  batch: 206 / 721  loss: 0.022801084026866095  hr: 0  min: 16  sec: 4\n",
      "epoch: 1  batch: 207 / 721  loss: 0.02280145729577016  hr: 0  min: 16  sec: 3\n",
      "epoch: 1  batch: 208 / 721  loss: 0.02272729381421903  hr: 0  min: 16  sec: 1\n",
      "epoch: 1  batch: 209 / 721  loss: 0.022731111435059227  hr: 0  min: 16  sec: 0\n",
      "epoch: 1  batch: 210 / 721  loss: 0.022640270949341356  hr: 0  min: 15  sec: 59\n",
      "epoch: 1  batch: 211 / 721  loss: 0.022687501019149382  hr: 0  min: 15  sec: 58\n",
      "epoch: 1  batch: 212 / 721  loss: 0.022594766168556404  hr: 0  min: 15  sec: 58\n",
      "epoch: 1  batch: 213 / 721  loss: 0.022714368408729493  hr: 0  min: 15  sec: 57\n",
      "epoch: 1  batch: 214 / 721  loss: 0.022721346646164344  hr: 0  min: 15  sec: 56\n",
      "epoch: 1  batch: 215 / 721  loss: 0.022634519110325463  hr: 0  min: 15  sec: 56\n",
      "epoch: 1  batch: 216 / 721  loss: 0.02267443596738977  hr: 0  min: 15  sec: 55\n",
      "epoch: 1  batch: 217 / 721  loss: 0.022609266240046732  hr: 0  min: 15  sec: 54\n",
      "epoch: 1  batch: 218 / 721  loss: 0.022597615023938204  hr: 0  min: 15  sec: 53\n",
      "epoch: 1  batch: 219 / 721  loss: 0.022513130413938195  hr: 0  min: 15  sec: 52\n",
      "epoch: 1  batch: 220 / 721  loss: 0.02244758306452158  hr: 0  min: 15  sec: 51\n",
      "epoch: 1  batch: 221 / 721  loss: 0.02262726453305707  hr: 0  min: 15  sec: 50\n",
      "epoch: 1  batch: 222 / 721  loss: 0.02256685864759257  hr: 0  min: 15  sec: 48\n",
      "epoch: 1  batch: 223 / 721  loss: 0.022545552501379774  hr: 0  min: 15  sec: 47\n",
      "epoch: 1  batch: 224 / 721  loss: 0.022565659387120313  hr: 0  min: 15  sec: 46\n",
      "epoch: 1  batch: 225 / 721  loss: 0.022494333216713534  hr: 0  min: 15  sec: 45\n",
      "epoch: 1  batch: 226 / 721  loss: 0.022410761004528112  hr: 0  min: 15  sec: 44\n",
      "epoch: 1  batch: 227 / 721  loss: 0.02233687340023431  hr: 0  min: 15  sec: 43\n",
      "epoch: 1  batch: 228 / 721  loss: 0.02230185521317221  hr: 0  min: 15  sec: 42\n",
      "epoch: 1  batch: 229 / 721  loss: 0.02228924933227194  hr: 0  min: 15  sec: 41\n",
      "epoch: 1  batch: 230 / 721  loss: 0.02219842303564529  hr: 0  min: 15  sec: 40\n",
      "epoch: 1  batch: 231 / 721  loss: 0.02215282388083895  hr: 0  min: 15  sec: 39\n",
      "epoch: 1  batch: 232 / 721  loss: 0.02264100353028786  hr: 0  min: 15  sec: 39\n",
      "epoch: 1  batch: 233 / 721  loss: 0.02255378971615798  hr: 0  min: 15  sec: 38\n",
      "epoch: 1  batch: 234 / 721  loss: 0.022559259137592446  hr: 0  min: 15  sec: 39\n",
      "epoch: 1  batch: 235 / 721  loss: 0.02251534468613248  hr: 0  min: 15  sec: 38\n",
      "epoch: 1  batch: 236 / 721  loss: 0.0226954903041202  hr: 0  min: 15  sec: 38\n",
      "epoch: 1  batch: 237 / 721  loss: 0.023208959321864734  hr: 0  min: 15  sec: 37\n",
      "epoch: 1  batch: 238 / 721  loss: 0.0232251180960022  hr: 0  min: 15  sec: 36\n",
      "epoch: 1  batch: 239 / 721  loss: 0.023242318499092963  hr: 0  min: 15  sec: 36\n",
      "epoch: 1  batch: 240 / 721  loss: 0.02316059580092163  hr: 0  min: 15  sec: 35\n",
      "epoch: 1  batch: 241 / 721  loss: 0.023186227265181197  hr: 0  min: 15  sec: 34\n",
      "epoch: 1  batch: 242 / 721  loss: 0.02314887117798448  hr: 0  min: 15  sec: 34\n",
      "epoch: 1  batch: 243 / 721  loss: 0.023092452644604286  hr: 0  min: 15  sec: 34\n",
      "epoch: 1  batch: 244 / 721  loss: 0.023032828532808202  hr: 0  min: 15  sec: 34\n",
      "epoch: 1  batch: 245 / 721  loss: 0.023001516322434253  hr: 0  min: 15  sec: 32\n",
      "epoch: 1  batch: 246 / 721  loss: 0.02294716122844143  hr: 0  min: 15  sec: 31\n",
      "epoch: 1  batch: 247 / 721  loss: 0.023000546377566964  hr: 0  min: 15  sec: 30\n",
      "epoch: 1  batch: 248 / 721  loss: 0.02296566688023778  hr: 0  min: 15  sec: 30\n",
      "epoch: 1  batch: 249 / 721  loss: 0.022916188056816358  hr: 0  min: 15  sec: 28\n",
      "epoch: 1  batch: 250 / 721  loss: 0.02284457836393267  hr: 0  min: 15  sec: 27\n",
      "epoch: 1  batch: 251 / 721  loss: 0.022765298270104654  hr: 0  min: 15  sec: 27\n",
      "epoch: 1  batch: 252 / 721  loss: 0.022708579284390285  hr: 0  min: 15  sec: 26\n",
      "epoch: 1  batch: 253 / 721  loss: 0.02262551664823865  hr: 0  min: 15  sec: 25\n",
      "epoch: 1  batch: 254 / 721  loss: 0.02268034126334567  hr: 0  min: 15  sec: 24\n",
      "epoch: 1  batch: 255 / 721  loss: 0.022598494126406663  hr: 0  min: 15  sec: 23\n",
      "epoch: 1  batch: 256 / 721  loss: 0.02251921791685163  hr: 0  min: 15  sec: 23\n",
      "epoch: 1  batch: 257 / 721  loss: 0.022442442283454687  hr: 0  min: 15  sec: 22\n",
      "epoch: 1  batch: 258 / 721  loss: 0.02237009711549148  hr: 0  min: 15  sec: 21\n",
      "epoch: 1  batch: 259 / 721  loss: 0.02228951023470855  hr: 0  min: 15  sec: 20\n",
      "epoch: 1  batch: 260 / 721  loss: 0.022353107791143256  hr: 0  min: 15  sec: 19\n",
      "epoch: 1  batch: 261 / 721  loss: 0.022273831119425453  hr: 0  min: 15  sec: 18\n",
      "epoch: 1  batch: 262 / 721  loss: 0.022202297367994454  hr: 0  min: 15  sec: 17\n",
      "epoch: 1  batch: 263 / 721  loss: 0.022136550210770434  hr: 0  min: 15  sec: 16\n",
      "epoch: 1  batch: 264 / 721  loss: 0.022055191482650116  hr: 0  min: 15  sec: 15\n",
      "epoch: 1  batch: 265 / 721  loss: 0.021975118658320872  hr: 0  min: 15  sec: 15\n",
      "epoch: 1  batch: 266 / 721  loss: 0.02189656886244998  hr: 0  min: 15  sec: 16\n",
      "epoch: 1  batch: 267 / 721  loss: 0.02189885701461824  hr: 0  min: 15  sec: 15\n",
      "epoch: 1  batch: 268 / 721  loss: 0.021822541149605912  hr: 0  min: 15  sec: 14\n",
      "epoch: 1  batch: 269 / 721  loss: 0.021745790798627325  hr: 0  min: 15  sec: 13\n",
      "epoch: 1  batch: 270 / 721  loss: 0.021668012151106572  hr: 0  min: 15  sec: 14\n",
      "epoch: 1  batch: 271 / 721  loss: 0.021615062557406573  hr: 0  min: 15  sec: 13\n",
      "epoch: 1  batch: 272 / 721  loss: 0.021539285080721397  hr: 0  min: 15  sec: 13\n",
      "epoch: 1  batch: 273 / 721  loss: 0.021476369305307406  hr: 0  min: 15  sec: 12\n",
      "epoch: 1  batch: 274 / 721  loss: 0.02150984466892504  hr: 0  min: 15  sec: 11\n",
      "epoch: 1  batch: 275 / 721  loss: 0.021442226411309093  hr: 0  min: 15  sec: 9\n",
      "epoch: 1  batch: 276 / 721  loss: 0.02137873244098138  hr: 0  min: 15  sec: 9\n",
      "epoch: 1  batch: 277 / 721  loss: 0.021314198445631292  hr: 0  min: 15  sec: 7\n",
      "epoch: 1  batch: 278 / 721  loss: 0.021239884118147517  hr: 0  min: 15  sec: 6\n",
      "epoch: 1  batch: 279 / 721  loss: 0.021174613727710633  hr: 0  min: 15  sec: 6\n",
      "epoch: 1  batch: 280 / 721  loss: 0.021101720199762246  hr: 0  min: 15  sec: 5\n",
      "epoch: 1  batch: 281 / 721  loss: 0.021035389960006052  hr: 0  min: 15  sec: 4\n",
      "epoch: 1  batch: 282 / 721  loss: 0.021008329266386686  hr: 0  min: 15  sec: 3\n",
      "epoch: 1  batch: 283 / 721  loss: 0.020949536199186183  hr: 0  min: 15  sec: 2\n",
      "epoch: 1  batch: 284 / 721  loss: 0.020906336843503417  hr: 0  min: 15  sec: 2\n",
      "epoch: 1  batch: 285 / 721  loss: 0.021222635007487903  hr: 0  min: 15  sec: 1\n",
      "epoch: 1  batch: 286 / 721  loss: 0.02115218669898202  hr: 0  min: 15  sec: 0\n",
      "epoch: 1  batch: 287 / 721  loss: 0.021168161082797024  hr: 0  min: 14  sec: 59\n",
      "epoch: 1  batch: 288 / 721  loss: 0.021127442560706793  hr: 0  min: 14  sec: 58\n",
      "epoch: 1  batch: 289 / 721  loss: 0.02105858710302027  hr: 0  min: 14  sec: 58\n",
      "epoch: 1  batch: 290 / 721  loss: 0.021101982811473886  hr: 0  min: 14  sec: 57\n",
      "epoch: 1  batch: 291 / 721  loss: 0.02111831663638078  hr: 0  min: 14  sec: 55\n",
      "epoch: 1  batch: 292 / 721  loss: 0.021055354467435256  hr: 0  min: 14  sec: 55\n",
      "epoch: 1  batch: 293 / 721  loss: 0.021100373638502985  hr: 0  min: 14  sec: 54\n",
      "epoch: 1  batch: 294 / 721  loss: 0.02103209239133748  hr: 0  min: 14  sec: 53\n",
      "epoch: 1  batch: 295 / 721  loss: 0.020979145895571337  hr: 0  min: 14  sec: 53\n",
      "epoch: 1  batch: 296 / 721  loss: 0.020915395297256386  hr: 0  min: 14  sec: 52\n",
      "epoch: 1  batch: 297 / 721  loss: 0.020859778829127464  hr: 0  min: 14  sec: 51\n",
      "epoch: 1  batch: 298 / 721  loss: 0.020811614808790548  hr: 0  min: 14  sec: 50\n",
      "epoch: 1  batch: 299 / 721  loss: 0.020907423029346922  hr: 0  min: 14  sec: 49\n",
      "epoch: 1  batch: 300 / 721  loss: 0.020844949375411186  hr: 0  min: 14  sec: 48\n",
      "epoch: 1  batch: 301 / 721  loss: 0.02079732907163227  hr: 0  min: 14  sec: 47\n",
      "epoch: 1  batch: 302 / 721  loss: 0.02074390336958266  hr: 0  min: 14  sec: 47\n",
      "epoch: 1  batch: 303 / 721  loss: 0.02085293967621831  hr: 0  min: 14  sec: 46\n",
      "epoch: 1  batch: 304 / 721  loss: 0.0207966062139588  hr: 0  min: 14  sec: 45\n",
      "epoch: 1  batch: 305 / 721  loss: 0.020775820648671724  hr: 0  min: 14  sec: 44\n",
      "epoch: 1  batch: 306 / 721  loss: 0.020718009250118317  hr: 0  min: 14  sec: 43\n",
      "epoch: 1  batch: 307 / 721  loss: 0.020658739679606773  hr: 0  min: 14  sec: 43\n",
      "epoch: 1  batch: 308 / 721  loss: 0.020598821828252175  hr: 0  min: 14  sec: 42\n",
      "epoch: 1  batch: 309 / 721  loss: 0.020552674571578135  hr: 0  min: 14  sec: 40\n",
      "epoch: 1  batch: 310 / 721  loss: 0.02049303060987093  hr: 0  min: 14  sec: 40\n",
      "epoch: 1  batch: 311 / 721  loss: 0.020446347451725393  hr: 0  min: 14  sec: 40\n",
      "epoch: 1  batch: 312 / 721  loss: 0.02067926470119393  hr: 0  min: 14  sec: 40\n",
      "epoch: 1  batch: 313 / 721  loss: 0.020638498351366198  hr: 0  min: 14  sec: 39\n",
      "epoch: 1  batch: 314 / 721  loss: 0.020584738516184992  hr: 0  min: 14  sec: 39\n",
      "epoch: 1  batch: 315 / 721  loss: 0.020534053371871808  hr: 0  min: 14  sec: 38\n",
      "epoch: 1  batch: 316 / 721  loss: 0.020552594507373928  hr: 0  min: 14  sec: 38\n",
      "epoch: 1  batch: 317 / 721  loss: 0.020795794889332333  hr: 0  min: 14  sec: 37\n",
      "epoch: 1  batch: 318 / 721  loss: 0.02075234119309575  hr: 0  min: 14  sec: 36\n",
      "epoch: 1  batch: 319 / 721  loss: 0.020706144077769044  hr: 0  min: 14  sec: 35\n",
      "epoch: 1  batch: 320 / 721  loss: 0.02065922856800171  hr: 0  min: 14  sec: 34\n",
      "epoch: 1  batch: 321 / 721  loss: 0.02067058824377738  hr: 0  min: 14  sec: 33\n",
      "epoch: 1  batch: 322 / 721  loss: 0.02061504930614274  hr: 0  min: 14  sec: 32\n",
      "epoch: 1  batch: 323 / 721  loss: 0.02060461159447358  hr: 0  min: 14  sec: 31\n",
      "epoch: 1  batch: 324 / 721  loss: 0.020614458740448843  hr: 0  min: 14  sec: 31\n",
      "epoch: 1  batch: 325 / 721  loss: 0.02075590040624285  hr: 0  min: 14  sec: 30\n",
      "epoch: 1  batch: 326 / 721  loss: 0.02070021666918504  hr: 0  min: 14  sec: 29\n",
      "epoch: 1  batch: 327 / 721  loss: 0.020795394144048032  hr: 0  min: 14  sec: 29\n",
      "epoch: 1  batch: 328 / 721  loss: 0.021166027987996265  hr: 0  min: 14  sec: 28\n",
      "epoch: 1  batch: 329 / 721  loss: 0.02120993190001283  hr: 0  min: 14  sec: 26\n",
      "epoch: 1  batch: 330 / 721  loss: 0.021291179761862043  hr: 0  min: 14  sec: 25\n",
      "epoch: 1  batch: 331 / 721  loss: 0.021255455873350915  hr: 0  min: 14  sec: 24\n",
      "epoch: 1  batch: 332 / 721  loss: 0.02120811794761776  hr: 0  min: 14  sec: 23\n",
      "epoch: 1  batch: 333 / 721  loss: 0.021153995357684557  hr: 0  min: 14  sec: 22\n",
      "epoch: 1  batch: 334 / 721  loss: 0.021102862537338004  hr: 0  min: 14  sec: 22\n",
      "epoch: 1  batch: 335 / 721  loss: 0.021057573255714473  hr: 0  min: 14  sec: 22\n",
      "epoch: 1  batch: 336 / 721  loss: 0.021208182736916655  hr: 0  min: 14  sec: 21\n",
      "epoch: 1  batch: 337 / 721  loss: 0.021169434240929375  hr: 0  min: 14  sec: 21\n",
      "epoch: 1  batch: 338 / 721  loss: 0.02112150196273282  hr: 0  min: 14  sec: 20\n",
      "epoch: 1  batch: 339 / 721  loss: 0.02109640938793735  hr: 0  min: 14  sec: 20\n",
      "epoch: 1  batch: 340 / 721  loss: 0.021056624817120954  hr: 0  min: 14  sec: 19\n",
      "epoch: 1  batch: 341 / 721  loss: 0.02100495663365688  hr: 0  min: 14  sec: 18\n",
      "epoch: 1  batch: 342 / 721  loss: 0.02124554490992576  hr: 0  min: 14  sec: 17\n",
      "epoch: 1  batch: 343 / 721  loss: 0.02119563098568551  hr: 0  min: 14  sec: 17\n",
      "epoch: 1  batch: 344 / 721  loss: 0.021207036466918464  hr: 0  min: 14  sec: 15\n",
      "epoch: 1  batch: 345 / 721  loss: 0.021360943925506234  hr: 0  min: 14  sec: 15\n",
      "epoch: 1  batch: 346 / 721  loss: 0.021319725001775904  hr: 0  min: 14  sec: 14\n",
      "epoch: 1  batch: 347 / 721  loss: 0.021419716945423956  hr: 0  min: 14  sec: 13\n",
      "epoch: 1  batch: 348 / 721  loss: 0.0213629251813326  hr: 0  min: 14  sec: 13\n",
      "epoch: 1  batch: 349 / 721  loss: 0.021319490275846096  hr: 0  min: 14  sec: 12\n",
      "epoch: 1  batch: 350 / 721  loss: 0.02129483967770024  hr: 0  min: 14  sec: 11\n",
      "epoch: 1  batch: 351 / 721  loss: 0.021454886045271102  hr: 0  min: 14  sec: 10\n",
      "epoch: 1  batch: 352 / 721  loss: 0.021402087251276083  hr: 0  min: 14  sec: 10\n",
      "epoch: 1  batch: 353 / 721  loss: 0.021369757535227605  hr: 0  min: 14  sec: 10\n",
      "epoch: 1  batch: 354 / 721  loss: 0.02136162696698928  hr: 0  min: 14  sec: 9\n",
      "epoch: 1  batch: 355 / 721  loss: 0.02130892400344519  hr: 0  min: 14  sec: 8\n",
      "epoch: 1  batch: 356 / 721  loss: 0.02139536691898234  hr: 0  min: 14  sec: 7\n",
      "epoch: 1  batch: 357 / 721  loss: 0.021339816964297955  hr: 0  min: 14  sec: 6\n",
      "epoch: 1  batch: 358 / 721  loss: 0.021425290196625434  hr: 0  min: 14  sec: 5\n",
      "epoch: 1  batch: 359 / 721  loss: 0.021410760432929266  hr: 0  min: 14  sec: 4\n",
      "epoch: 1  batch: 360 / 721  loss: 0.02137370870607103  hr: 0  min: 14  sec: 3\n",
      "epoch: 1  batch: 361 / 721  loss: 0.02131937818819079  hr: 0  min: 14  sec: 2\n",
      "epoch: 1  batch: 362 / 721  loss: 0.021354617046877918  hr: 0  min: 14  sec: 1\n",
      "epoch: 1  batch: 363 / 721  loss: 0.02130604235692073  hr: 0  min: 14  sec: 0\n",
      "epoch: 1  batch: 364 / 721  loss: 0.02125630244452143  hr: 0  min: 13  sec: 59\n",
      "epoch: 1  batch: 365 / 721  loss: 0.021202693171065608  hr: 0  min: 13  sec: 59\n",
      "epoch: 1  batch: 366 / 721  loss: 0.02117415243340529  hr: 0  min: 13  sec: 58\n",
      "epoch: 1  batch: 367 / 721  loss: 0.021159849604953346  hr: 0  min: 13  sec: 57\n",
      "epoch: 1  batch: 368 / 721  loss: 0.021190352515986888  hr: 0  min: 13  sec: 56\n",
      "epoch: 1  batch: 369 / 721  loss: 0.021145670390328226  hr: 0  min: 13  sec: 55\n",
      "epoch: 1  batch: 370 / 721  loss: 0.02124836818972678  hr: 0  min: 13  sec: 54\n",
      "epoch: 1  batch: 371 / 721  loss: 0.02119617134121164  hr: 0  min: 13  sec: 54\n",
      "epoch: 1  batch: 372 / 721  loss: 0.021142324993057647  hr: 0  min: 13  sec: 52\n",
      "epoch: 1  batch: 373 / 721  loss: 0.021095696070959326  hr: 0  min: 13  sec: 52\n",
      "epoch: 1  batch: 374 / 721  loss: 0.021047788153104933  hr: 0  min: 13  sec: 51\n",
      "epoch: 1  batch: 375 / 721  loss: 0.020995063831564038  hr: 0  min: 13  sec: 50\n",
      "epoch: 1  batch: 376 / 721  loss: 0.020946607672506598  hr: 0  min: 13  sec: 49\n",
      "epoch: 1  batch: 377 / 721  loss: 0.020973799740481844  hr: 0  min: 13  sec: 48\n",
      "epoch: 1  batch: 378 / 721  loss: 0.020925214245118368  hr: 0  min: 13  sec: 48\n",
      "epoch: 1  batch: 379 / 721  loss: 0.020882917169541722  hr: 0  min: 13  sec: 47\n",
      "epoch: 1  batch: 380 / 721  loss: 0.02090012634029048  hr: 0  min: 13  sec: 46\n",
      "epoch: 1  batch: 381 / 721  loss: 0.021116136712128458  hr: 0  min: 13  sec: 44\n",
      "epoch: 1  batch: 382 / 721  loss: 0.021092016639520723  hr: 0  min: 13  sec: 44\n",
      "epoch: 1  batch: 383 / 721  loss: 0.02104271028337358  hr: 0  min: 13  sec: 43\n",
      "epoch: 1  batch: 384 / 721  loss: 0.020993434002624174  hr: 0  min: 13  sec: 42\n",
      "epoch: 1  batch: 385 / 721  loss: 0.02095446749726407  hr: 0  min: 13  sec: 42\n",
      "epoch: 1  batch: 386 / 721  loss: 0.02130994066150275  hr: 0  min: 13  sec: 41\n",
      "epoch: 1  batch: 387 / 721  loss: 0.021480488536298248  hr: 0  min: 13  sec: 40\n",
      "epoch: 1  batch: 388 / 721  loss: 0.021440133151925916  hr: 0  min: 13  sec: 39\n",
      "epoch: 1  batch: 389 / 721  loss: 0.02138832023000383  hr: 0  min: 13  sec: 39\n",
      "epoch: 1  batch: 390 / 721  loss: 0.021348363264219072  hr: 0  min: 13  sec: 38\n",
      "epoch: 1  batch: 391 / 721  loss: 0.021297193785427885  hr: 0  min: 13  sec: 37\n",
      "epoch: 1  batch: 392 / 721  loss: 0.021319605260867652  hr: 0  min: 13  sec: 36\n",
      "epoch: 1  batch: 393 / 721  loss: 0.021270973476226994  hr: 0  min: 13  sec: 36\n",
      "epoch: 1  batch: 394 / 721  loss: 0.021243532243637404  hr: 0  min: 13  sec: 35\n",
      "epoch: 1  batch: 395 / 721  loss: 0.02121905726071188  hr: 0  min: 13  sec: 35\n",
      "epoch: 1  batch: 396 / 721  loss: 0.02117129838352228  hr: 0  min: 13  sec: 35\n",
      "epoch: 1  batch: 397 / 721  loss: 0.02114500165421408  hr: 0  min: 13  sec: 34\n",
      "epoch: 1  batch: 398 / 721  loss: 0.021096041017000203  hr: 0  min: 13  sec: 33\n",
      "epoch: 1  batch: 399 / 721  loss: 0.021276100156309416  hr: 0  min: 13  sec: 32\n",
      "epoch: 1  batch: 400 / 721  loss: 0.02128740836211364  hr: 0  min: 13  sec: 31\n",
      "epoch: 1  batch: 401 / 721  loss: 0.021239407430664214  hr: 0  min: 13  sec: 30\n",
      "epoch: 1  batch: 402 / 721  loss: 0.02118886338311725  hr: 0  min: 13  sec: 30\n",
      "epoch: 1  batch: 403 / 721  loss: 0.02116878137885646  hr: 0  min: 13  sec: 29\n",
      "epoch: 1  batch: 404 / 721  loss: 0.021132841824132047  hr: 0  min: 13  sec: 28\n",
      "epoch: 1  batch: 405 / 721  loss: 0.021086367956983546  hr: 0  min: 13  sec: 27\n",
      "epoch: 1  batch: 406 / 721  loss: 0.021082928800196136  hr: 0  min: 13  sec: 26\n",
      "epoch: 1  batch: 407 / 721  loss: 0.021092280353355412  hr: 0  min: 13  sec: 26\n",
      "epoch: 1  batch: 408 / 721  loss: 0.021045135795800744  hr: 0  min: 13  sec: 25\n",
      "epoch: 1  batch: 409 / 721  loss: 0.021149230329092127  hr: 0  min: 13  sec: 24\n",
      "epoch: 1  batch: 410 / 721  loss: 0.021286967238297733  hr: 0  min: 13  sec: 23\n",
      "epoch: 1  batch: 411 / 721  loss: 0.02124645059459238  hr: 0  min: 13  sec: 22\n",
      "epoch: 1  batch: 412 / 721  loss: 0.021199665064539183  hr: 0  min: 13  sec: 21\n",
      "epoch: 1  batch: 413 / 721  loss: 0.021219087954606076  hr: 0  min: 13  sec: 20\n",
      "epoch: 1  batch: 414 / 721  loss: 0.021227524377932044  hr: 0  min: 13  sec: 20\n",
      "epoch: 1  batch: 415 / 721  loss: 0.02118024007523859  hr: 0  min: 13  sec: 19\n",
      "epoch: 1  batch: 416 / 721  loss: 0.02113591782919964  hr: 0  min: 13  sec: 18\n",
      "epoch: 1  batch: 417 / 721  loss: 0.021092317105333457  hr: 0  min: 13  sec: 17\n",
      "epoch: 1  batch: 418 / 721  loss: 0.021132010735101268  hr: 0  min: 13  sec: 16\n",
      "epoch: 1  batch: 419 / 721  loss: 0.021088624696762953  hr: 0  min: 13  sec: 15\n",
      "epoch: 1  batch: 420 / 721  loss: 0.021041892102498207  hr: 0  min: 13  sec: 15\n",
      "epoch: 1  batch: 421 / 721  loss: 0.021022977415282022  hr: 0  min: 13  sec: 14\n",
      "epoch: 1  batch: 422 / 721  loss: 0.020987029586283058  hr: 0  min: 13  sec: 13\n",
      "epoch: 1  batch: 423 / 721  loss: 0.02094060266043674  hr: 0  min: 13  sec: 13\n",
      "epoch: 1  batch: 424 / 721  loss: 0.020894204602436215  hr: 0  min: 13  sec: 12\n",
      "epoch: 1  batch: 425 / 721  loss: 0.0211118718477733  hr: 0  min: 13  sec: 11\n",
      "epoch: 1  batch: 426 / 721  loss: 0.02106577264715616  hr: 0  min: 13  sec: 10\n",
      "epoch: 1  batch: 427 / 721  loss: 0.02112918657761076  hr: 0  min: 13  sec: 9\n",
      "epoch: 1  batch: 428 / 721  loss: 0.021110713745741538  hr: 0  min: 13  sec: 8\n",
      "epoch: 1  batch: 429 / 721  loss: 0.021099005413886446  hr: 0  min: 13  sec: 8\n",
      "epoch: 1  batch: 430 / 721  loss: 0.02109360960258041  hr: 0  min: 13  sec: 7\n",
      "epoch: 1  batch: 431 / 721  loss: 0.021059107296320063  hr: 0  min: 13  sec: 6\n",
      "epoch: 1  batch: 432 / 721  loss: 0.021062787347882812  hr: 0  min: 13  sec: 5\n",
      "epoch: 1  batch: 433 / 721  loss: 0.0210822764154971  hr: 0  min: 13  sec: 4\n",
      "epoch: 1  batch: 434 / 721  loss: 0.021206766565991266  hr: 0  min: 13  sec: 3\n",
      "epoch: 1  batch: 435 / 721  loss: 0.02119277719697305  hr: 0  min: 13  sec: 2\n",
      "epoch: 1  batch: 436 / 721  loss: 0.021160288506140504  hr: 0  min: 13  sec: 1\n",
      "epoch: 1  batch: 437 / 721  loss: 0.021139531059071286  hr: 0  min: 13  sec: 1\n",
      "epoch: 1  batch: 438 / 721  loss: 0.02109794805759828  hr: 0  min: 13  sec: 0\n",
      "epoch: 1  batch: 439 / 721  loss: 0.02109839740646303  hr: 0  min: 12  sec: 59\n",
      "epoch: 1  batch: 440 / 721  loss: 0.021057185760847377  hr: 0  min: 12  sec: 58\n",
      "epoch: 1  batch: 441 / 721  loss: 0.021024305443649017  hr: 0  min: 12  sec: 58\n",
      "epoch: 1  batch: 442 / 721  loss: 0.021230802237587176  hr: 0  min: 12  sec: 57\n",
      "epoch: 1  batch: 443 / 721  loss: 0.021418050817123554  hr: 0  min: 12  sec: 56\n",
      "epoch: 1  batch: 444 / 721  loss: 0.021407739287095464  hr: 0  min: 12  sec: 55\n",
      "epoch: 1  batch: 445 / 721  loss: 0.021366973269437806  hr: 0  min: 12  sec: 54\n",
      "epoch: 1  batch: 446 / 721  loss: 0.02133734191813811  hr: 0  min: 12  sec: 53\n",
      "epoch: 1  batch: 447 / 721  loss: 0.021300062620533126  hr: 0  min: 12  sec: 53\n",
      "epoch: 1  batch: 448 / 721  loss: 0.021265291091237617  hr: 0  min: 12  sec: 53\n",
      "epoch: 1  batch: 449 / 721  loss: 0.021224158637138512  hr: 0  min: 12  sec: 52\n",
      "epoch: 1  batch: 450 / 721  loss: 0.021184619612370927  hr: 0  min: 12  sec: 51\n",
      "epoch: 1  batch: 451 / 721  loss: 0.021144545324221088  hr: 0  min: 12  sec: 51\n",
      "epoch: 1  batch: 452 / 721  loss: 0.021118084470477715  hr: 0  min: 12  sec: 50\n",
      "epoch: 1  batch: 453 / 721  loss: 0.021075774696563056  hr: 0  min: 12  sec: 49\n",
      "epoch: 1  batch: 454 / 721  loss: 0.02106263224827331  hr: 0  min: 12  sec: 48\n",
      "epoch: 1  batch: 455 / 721  loss: 0.02102230922779539  hr: 0  min: 12  sec: 47\n",
      "epoch: 1  batch: 456 / 721  loss: 0.021013620088955288  hr: 0  min: 12  sec: 46\n",
      "epoch: 1  batch: 457 / 721  loss: 0.02099528883255741  hr: 0  min: 12  sec: 46\n",
      "epoch: 1  batch: 458 / 721  loss: 0.02121140849146195  hr: 0  min: 12  sec: 45\n",
      "epoch: 1  batch: 459 / 721  loss: 0.021169493986853683  hr: 0  min: 12  sec: 44\n",
      "epoch: 1  batch: 460 / 721  loss: 0.02112873722444815  hr: 0  min: 12  sec: 43\n",
      "epoch: 1  batch: 461 / 721  loss: 0.021217995453040053  hr: 0  min: 12  sec: 42\n",
      "epoch: 1  batch: 462 / 721  loss: 0.021226605640833156  hr: 0  min: 12  sec: 42\n",
      "epoch: 1  batch: 463 / 721  loss: 0.02121845470111793  hr: 0  min: 12  sec: 42\n",
      "epoch: 1  batch: 464 / 721  loss: 0.021196384119885524  hr: 0  min: 12  sec: 41\n",
      "epoch: 1  batch: 465 / 721  loss: 0.02115956874794617  hr: 0  min: 12  sec: 40\n",
      "epoch: 1  batch: 466 / 721  loss: 0.02113093663248845  hr: 0  min: 12  sec: 39\n",
      "epoch: 1  batch: 467 / 721  loss: 0.021099393322317996  hr: 0  min: 12  sec: 39\n",
      "epoch: 1  batch: 468 / 721  loss: 0.021056859595074248  hr: 0  min: 12  sec: 38\n",
      "epoch: 1  batch: 469 / 721  loss: 0.0210147336721599  hr: 0  min: 12  sec: 37\n",
      "epoch: 1  batch: 470 / 721  loss: 0.020989288381013545  hr: 0  min: 12  sec: 36\n",
      "epoch: 1  batch: 471 / 721  loss: 0.021006677658870546  hr: 0  min: 12  sec: 35\n",
      "epoch: 1  batch: 472 / 721  loss: 0.02103430769426977  hr: 0  min: 12  sec: 35\n",
      "epoch: 1  batch: 473 / 721  loss: 0.021001931161021156  hr: 0  min: 12  sec: 34\n",
      "epoch: 1  batch: 474 / 721  loss: 0.02099411473987734  hr: 0  min: 12  sec: 33\n",
      "epoch: 1  batch: 475 / 721  loss: 0.02095212143086093  hr: 0  min: 12  sec: 33\n",
      "epoch: 1  batch: 476 / 721  loss: 0.020929518681823692  hr: 0  min: 12  sec: 32\n",
      "epoch: 1  batch: 477 / 721  loss: 0.020999536490061983  hr: 0  min: 12  sec: 31\n",
      "epoch: 1  batch: 478 / 721  loss: 0.020957810081401254  hr: 0  min: 12  sec: 30\n",
      "epoch: 1  batch: 479 / 721  loss: 0.02095850164144609  hr: 0  min: 12  sec: 30\n",
      "epoch: 1  batch: 480 / 721  loss: 0.021114756498718634  hr: 0  min: 12  sec: 29\n",
      "epoch: 1  batch: 481 / 721  loss: 0.02107637887965818  hr: 0  min: 12  sec: 28\n",
      "epoch: 1  batch: 482 / 721  loss: 0.02109925800217586  hr: 0  min: 12  sec: 27\n",
      "epoch: 1  batch: 483 / 721  loss: 0.021065270926853483  hr: 0  min: 12  sec: 26\n",
      "epoch: 1  batch: 484 / 721  loss: 0.02102642803364395  hr: 0  min: 12  sec: 25\n",
      "epoch: 1  batch: 485 / 721  loss: 0.020986634107383408  hr: 0  min: 12  sec: 24\n",
      "epoch: 1  batch: 486 / 721  loss: 0.02096783727984262  hr: 0  min: 12  sec: 23\n",
      "epoch: 1  batch: 487 / 721  loss: 0.02175231861836015  hr: 0  min: 12  sec: 23\n",
      "epoch: 1  batch: 488 / 721  loss: 0.021724732856544835  hr: 0  min: 12  sec: 22\n",
      "epoch: 1  batch: 489 / 721  loss: 0.021692611559452242  hr: 0  min: 12  sec: 21\n",
      "epoch: 1  batch: 490 / 721  loss: 0.021736622085243615  hr: 0  min: 12  sec: 20\n",
      "epoch: 1  batch: 491 / 721  loss: 0.021821760400359626  hr: 0  min: 12  sec: 19\n",
      "epoch: 1  batch: 492 / 721  loss: 0.0217931266941274  hr: 0  min: 12  sec: 19\n",
      "epoch: 1  batch: 493 / 721  loss: 0.021785840302853115  hr: 0  min: 12  sec: 18\n",
      "epoch: 1  batch: 494 / 721  loss: 0.021761338827426466  hr: 0  min: 12  sec: 17\n",
      "epoch: 1  batch: 495 / 721  loss: 0.021742448104884137  hr: 0  min: 12  sec: 16\n",
      "epoch: 1  batch: 496 / 721  loss: 0.021734774574137954  hr: 0  min: 12  sec: 16\n",
      "epoch: 1  batch: 497 / 721  loss: 0.02169605295203806  hr: 0  min: 12  sec: 15\n",
      "epoch: 1  batch: 498 / 721  loss: 0.02180341206421799  hr: 0  min: 12  sec: 14\n",
      "epoch: 1  batch: 499 / 721  loss: 0.021778554677541456  hr: 0  min: 12  sec: 13\n",
      "epoch: 1  batch: 500 / 721  loss: 0.021855966021772474  hr: 0  min: 12  sec: 12\n",
      "epoch: 1  batch: 501 / 721  loss: 0.021822750642282254  hr: 0  min: 12  sec: 11\n",
      "epoch: 1  batch: 502 / 721  loss: 0.02181062830261426  hr: 0  min: 12  sec: 11\n",
      "epoch: 1  batch: 503 / 721  loss: 0.02178338354047958  hr: 0  min: 12  sec: 10\n",
      "epoch: 1  batch: 504 / 721  loss: 0.02178522479851612  hr: 0  min: 12  sec: 9\n",
      "epoch: 1  batch: 505 / 721  loss: 0.021749320254190872  hr: 0  min: 12  sec: 9\n",
      "epoch: 1  batch: 506 / 721  loss: 0.02177016988899117  hr: 0  min: 12  sec: 8\n",
      "epoch: 1  batch: 507 / 721  loss: 0.021741394380188347  hr: 0  min: 12  sec: 7\n",
      "epoch: 1  batch: 508 / 721  loss: 0.021702763650614007  hr: 0  min: 12  sec: 6\n",
      "epoch: 1  batch: 509 / 721  loss: 0.02168866770464583  hr: 0  min: 12  sec: 5\n",
      "epoch: 1  batch: 510 / 721  loss: 0.021906818312994552  hr: 0  min: 12  sec: 4\n",
      "epoch: 1  batch: 511 / 721  loss: 0.021871312613892434  hr: 0  min: 12  sec: 4\n",
      "epoch: 1  batch: 512 / 721  loss: 0.021891919269819482  hr: 0  min: 12  sec: 3\n",
      "epoch: 1  batch: 513 / 721  loss: 0.021852644699576654  hr: 0  min: 12  sec: 2\n",
      "epoch: 1  batch: 514 / 721  loss: 0.02193274852504468  hr: 0  min: 12  sec: 1\n",
      "epoch: 1  batch: 515 / 721  loss: 0.02191368759546465  hr: 0  min: 12  sec: 1\n",
      "epoch: 1  batch: 516 / 721  loss: 0.021997317772849585  hr: 0  min: 12  sec: 0\n",
      "epoch: 1  batch: 517 / 721  loss: 0.021962666446325052  hr: 0  min: 11  sec: 59\n",
      "epoch: 1  batch: 518 / 721  loss: 0.0219541399552699  hr: 0  min: 11  sec: 58\n",
      "epoch: 1  batch: 519 / 721  loss: 0.02195147704333067  hr: 0  min: 11  sec: 57\n",
      "epoch: 1  batch: 520 / 721  loss: 0.022077306469257634  hr: 0  min: 11  sec: 56\n",
      "epoch: 1  batch: 521 / 721  loss: 0.022098904159170308  hr: 0  min: 11  sec: 56\n",
      "epoch: 1  batch: 522 / 721  loss: 0.022060051481676612  hr: 0  min: 11  sec: 55\n",
      "epoch: 1  batch: 523 / 721  loss: 0.022025884244631956  hr: 0  min: 11  sec: 54\n",
      "epoch: 1  batch: 524 / 721  loss: 0.021993651686419327  hr: 0  min: 11  sec: 53\n",
      "epoch: 1  batch: 525 / 721  loss: 0.021962640460004054  hr: 0  min: 11  sec: 52\n",
      "epoch: 1  batch: 526 / 721  loss: 0.02192777031284541  hr: 0  min: 11  sec: 52\n",
      "epoch: 1  batch: 527 / 721  loss: 0.021920869332575636  hr: 0  min: 11  sec: 51\n",
      "epoch: 1  batch: 528 / 721  loss: 0.021885368921203102  hr: 0  min: 11  sec: 50\n",
      "epoch: 1  batch: 529 / 721  loss: 0.02186499347946376  hr: 0  min: 11  sec: 49\n",
      "epoch: 1  batch: 530 / 721  loss: 0.02195048755988092  hr: 0  min: 11  sec: 48\n",
      "epoch: 1  batch: 531 / 721  loss: 0.0219361552113155  hr: 0  min: 11  sec: 48\n",
      "epoch: 1  batch: 532 / 721  loss: 0.02192542304681383  hr: 0  min: 11  sec: 47\n",
      "epoch: 1  batch: 533 / 721  loss: 0.02188711812109014  hr: 0  min: 11  sec: 46\n",
      "epoch: 1  batch: 534 / 721  loss: 0.021888968247217422  hr: 0  min: 11  sec: 45\n",
      "epoch: 1  batch: 535 / 721  loss: 0.02189897349839769  hr: 0  min: 11  sec: 44\n",
      "epoch: 1  batch: 536 / 721  loss: 0.02190033873085301  hr: 0  min: 11  sec: 44\n",
      "epoch: 1  batch: 537 / 721  loss: 0.021895068444639402  hr: 0  min: 11  sec: 43\n",
      "epoch: 1  batch: 538 / 721  loss: 0.021956477454667875  hr: 0  min: 11  sec: 42\n",
      "epoch: 1  batch: 539 / 721  loss: 0.021933700065370407  hr: 0  min: 11  sec: 41\n",
      "epoch: 1  batch: 540 / 721  loss: 0.021914996018986804  hr: 0  min: 11  sec: 40\n",
      "epoch: 1  batch: 541 / 721  loss: 0.021916906962424255  hr: 0  min: 11  sec: 40\n",
      "epoch: 1  batch: 542 / 721  loss: 0.0219089022145531  hr: 0  min: 11  sec: 39\n",
      "epoch: 1  batch: 543 / 721  loss: 0.021872032533655317  hr: 0  min: 11  sec: 38\n",
      "epoch: 1  batch: 544 / 721  loss: 0.021870110646619544  hr: 0  min: 11  sec: 37\n",
      "epoch: 1  batch: 545 / 721  loss: 0.021851033996363435  hr: 0  min: 11  sec: 36\n",
      "epoch: 1  batch: 546 / 721  loss: 0.021856542631059257  hr: 0  min: 11  sec: 36\n",
      "epoch: 1  batch: 547 / 721  loss: 0.021823322127183298  hr: 0  min: 11  sec: 35\n",
      "epoch: 1  batch: 548 / 721  loss: 0.02186415222368074  hr: 0  min: 11  sec: 34\n",
      "epoch: 1  batch: 549 / 721  loss: 0.02186536741356462  hr: 0  min: 11  sec: 33\n",
      "epoch: 1  batch: 550 / 721  loss: 0.0218293746962974  hr: 0  min: 11  sec: 33\n",
      "epoch: 1  batch: 551 / 721  loss: 0.021824754540894702  hr: 0  min: 11  sec: 32\n",
      "epoch: 1  batch: 552 / 721  loss: 0.021787827524678796  hr: 0  min: 11  sec: 31\n",
      "epoch: 1  batch: 553 / 721  loss: 0.021763956067101797  hr: 0  min: 11  sec: 30\n",
      "epoch: 1  batch: 554 / 721  loss: 0.021932659230896  hr: 0  min: 11  sec: 29\n",
      "epoch: 1  batch: 555 / 721  loss: 0.021903109753311417  hr: 0  min: 11  sec: 29\n",
      "epoch: 1  batch: 556 / 721  loss: 0.021865439670337966  hr: 0  min: 11  sec: 28\n",
      "epoch: 1  batch: 557 / 721  loss: 0.021827781766893834  hr: 0  min: 11  sec: 27\n",
      "epoch: 1  batch: 558 / 721  loss: 0.021793576164378062  hr: 0  min: 11  sec: 26\n",
      "epoch: 1  batch: 559 / 721  loss: 0.021788260241393968  hr: 0  min: 11  sec: 25\n",
      "epoch: 1  batch: 560 / 721  loss: 0.02175107822768041  hr: 0  min: 11  sec: 24\n",
      "epoch: 1  batch: 561 / 721  loss: 0.02171416225672555  hr: 0  min: 11  sec: 24\n",
      "epoch: 1  batch: 562 / 721  loss: 0.021677210008426408  hr: 0  min: 11  sec: 23\n",
      "epoch: 1  batch: 563 / 721  loss: 0.021659382539044498  hr: 0  min: 11  sec: 22\n",
      "epoch: 1  batch: 564 / 721  loss: 0.021629008590322192  hr: 0  min: 11  sec: 22\n",
      "epoch: 1  batch: 565 / 721  loss: 0.021598913513466024  hr: 0  min: 11  sec: 21\n",
      "epoch: 1  batch: 566 / 721  loss: 0.02158051232390629  hr: 0  min: 11  sec: 20\n",
      "epoch: 1  batch: 567 / 721  loss: 0.021543933755682858  hr: 0  min: 11  sec: 19\n",
      "epoch: 1  batch: 568 / 721  loss: 0.021520474470121308  hr: 0  min: 11  sec: 18\n",
      "epoch: 1  batch: 569 / 721  loss: 0.021484476544747787  hr: 0  min: 11  sec: 17\n",
      "epoch: 1  batch: 570 / 721  loss: 0.021448690260315248  hr: 0  min: 11  sec: 17\n",
      "epoch: 1  batch: 571 / 721  loss: 0.021413040732831942  hr: 0  min: 11  sec: 16\n",
      "epoch: 1  batch: 572 / 721  loss: 0.021412639691957624  hr: 0  min: 11  sec: 15\n",
      "epoch: 1  batch: 573 / 721  loss: 0.021380607404540508  hr: 0  min: 11  sec: 15\n",
      "epoch: 1  batch: 574 / 721  loss: 0.02134515162168366  hr: 0  min: 11  sec: 14\n",
      "epoch: 1  batch: 575 / 721  loss: 0.021309070134353215  hr: 0  min: 11  sec: 14\n",
      "epoch: 1  batch: 576 / 721  loss: 0.021273747952262865  hr: 0  min: 11  sec: 13\n",
      "epoch: 1  batch: 577 / 721  loss: 0.02130323648361967  hr: 0  min: 11  sec: 12\n",
      "epoch: 1  batch: 578 / 721  loss: 0.02126707327381948  hr: 0  min: 11  sec: 11\n",
      "epoch: 1  batch: 579 / 721  loss: 0.021390558369785453  hr: 0  min: 11  sec: 11\n",
      "epoch: 1  batch: 580 / 721  loss: 0.02135467587288766  hr: 0  min: 11  sec: 10\n",
      "epoch: 1  batch: 581 / 721  loss: 0.02136006210947677  hr: 0  min: 11  sec: 9\n",
      "epoch: 1  batch: 582 / 721  loss: 0.02134649681682095  hr: 0  min: 11  sec: 8\n",
      "epoch: 1  batch: 583 / 721  loss: 0.021311568299008495  hr: 0  min: 11  sec: 8\n",
      "epoch: 1  batch: 584 / 721  loss: 0.021307440543053304  hr: 0  min: 11  sec: 7\n",
      "epoch: 1  batch: 585 / 721  loss: 0.021274898445052613  hr: 0  min: 11  sec: 6\n",
      "epoch: 1  batch: 586 / 721  loss: 0.021240297592145314  hr: 0  min: 11  sec: 5\n",
      "epoch: 1  batch: 587 / 721  loss: 0.02120661941914045  hr: 0  min: 11  sec: 4\n",
      "epoch: 1  batch: 588 / 721  loss: 0.021171428752458435  hr: 0  min: 11  sec: 4\n",
      "epoch: 1  batch: 589 / 721  loss: 0.0211511784293337  hr: 0  min: 11  sec: 3\n",
      "epoch: 1  batch: 590 / 721  loss: 0.021138789968701253  hr: 0  min: 11  sec: 2\n",
      "epoch: 1  batch: 591 / 721  loss: 0.021236950354959875  hr: 0  min: 11  sec: 1\n",
      "epoch: 1  batch: 592 / 721  loss: 0.021202758713902644  hr: 0  min: 11  sec: 0\n",
      "epoch: 1  batch: 593 / 721  loss: 0.021176195806561784  hr: 0  min: 11  sec: 0\n",
      "epoch: 1  batch: 594 / 721  loss: 0.021143108010395924  hr: 0  min: 10  sec: 59\n",
      "epoch: 1  batch: 595 / 721  loss: 0.02111070221568802  hr: 0  min: 10  sec: 58\n",
      "epoch: 1  batch: 596 / 721  loss: 0.021078675029060613  hr: 0  min: 10  sec: 57\n",
      "epoch: 1  batch: 597 / 721  loss: 0.02104461820035825  hr: 0  min: 10  sec: 56\n",
      "epoch: 1  batch: 598 / 721  loss: 0.021021935256226473  hr: 0  min: 10  sec: 55\n",
      "epoch: 1  batch: 599 / 721  loss: 0.021010886177194005  hr: 0  min: 10  sec: 54\n",
      "epoch: 1  batch: 600 / 721  loss: 0.021003514968178934  hr: 0  min: 10  sec: 54\n",
      "epoch: 1  batch: 601 / 721  loss: 0.02097905179805869  hr: 0  min: 10  sec: 53\n",
      "epoch: 1  batch: 602 / 721  loss: 0.020948292415109895  hr: 0  min: 10  sec: 52\n",
      "epoch: 1  batch: 603 / 721  loss: 0.020927255108165222  hr: 0  min: 10  sec: 51\n",
      "epoch: 1  batch: 604 / 721  loss: 0.020900007761216536  hr: 0  min: 10  sec: 50\n",
      "epoch: 1  batch: 605 / 721  loss: 0.020872524079591545  hr: 0  min: 10  sec: 50\n",
      "epoch: 1  batch: 606 / 721  loss: 0.02084132172952048  hr: 0  min: 10  sec: 49\n",
      "epoch: 1  batch: 607 / 721  loss: 0.020841607587541264  hr: 0  min: 10  sec: 48\n",
      "epoch: 1  batch: 608 / 721  loss: 0.020808800692544826  hr: 0  min: 10  sec: 47\n",
      "epoch: 1  batch: 609 / 721  loss: 0.02106818038579696  hr: 0  min: 10  sec: 46\n",
      "epoch: 1  batch: 610 / 721  loss: 0.021041216582038866  hr: 0  min: 10  sec: 45\n",
      "epoch: 1  batch: 611 / 721  loss: 0.021009575403777504  hr: 0  min: 10  sec: 45\n",
      "epoch: 1  batch: 612 / 721  loss: 0.020978795005028743  hr: 0  min: 10  sec: 44\n",
      "epoch: 1  batch: 613 / 721  loss: 0.021002647531954542  hr: 0  min: 10  sec: 43\n",
      "epoch: 1  batch: 614 / 721  loss: 0.02097478009391984  hr: 0  min: 10  sec: 42\n",
      "epoch: 1  batch: 615 / 721  loss: 0.020947114560739857  hr: 0  min: 10  sec: 41\n",
      "epoch: 1  batch: 616 / 721  loss: 0.02097704504128408  hr: 0  min: 10  sec: 41\n",
      "epoch: 1  batch: 617 / 721  loss: 0.020950105794521678  hr: 0  min: 10  sec: 40\n",
      "epoch: 1  batch: 618 / 721  loss: 0.020928046195950543  hr: 0  min: 10  sec: 39\n",
      "epoch: 1  batch: 619 / 721  loss: 0.020902747046159166  hr: 0  min: 10  sec: 38\n",
      "epoch: 1  batch: 620 / 721  loss: 0.020882727614380495  hr: 0  min: 10  sec: 37\n",
      "epoch: 1  batch: 621 / 721  loss: 0.020960510676944533  hr: 0  min: 10  sec: 37\n",
      "epoch: 1  batch: 622 / 721  loss: 0.020928917162262962  hr: 0  min: 10  sec: 36\n",
      "epoch: 1  batch: 623 / 721  loss: 0.020962938585665815  hr: 0  min: 10  sec: 35\n",
      "epoch: 1  batch: 624 / 721  loss: 0.0209322321361236  hr: 0  min: 10  sec: 34\n",
      "epoch: 1  batch: 625 / 721  loss: 0.02090706404861994  hr: 0  min: 10  sec: 33\n",
      "epoch: 1  batch: 626 / 721  loss: 0.02089489550228765  hr: 0  min: 10  sec: 32\n",
      "epoch: 1  batch: 627 / 721  loss: 0.02087227155034285  hr: 0  min: 10  sec: 32\n",
      "epoch: 1  batch: 628 / 721  loss: 0.02088184838646105  hr: 0  min: 10  sec: 31\n",
      "epoch: 1  batch: 629 / 721  loss: 0.02088442473863518  hr: 0  min: 10  sec: 30\n",
      "epoch: 1  batch: 630 / 721  loss: 0.020856126999927848  hr: 0  min: 10  sec: 29\n",
      "epoch: 1  batch: 631 / 721  loss: 0.020951929363865248  hr: 0  min: 10  sec: 28\n",
      "epoch: 1  batch: 632 / 721  loss: 0.020921520027709622  hr: 0  min: 10  sec: 27\n",
      "epoch: 1  batch: 633 / 721  loss: 0.020891581057531517  hr: 0  min: 10  sec: 27\n",
      "epoch: 1  batch: 634 / 721  loss: 0.020897570912506245  hr: 0  min: 10  sec: 26\n",
      "epoch: 1  batch: 635 / 721  loss: 0.02086890093147146  hr: 0  min: 10  sec: 25\n",
      "epoch: 1  batch: 636 / 721  loss: 0.02084548109923614  hr: 0  min: 10  sec: 24\n",
      "epoch: 1  batch: 637 / 721  loss: 0.02082412573677967  hr: 0  min: 10  sec: 23\n",
      "epoch: 1  batch: 638 / 721  loss: 0.020794732775913222  hr: 0  min: 10  sec: 23\n",
      "epoch: 1  batch: 639 / 721  loss: 0.020791537006917965  hr: 0  min: 10  sec: 22\n",
      "epoch: 1  batch: 640 / 721  loss: 0.020767725375708323  hr: 0  min: 10  sec: 22\n",
      "epoch: 1  batch: 641 / 721  loss: 0.02073680128724759  hr: 0  min: 10  sec: 21\n",
      "epoch: 1  batch: 642 / 721  loss: 0.02072346250549636  hr: 0  min: 10  sec: 20\n",
      "epoch: 1  batch: 643 / 721  loss: 0.02069596367996602  hr: 0  min: 10  sec: 20\n",
      "epoch: 1  batch: 644 / 721  loss: 0.020680444734053532  hr: 0  min: 10  sec: 19\n",
      "epoch: 1  batch: 645 / 721  loss: 0.020650239837808938  hr: 0  min: 10  sec: 18\n",
      "epoch: 1  batch: 646 / 721  loss: 0.02062213904725859  hr: 0  min: 10  sec: 17\n",
      "epoch: 1  batch: 647 / 721  loss: 0.02059320862892795  hr: 0  min: 10  sec: 16\n",
      "epoch: 1  batch: 648 / 721  loss: 0.020564223155456546  hr: 0  min: 10  sec: 16\n",
      "epoch: 1  batch: 649 / 721  loss: 0.020634538115341956  hr: 0  min: 10  sec: 15\n",
      "epoch: 1  batch: 650 / 721  loss: 0.020604715153100327  hr: 0  min: 10  sec: 14\n",
      "epoch: 1  batch: 651 / 721  loss: 0.020577222811937627  hr: 0  min: 10  sec: 13\n",
      "epoch: 1  batch: 652 / 721  loss: 0.02055302733605252  hr: 0  min: 10  sec: 12\n",
      "epoch: 1  batch: 653 / 721  loss: 0.020522964302267504  hr: 0  min: 10  sec: 12\n",
      "epoch: 1  batch: 654 / 721  loss: 0.020519588287663076  hr: 0  min: 10  sec: 11\n",
      "epoch: 1  batch: 655 / 721  loss: 0.020488870579514738  hr: 0  min: 10  sec: 10\n",
      "epoch: 1  batch: 656 / 721  loss: 0.020460102565887543  hr: 0  min: 10  sec: 10\n",
      "epoch: 1  batch: 657 / 721  loss: 0.020444473796743825  hr: 0  min: 10  sec: 9\n",
      "epoch: 1  batch: 658 / 721  loss: 0.020420197742824133  hr: 0  min: 10  sec: 8\n",
      "epoch: 1  batch: 659 / 721  loss: 0.020433099855625032  hr: 0  min: 10  sec: 7\n",
      "epoch: 1  batch: 660 / 721  loss: 0.020411198268729176  hr: 0  min: 10  sec: 6\n",
      "epoch: 1  batch: 661 / 721  loss: 0.02040255708414244  hr: 0  min: 10  sec: 5\n",
      "epoch: 1  batch: 662 / 721  loss: 0.020376068131232866  hr: 0  min: 10  sec: 4\n",
      "epoch: 1  batch: 663 / 721  loss: 0.020368452978265637  hr: 0  min: 10  sec: 3\n",
      "epoch: 1  batch: 664 / 721  loss: 0.020342234860480122  hr: 0  min: 10  sec: 3\n",
      "epoch: 1  batch: 665 / 721  loss: 0.020315084842390807  hr: 0  min: 10  sec: 2\n",
      "epoch: 1  batch: 666 / 721  loss: 0.020285606885684777  hr: 0  min: 10  sec: 1\n",
      "epoch: 1  batch: 667 / 721  loss: 0.020257579416229993  hr: 0  min: 10  sec: 0\n",
      "epoch: 1  batch: 668 / 721  loss: 0.020259736696328596  hr: 0  min: 10  sec: 0\n",
      "epoch: 1  batch: 669 / 721  loss: 0.0202353583463128  hr: 0  min: 9  sec: 59\n",
      "epoch: 1  batch: 670 / 721  loss: 0.020208534495922418  hr: 0  min: 9  sec: 58\n",
      "epoch: 1  batch: 671 / 721  loss: 0.020226063457058796  hr: 0  min: 9  sec: 57\n",
      "epoch: 1  batch: 672 / 721  loss: 0.020201855191252043  hr: 0  min: 9  sec: 57\n",
      "epoch: 1  batch: 673 / 721  loss: 0.020178591717400603  hr: 0  min: 9  sec: 56\n",
      "epoch: 1  batch: 674 / 721  loss: 0.020152226109171664  hr: 0  min: 9  sec: 55\n",
      "epoch: 1  batch: 675 / 721  loss: 0.02014265866406883  hr: 0  min: 9  sec: 54\n",
      "epoch: 1  batch: 676 / 721  loss: 0.020114329451789457  hr: 0  min: 9  sec: 53\n",
      "epoch: 1  batch: 677 / 721  loss: 0.02009268602030627  hr: 0  min: 9  sec: 52\n",
      "epoch: 1  batch: 678 / 721  loss: 0.020159544512251073  hr: 0  min: 9  sec: 51\n",
      "epoch: 1  batch: 679 / 721  loss: 0.020132748362870916  hr: 0  min: 9  sec: 51\n",
      "epoch: 1  batch: 680 / 721  loss: 0.0201039435720304  hr: 0  min: 9  sec: 50\n",
      "epoch: 1  batch: 681 / 721  loss: 0.020078424858537922  hr: 0  min: 9  sec: 49\n",
      "epoch: 1  batch: 682 / 721  loss: 0.020051177135495268  hr: 0  min: 9  sec: 48\n",
      "epoch: 1  batch: 683 / 721  loss: 0.020022778219313635  hr: 0  min: 9  sec: 47\n",
      "epoch: 1  batch: 684 / 721  loss: 0.020059447729846393  hr: 0  min: 9  sec: 46\n",
      "epoch: 1  batch: 685 / 721  loss: 0.020032040225290933  hr: 0  min: 9  sec: 46\n",
      "epoch: 1  batch: 686 / 721  loss: 0.02000370949782355  hr: 0  min: 9  sec: 45\n",
      "epoch: 1  batch: 687 / 721  loss: 0.020045113741329383  hr: 0  min: 9  sec: 44\n",
      "epoch: 1  batch: 688 / 721  loss: 0.02001932314204507  hr: 0  min: 9  sec: 43\n",
      "epoch: 1  batch: 689 / 721  loss: 0.020058390925704613  hr: 0  min: 9  sec: 43\n",
      "epoch: 1  batch: 690 / 721  loss: 0.020030699091612536  hr: 0  min: 9  sec: 42\n",
      "epoch: 1  batch: 691 / 721  loss: 0.02000295204550497  hr: 0  min: 9  sec: 41\n",
      "epoch: 1  batch: 692 / 721  loss: 0.01997642856339569  hr: 0  min: 9  sec: 40\n",
      "epoch: 1  batch: 693 / 721  loss: 0.01995047684849615  hr: 0  min: 9  sec: 39\n",
      "epoch: 1  batch: 694 / 721  loss: 0.019924064809812087  hr: 0  min: 9  sec: 39\n",
      "epoch: 1  batch: 695 / 721  loss: 0.019898770612718122  hr: 0  min: 9  sec: 38\n",
      "epoch: 1  batch: 696 / 721  loss: 0.01988108764351051  hr: 0  min: 9  sec: 37\n",
      "epoch: 1  batch: 697 / 721  loss: 0.02053858256324928  hr: 0  min: 9  sec: 36\n",
      "epoch: 1  batch: 698 / 721  loss: 0.020514692630342616  hr: 0  min: 9  sec: 35\n",
      "epoch: 1  batch: 699 / 721  loss: 0.020487942528685764  hr: 0  min: 9  sec: 35\n",
      "epoch: 1  batch: 700 / 721  loss: 0.020464760482510818  hr: 0  min: 9  sec: 34\n",
      "epoch: 1  batch: 701 / 721  loss: 0.020445319574483048  hr: 0  min: 9  sec: 33\n",
      "epoch: 1  batch: 702 / 721  loss: 0.02043171652076644  hr: 0  min: 9  sec: 32\n",
      "epoch: 1  batch: 703 / 721  loss: 0.020416173482827767  hr: 0  min: 9  sec: 32\n",
      "epoch: 1  batch: 704 / 721  loss: 0.020399963838082676  hr: 0  min: 9  sec: 31\n",
      "epoch: 1  batch: 705 / 721  loss: 0.02038689461034716  hr: 0  min: 9  sec: 30\n",
      "epoch: 1  batch: 706 / 721  loss: 0.020361560563393274  hr: 0  min: 9  sec: 30\n",
      "epoch: 1  batch: 707 / 721  loss: 0.020362705664440484  hr: 0  min: 9  sec: 29\n",
      "epoch: 1  batch: 708 / 721  loss: 0.02035030872346623  hr: 0  min: 9  sec: 28\n",
      "epoch: 1  batch: 709 / 721  loss: 0.020350428758194337  hr: 0  min: 9  sec: 27\n",
      "epoch: 1  batch: 710 / 721  loss: 0.020337522372564063  hr: 0  min: 9  sec: 26\n",
      "epoch: 1  batch: 711 / 721  loss: 0.020313514201049123  hr: 0  min: 9  sec: 26\n",
      "epoch: 1  batch: 712 / 721  loss: 0.020319119062774366  hr: 0  min: 9  sec: 25\n",
      "epoch: 1  batch: 713 / 721  loss: 0.020299414935264458  hr: 0  min: 9  sec: 24\n",
      "epoch: 1  batch: 714 / 721  loss: 0.020543126709934023  hr: 0  min: 9  sec: 23\n",
      "epoch: 1  batch: 715 / 721  loss: 0.02051794135721622  hr: 0  min: 9  sec: 22\n",
      "epoch: 1  batch: 716 / 721  loss: 0.020514978973881742  hr: 0  min: 9  sec: 22\n",
      "epoch: 1  batch: 717 / 721  loss: 0.02051468708291338  hr: 0  min: 9  sec: 21\n",
      "epoch: 1  batch: 718 / 721  loss: 0.020499804435936023  hr: 0  min: 9  sec: 20\n",
      "epoch: 1  batch: 719 / 721  loss: 0.020476761473516347  hr: 0  min: 9  sec: 20\n",
      "epoch: 1  batch: 720 / 721  loss: 0.020450654398721072  hr: 0  min: 9  sec: 19\n",
      "epoch: 1  batch: 721 / 721  loss: 0.020426997961250398  hr: 0  min: 9  sec: 18\n",
      "epoch: 2  batch: 1 / 721  loss: 0.006935685873031616  hr: 0  min: 8  sec: 3\n",
      "epoch: 2  batch: 2 / 721  loss: 0.004856541287153959  hr: 0  min: 8  sec: 27\n",
      "epoch: 2  batch: 3 / 721  loss: 0.005054228473454714  hr: 0  min: 8  sec: 22\n",
      "epoch: 2  batch: 4 / 721  loss: 0.004390839661937207  hr: 0  min: 8  sec: 35\n",
      "epoch: 2  batch: 5 / 721  loss: 0.003946677967905998  hr: 0  min: 8  sec: 32\n",
      "epoch: 2  batch: 6 / 721  loss: 0.004016576179613669  hr: 0  min: 8  sec: 36\n",
      "epoch: 2  batch: 7 / 721  loss: 0.003826518642849156  hr: 0  min: 8  sec: 40\n",
      "epoch: 2  batch: 8 / 721  loss: 0.004761306074215099  hr: 0  min: 8  sec: 43\n",
      "epoch: 2  batch: 9 / 721  loss: 0.004798072157427669  hr: 0  min: 8  sec: 37\n",
      "epoch: 2  batch: 10 / 721  loss: 0.0047173842554911975  hr: 0  min: 8  sec: 41\n",
      "epoch: 2  batch: 11 / 721  loss: 0.004383408252827146  hr: 0  min: 8  sec: 42\n",
      "epoch: 2  batch: 12 / 721  loss: 0.004138570143065105  hr: 0  min: 9  sec: 3\n",
      "epoch: 2  batch: 13 / 721  loss: 0.005077496475468461  hr: 0  min: 9  sec: 0\n",
      "epoch: 2  batch: 14 / 721  loss: 0.004781463707331568  hr: 0  min: 9  sec: 18\n",
      "epoch: 2  batch: 15 / 721  loss: 0.004577699373476208  hr: 0  min: 9  sec: 13\n",
      "epoch: 2  batch: 16 / 721  loss: 0.004501765513850842  hr: 0  min: 9  sec: 10\n",
      "epoch: 2  batch: 17 / 721  loss: 0.004317708649015164  hr: 0  min: 9  sec: 9\n",
      "epoch: 2  batch: 18 / 721  loss: 0.004191912497238566  hr: 0  min: 9  sec: 13\n",
      "epoch: 2  batch: 19 / 721  loss: 0.004044717398325079  hr: 0  min: 9  sec: 12\n",
      "epoch: 2  batch: 20 / 721  loss: 0.003927832382032648  hr: 0  min: 9  sec: 16\n",
      "epoch: 2  batch: 21 / 721  loss: 0.004131761739873106  hr: 0  min: 9  sec: 11\n",
      "epoch: 2  batch: 22 / 721  loss: 0.004299311946273188  hr: 0  min: 9  sec: 9\n",
      "epoch: 2  batch: 23 / 721  loss: 0.004151080709720111  hr: 0  min: 9  sec: 6\n",
      "epoch: 2  batch: 24 / 721  loss: 0.004055439727380872  hr: 0  min: 9  sec: 6\n",
      "epoch: 2  batch: 25 / 721  loss: 0.004128319751471281  hr: 0  min: 9  sec: 4\n",
      "epoch: 2  batch: 26 / 721  loss: 0.004037774038107063  hr: 0  min: 9  sec: 3\n",
      "epoch: 2  batch: 27 / 721  loss: 0.003913557823074775  hr: 0  min: 9  sec: 5\n",
      "epoch: 2  batch: 28 / 721  loss: 0.00896163834633106  hr: 0  min: 9  sec: 3\n",
      "epoch: 2  batch: 29 / 721  loss: 0.008742864775583791  hr: 0  min: 8  sec: 59\n",
      "epoch: 2  batch: 30 / 721  loss: 0.00868560485735846  hr: 0  min: 8  sec: 58\n",
      "epoch: 2  batch: 31 / 721  loss: 0.008699577243532985  hr: 0  min: 8  sec: 56\n",
      "epoch: 2  batch: 32 / 721  loss: 0.008445233055681456  hr: 0  min: 8  sec: 53\n",
      "epoch: 2  batch: 33 / 721  loss: 0.008813491549737977  hr: 0  min: 8  sec: 53\n",
      "epoch: 2  batch: 34 / 721  loss: 0.0085907938568305  hr: 0  min: 8  sec: 52\n",
      "epoch: 2  batch: 35 / 721  loss: 0.008881672488392464  hr: 0  min: 8  sec: 50\n",
      "epoch: 2  batch: 36 / 721  loss: 0.009419927855358563  hr: 0  min: 8  sec: 51\n",
      "epoch: 2  batch: 37 / 721  loss: 0.009203414174702924  hr: 0  min: 8  sec: 49\n",
      "epoch: 2  batch: 38 / 721  loss: 0.009014792911904422  hr: 0  min: 8  sec: 50\n",
      "epoch: 2  batch: 39 / 721  loss: 0.009265065742417788  hr: 0  min: 8  sec: 48\n",
      "epoch: 2  batch: 40 / 721  loss: 0.00906023905845359  hr: 0  min: 8  sec: 49\n",
      "epoch: 2  batch: 41 / 721  loss: 0.008860822268704906  hr: 0  min: 8  sec: 51\n",
      "epoch: 2  batch: 42 / 721  loss: 0.008748197097918905  hr: 0  min: 8  sec: 51\n",
      "epoch: 2  batch: 43 / 721  loss: 0.008586605216192384  hr: 0  min: 8  sec: 49\n",
      "epoch: 2  batch: 44 / 721  loss: 0.00840723888408816  hr: 0  min: 8  sec: 49\n",
      "epoch: 2  batch: 45 / 721  loss: 0.00832818554352141  hr: 0  min: 8  sec: 48\n",
      "epoch: 2  batch: 46 / 721  loss: 0.008215784593788989  hr: 0  min: 8  sec: 47\n",
      "epoch: 2  batch: 47 / 721  loss: 0.008063987006453123  hr: 0  min: 8  sec: 46\n",
      "epoch: 2  batch: 48 / 721  loss: 0.008140336964667464  hr: 0  min: 8  sec: 45\n",
      "epoch: 2  batch: 49 / 721  loss: 0.008002577325785342  hr: 0  min: 8  sec: 43\n",
      "epoch: 2  batch: 50 / 721  loss: 0.00815232056658715  hr: 0  min: 8  sec: 42\n",
      "epoch: 2  batch: 51 / 721  loss: 0.008766488368422086  hr: 0  min: 8  sec: 40\n",
      "epoch: 2  batch: 52 / 721  loss: 0.008930780275617368  hr: 0  min: 8  sec: 39\n",
      "epoch: 2  batch: 53 / 721  loss: 0.008777534644084298  hr: 0  min: 8  sec: 38\n",
      "epoch: 2  batch: 54 / 721  loss: 0.008625302829184674  hr: 0  min: 8  sec: 38\n",
      "epoch: 2  batch: 55 / 721  loss: 0.008495308148717  hr: 0  min: 8  sec: 36\n",
      "epoch: 2  batch: 56 / 721  loss: 0.00839051225817197  hr: 0  min: 8  sec: 34\n",
      "epoch: 2  batch: 57 / 721  loss: 0.009249327744136712  hr: 0  min: 8  sec: 32\n",
      "epoch: 2  batch: 58 / 721  loss: 0.01007470405752899  hr: 0  min: 8  sec: 32\n",
      "epoch: 2  batch: 59 / 721  loss: 0.009920810742927241  hr: 0  min: 8  sec: 30\n",
      "epoch: 2  batch: 60 / 721  loss: 0.009892309394005375  hr: 0  min: 8  sec: 29\n",
      "epoch: 2  batch: 61 / 721  loss: 0.009746234004240727  hr: 0  min: 8  sec: 29\n",
      "epoch: 2  batch: 62 / 721  loss: 0.009632654584786524  hr: 0  min: 8  sec: 28\n",
      "epoch: 2  batch: 63 / 721  loss: 0.0095235995702549  hr: 0  min: 8  sec: 28\n",
      "epoch: 2  batch: 64 / 721  loss: 0.009382805549648765  hr: 0  min: 8  sec: 30\n",
      "epoch: 2  batch: 65 / 721  loss: 0.009261106215238285  hr: 0  min: 8  sec: 29\n",
      "epoch: 2  batch: 66 / 721  loss: 0.009212830981458395  hr: 0  min: 8  sec: 27\n",
      "epoch: 2  batch: 67 / 721  loss: 0.009092933219447454  hr: 0  min: 8  sec: 29\n",
      "epoch: 2  batch: 68 / 721  loss: 0.009009515841813375  hr: 0  min: 8  sec: 30\n",
      "epoch: 2  batch: 69 / 721  loss: 0.008936791818258285  hr: 0  min: 8  sec: 28\n",
      "epoch: 2  batch: 70 / 721  loss: 0.009412879725485775  hr: 0  min: 8  sec: 27\n",
      "epoch: 2  batch: 71 / 721  loss: 0.009304574473505236  hr: 0  min: 8  sec: 26\n",
      "epoch: 2  batch: 72 / 721  loss: 0.009210952463035937  hr: 0  min: 8  sec: 25\n",
      "epoch: 2  batch: 73 / 721  loss: 0.009145656979343  hr: 0  min: 8  sec: 24\n",
      "epoch: 2  batch: 74 / 721  loss: 0.009091038731197399  hr: 0  min: 8  sec: 23\n",
      "epoch: 2  batch: 75 / 721  loss: 0.010281742385122925  hr: 0  min: 8  sec: 22\n",
      "epoch: 2  batch: 76 / 721  loss: 0.010204641775712125  hr: 0  min: 8  sec: 21\n",
      "epoch: 2  batch: 77 / 721  loss: 0.010085134931035678  hr: 0  min: 8  sec: 20\n",
      "epoch: 2  batch: 78 / 721  loss: 0.010444486186129209  hr: 0  min: 8  sec: 19\n",
      "epoch: 2  batch: 79 / 721  loss: 0.010319714471285196  hr: 0  min: 8  sec: 19\n",
      "epoch: 2  batch: 80 / 721  loss: 0.010278061965072994  hr: 0  min: 8  sec: 18\n",
      "epoch: 2  batch: 81 / 721  loss: 0.010160730405779625  hr: 0  min: 8  sec: 20\n",
      "epoch: 2  batch: 82 / 721  loss: 0.010048524059196253  hr: 0  min: 8  sec: 18\n",
      "epoch: 2  batch: 83 / 721  loss: 0.009941720018017453  hr: 0  min: 8  sec: 18\n",
      "epoch: 2  batch: 84 / 721  loss: 0.009856268936779261  hr: 0  min: 8  sec: 17\n",
      "epoch: 2  batch: 85 / 721  loss: 0.010202323422109819  hr: 0  min: 8  sec: 16\n",
      "epoch: 2  batch: 86 / 721  loss: 0.010099895581397293  hr: 0  min: 8  sec: 15\n",
      "epoch: 2  batch: 87 / 721  loss: 0.01000741515014235  hr: 0  min: 8  sec: 14\n",
      "epoch: 2  batch: 88 / 721  loss: 0.009906389025590297  hr: 0  min: 8  sec: 14\n",
      "epoch: 2  batch: 89 / 721  loss: 0.009814253587045529  hr: 0  min: 8  sec: 13\n",
      "epoch: 2  batch: 90 / 721  loss: 0.009715327032608911  hr: 0  min: 8  sec: 12\n",
      "epoch: 2  batch: 91 / 721  loss: 0.009660913503426894  hr: 0  min: 8  sec: 11\n",
      "epoch: 2  batch: 92 / 721  loss: 0.009628823332604952  hr: 0  min: 8  sec: 11\n",
      "epoch: 2  batch: 93 / 721  loss: 0.009533868078017227  hr: 0  min: 8  sec: 10\n",
      "epoch: 2  batch: 94 / 721  loss: 0.009606863943577565  hr: 0  min: 8  sec: 9\n",
      "epoch: 2  batch: 95 / 721  loss: 0.00951634259882236  hr: 0  min: 8  sec: 9\n",
      "epoch: 2  batch: 96 / 721  loss: 0.009432483631220142  hr: 0  min: 8  sec: 8\n",
      "epoch: 2  batch: 97 / 721  loss: 0.009354141446612967  hr: 0  min: 8  sec: 7\n",
      "epoch: 2  batch: 98 / 721  loss: 0.009266520785025264  hr: 0  min: 8  sec: 6\n",
      "epoch: 2  batch: 99 / 721  loss: 0.009186030195019387  hr: 0  min: 8  sec: 4\n",
      "epoch: 2  batch: 100 / 721  loss: 0.00915489265578799  hr: 0  min: 8  sec: 3\n",
      "epoch: 2  batch: 101 / 721  loss: 0.009086656800268383  hr: 0  min: 8  sec: 2\n",
      "epoch: 2  batch: 102 / 721  loss: 0.009006499683749223  hr: 0  min: 8  sec: 1\n",
      "epoch: 2  batch: 103 / 721  loss: 0.009015734982224705  hr: 0  min: 8  sec: 0\n",
      "epoch: 2  batch: 104 / 721  loss: 0.008952803429565392  hr: 0  min: 7  sec: 59\n",
      "epoch: 2  batch: 105 / 721  loss: 0.008873712395073934  hr: 0  min: 7  sec: 58\n",
      "epoch: 2  batch: 106 / 721  loss: 0.008801013892977763  hr: 0  min: 7  sec: 57\n",
      "epoch: 2  batch: 107 / 721  loss: 0.008903350239885083  hr: 0  min: 7  sec: 56\n",
      "epoch: 2  batch: 108 / 721  loss: 0.008837420134956052  hr: 0  min: 7  sec: 55\n",
      "epoch: 2  batch: 109 / 721  loss: 0.008762352880936354  hr: 0  min: 7  sec: 55\n",
      "epoch: 2  batch: 110 / 721  loss: 0.00870649595004083  hr: 0  min: 7  sec: 53\n",
      "epoch: 2  batch: 111 / 721  loss: 0.008788606039718263  hr: 0  min: 7  sec: 53\n",
      "epoch: 2  batch: 112 / 721  loss: 0.008721421238858187  hr: 0  min: 7  sec: 52\n",
      "epoch: 2  batch: 113 / 721  loss: 0.008658455049132813  hr: 0  min: 7  sec: 51\n",
      "epoch: 2  batch: 114 / 721  loss: 0.008587735625935653  hr: 0  min: 7  sec: 51\n",
      "epoch: 2  batch: 115 / 721  loss: 0.00934542660733037  hr: 0  min: 7  sec: 49\n",
      "epoch: 2  batch: 116 / 721  loss: 0.009349962506254588  hr: 0  min: 7  sec: 48\n",
      "epoch: 2  batch: 117 / 721  loss: 0.009287139982418913  hr: 0  min: 7  sec: 47\n",
      "epoch: 2  batch: 118 / 721  loss: 0.00922659402801971  hr: 0  min: 7  sec: 46\n",
      "epoch: 2  batch: 119 / 721  loss: 0.009159044312223197  hr: 0  min: 7  sec: 45\n",
      "epoch: 2  batch: 120 / 721  loss: 0.009088274721580091  hr: 0  min: 7  sec: 45\n",
      "epoch: 2  batch: 121 / 721  loss: 0.009139226697104375  hr: 0  min: 7  sec: 44\n",
      "epoch: 2  batch: 122 / 721  loss: 0.009073569711782496  hr: 0  min: 7  sec: 44\n",
      "epoch: 2  batch: 123 / 721  loss: 0.009013646378321952  hr: 0  min: 7  sec: 43\n",
      "epoch: 2  batch: 124 / 721  loss: 0.008955802989564086  hr: 0  min: 7  sec: 42\n",
      "epoch: 2  batch: 125 / 721  loss: 0.00889349021250382  hr: 0  min: 7  sec: 41\n",
      "epoch: 2  batch: 126 / 721  loss: 0.008842422261497833  hr: 0  min: 7  sec: 40\n",
      "epoch: 2  batch: 127 / 721  loss: 0.008777140770345838  hr: 0  min: 7  sec: 40\n",
      "epoch: 2  batch: 128 / 721  loss: 0.008724342309960775  hr: 0  min: 7  sec: 39\n",
      "epoch: 2  batch: 129 / 721  loss: 0.00866788497434698  hr: 0  min: 7  sec: 38\n",
      "epoch: 2  batch: 130 / 721  loss: 0.008608293374821257  hr: 0  min: 7  sec: 37\n",
      "epoch: 2  batch: 131 / 721  loss: 0.008552272345529485  hr: 0  min: 7  sec: 37\n",
      "epoch: 2  batch: 132 / 721  loss: 0.008495403358458796  hr: 0  min: 7  sec: 35\n",
      "epoch: 2  batch: 133 / 721  loss: 0.008688702384584156  hr: 0  min: 7  sec: 35\n",
      "epoch: 2  batch: 134 / 721  loss: 0.008954696319024288  hr: 0  min: 7  sec: 34\n",
      "epoch: 2  batch: 135 / 721  loss: 0.008893160061927995  hr: 0  min: 7  sec: 33\n",
      "epoch: 2  batch: 136 / 721  loss: 0.008835817813472686  hr: 0  min: 7  sec: 32\n",
      "epoch: 2  batch: 137 / 721  loss: 0.008786855644060393  hr: 0  min: 7  sec: 31\n",
      "epoch: 2  batch: 138 / 721  loss: 0.0087326777802841  hr: 0  min: 7  sec: 30\n",
      "epoch: 2  batch: 139 / 721  loss: 0.009150562880363101  hr: 0  min: 7  sec: 29\n",
      "epoch: 2  batch: 140 / 721  loss: 0.009118351225957408  hr: 0  min: 7  sec: 28\n",
      "epoch: 2  batch: 141 / 721  loss: 0.009061543453824938  hr: 0  min: 7  sec: 26\n",
      "epoch: 2  batch: 142 / 721  loss: 0.00901174709540267  hr: 0  min: 7  sec: 25\n",
      "epoch: 2  batch: 143 / 721  loss: 0.008954748016752797  hr: 0  min: 7  sec: 25\n",
      "epoch: 2  batch: 144 / 721  loss: 0.008927813579551488  hr: 0  min: 7  sec: 24\n",
      "epoch: 2  batch: 145 / 721  loss: 0.009393668794956315  hr: 0  min: 7  sec: 23\n",
      "epoch: 2  batch: 146 / 721  loss: 0.009337929565832657  hr: 0  min: 7  sec: 22\n",
      "epoch: 2  batch: 147 / 721  loss: 0.009292935995704995  hr: 0  min: 7  sec: 22\n",
      "epoch: 2  batch: 148 / 721  loss: 0.009243906529484997  hr: 0  min: 7  sec: 21\n",
      "epoch: 2  batch: 149 / 721  loss: 0.009202840633693482  hr: 0  min: 7  sec: 20\n",
      "epoch: 2  batch: 150 / 721  loss: 0.009247454766882584  hr: 0  min: 7  sec: 18\n",
      "epoch: 2  batch: 151 / 721  loss: 0.009190375414377831  hr: 0  min: 7  sec: 18\n",
      "epoch: 2  batch: 152 / 721  loss: 0.00913526013870356  hr: 0  min: 7  sec: 17\n",
      "epoch: 2  batch: 153 / 721  loss: 0.0095312112198193  hr: 0  min: 7  sec: 16\n",
      "epoch: 2  batch: 154 / 721  loss: 0.010233424787633363  hr: 0  min: 7  sec: 15\n",
      "epoch: 2  batch: 155 / 721  loss: 0.010416722864723735  hr: 0  min: 7  sec: 14\n",
      "epoch: 2  batch: 156 / 721  loss: 0.010353903007807018  hr: 0  min: 7  sec: 14\n",
      "epoch: 2  batch: 157 / 721  loss: 0.010296069059645531  hr: 0  min: 7  sec: 13\n",
      "epoch: 2  batch: 158 / 721  loss: 0.010239616543337514  hr: 0  min: 7  sec: 12\n",
      "epoch: 2  batch: 159 / 721  loss: 0.010182129055900066  hr: 0  min: 7  sec: 12\n",
      "epoch: 2  batch: 160 / 721  loss: 0.010132323914876906  hr: 0  min: 7  sec: 10\n",
      "epoch: 2  batch: 161 / 721  loss: 0.010080776314731248  hr: 0  min: 7  sec: 10\n",
      "epoch: 2  batch: 162 / 721  loss: 0.0100301777703952  hr: 0  min: 7  sec: 9\n",
      "epoch: 2  batch: 163 / 721  loss: 0.010040917934455617  hr: 0  min: 7  sec: 8\n",
      "epoch: 2  batch: 164 / 721  loss: 0.009992247506288993  hr: 0  min: 7  sec: 7\n",
      "epoch: 2  batch: 165 / 721  loss: 0.009980261062666999  hr: 0  min: 7  sec: 6\n",
      "epoch: 2  batch: 166 / 721  loss: 0.009922351126232274  hr: 0  min: 7  sec: 6\n",
      "epoch: 2  batch: 167 / 721  loss: 0.009868357246386133  hr: 0  min: 7  sec: 5\n",
      "epoch: 2  batch: 168 / 721  loss: 0.009814081158332382  hr: 0  min: 7  sec: 5\n",
      "epoch: 2  batch: 169 / 721  loss: 0.00976155300541318  hr: 0  min: 7  sec: 5\n",
      "epoch: 2  batch: 170 / 721  loss: 0.009709401832215542  hr: 0  min: 7  sec: 4\n",
      "epoch: 2  batch: 171 / 721  loss: 0.009660356151987117  hr: 0  min: 7  sec: 3\n",
      "epoch: 2  batch: 172 / 721  loss: 0.009619193577490541  hr: 0  min: 7  sec: 3\n",
      "epoch: 2  batch: 173 / 721  loss: 0.009605436400958536  hr: 0  min: 7  sec: 2\n",
      "epoch: 2  batch: 174 / 721  loss: 0.00956621176871637  hr: 0  min: 7  sec: 1\n",
      "epoch: 2  batch: 175 / 721  loss: 0.009515937133193281  hr: 0  min: 7  sec: 0\n",
      "epoch: 2  batch: 176 / 721  loss: 0.009469087391897285  hr: 0  min: 6  sec: 59\n",
      "epoch: 2  batch: 177 / 721  loss: 0.009420565246126883  hr: 0  min: 6  sec: 58\n",
      "epoch: 2  batch: 178 / 721  loss: 0.009382111846575295  hr: 0  min: 6  sec: 57\n",
      "epoch: 2  batch: 179 / 721  loss: 0.00933527389645977  hr: 0  min: 6  sec: 56\n",
      "epoch: 2  batch: 180 / 721  loss: 0.009295457433553464  hr: 0  min: 6  sec: 55\n",
      "epoch: 2  batch: 181 / 721  loss: 0.009249055804588282  hr: 0  min: 6  sec: 55\n",
      "epoch: 2  batch: 182 / 721  loss: 0.009199936814404062  hr: 0  min: 6  sec: 54\n",
      "epoch: 2  batch: 183 / 721  loss: 0.009152951965215386  hr: 0  min: 6  sec: 53\n",
      "epoch: 2  batch: 184 / 721  loss: 0.00910551268173984  hr: 0  min: 6  sec: 52\n",
      "epoch: 2  batch: 185 / 721  loss: 0.00905837687865101  hr: 0  min: 6  sec: 52\n",
      "epoch: 2  batch: 186 / 721  loss: 0.009012321517201922  hr: 0  min: 6  sec: 52\n",
      "epoch: 2  batch: 187 / 721  loss: 0.008965640315595507  hr: 0  min: 6  sec: 51\n",
      "epoch: 2  batch: 188 / 721  loss: 0.00891999471678765  hr: 0  min: 6  sec: 51\n",
      "epoch: 2  batch: 189 / 721  loss: 0.008875319290882784  hr: 0  min: 6  sec: 50\n",
      "epoch: 2  batch: 190 / 721  loss: 0.0088307667122296  hr: 0  min: 6  sec: 49\n",
      "epoch: 2  batch: 191 / 721  loss: 0.008919121335804248  hr: 0  min: 6  sec: 48\n",
      "epoch: 2  batch: 192 / 721  loss: 0.008900318640674717  hr: 0  min: 6  sec: 47\n",
      "epoch: 2  batch: 193 / 721  loss: 0.008864133490594539  hr: 0  min: 6  sec: 47\n",
      "epoch: 2  batch: 194 / 721  loss: 0.00888093725325703  hr: 0  min: 6  sec: 45\n",
      "epoch: 2  batch: 195 / 721  loss: 0.009241990317315913  hr: 0  min: 6  sec: 45\n",
      "epoch: 2  batch: 196 / 721  loss: 0.009195907995546098  hr: 0  min: 6  sec: 44\n",
      "epoch: 2  batch: 197 / 721  loss: 0.009151757918378021  hr: 0  min: 6  sec: 43\n",
      "epoch: 2  batch: 198 / 721  loss: 0.009113037952494154  hr: 0  min: 6  sec: 43\n",
      "epoch: 2  batch: 199 / 721  loss: 0.00906894166844759  hr: 0  min: 6  sec: 42\n",
      "epoch: 2  batch: 200 / 721  loss: 0.009025515793109663  hr: 0  min: 6  sec: 41\n",
      "epoch: 2  batch: 201 / 721  loss: 0.008989969285454634  hr: 0  min: 6  sec: 41\n",
      "epoch: 2  batch: 202 / 721  loss: 0.008963421463956572  hr: 0  min: 6  sec: 40\n",
      "epoch: 2  batch: 203 / 721  loss: 0.008922714982545944  hr: 0  min: 6  sec: 39\n",
      "epoch: 2  batch: 204 / 721  loss: 0.00888031680919013  hr: 0  min: 6  sec: 39\n",
      "epoch: 2  batch: 205 / 721  loss: 0.00901297757880501  hr: 0  min: 6  sec: 38\n",
      "epoch: 2  batch: 206 / 721  loss: 0.008970981545097  hr: 0  min: 6  sec: 37\n",
      "epoch: 2  batch: 207 / 721  loss: 0.008930964083486722  hr: 0  min: 6  sec: 37\n",
      "epoch: 2  batch: 208 / 721  loss: 0.009123536671582909  hr: 0  min: 6  sec: 36\n",
      "epoch: 2  batch: 209 / 721  loss: 0.00919454220678762  hr: 0  min: 6  sec: 35\n",
      "epoch: 2  batch: 210 / 721  loss: 0.009154049928751885  hr: 0  min: 6  sec: 34\n",
      "epoch: 2  batch: 211 / 721  loss: 0.009128263884130756  hr: 0  min: 6  sec: 33\n",
      "epoch: 2  batch: 212 / 721  loss: 0.009087043249650687  hr: 0  min: 6  sec: 33\n",
      "epoch: 2  batch: 213 / 721  loss: 0.009054678596980857  hr: 0  min: 6  sec: 32\n",
      "epoch: 2  batch: 214 / 721  loss: 0.009014551803672569  hr: 0  min: 6  sec: 31\n",
      "epoch: 2  batch: 215 / 721  loss: 0.0089740385115282  hr: 0  min: 6  sec: 31\n",
      "epoch: 2  batch: 216 / 721  loss: 0.008936451509591145  hr: 0  min: 6  sec: 30\n",
      "epoch: 2  batch: 217 / 721  loss: 0.00891820228990427  hr: 0  min: 6  sec: 29\n",
      "epoch: 2  batch: 218 / 721  loss: 0.008881008686722628  hr: 0  min: 6  sec: 28\n",
      "epoch: 2  batch: 219 / 721  loss: 0.00884186155017897  hr: 0  min: 6  sec: 27\n",
      "epoch: 2  batch: 220 / 721  loss: 0.009050328081767393  hr: 0  min: 6  sec: 27\n",
      "epoch: 2  batch: 221 / 721  loss: 0.009012640565220262  hr: 0  min: 6  sec: 26\n",
      "epoch: 2  batch: 222 / 721  loss: 0.00899393997030062  hr: 0  min: 6  sec: 25\n",
      "epoch: 2  batch: 223 / 721  loss: 0.008955397175856231  hr: 0  min: 6  sec: 24\n",
      "epoch: 2  batch: 224 / 721  loss: 0.008918664633516269  hr: 0  min: 6  sec: 23\n",
      "epoch: 2  batch: 225 / 721  loss: 0.008886205660269802  hr: 0  min: 6  sec: 22\n",
      "epoch: 2  batch: 226 / 721  loss: 0.00884847415875658  hr: 0  min: 6  sec: 21\n",
      "epoch: 2  batch: 227 / 721  loss: 0.008814502526615697  hr: 0  min: 6  sec: 21\n",
      "epoch: 2  batch: 228 / 721  loss: 0.008798322357171472  hr: 0  min: 6  sec: 20\n",
      "epoch: 2  batch: 229 / 721  loss: 0.008762281616490362  hr: 0  min: 6  sec: 19\n",
      "epoch: 2  batch: 230 / 721  loss: 0.008756585644606157  hr: 0  min: 6  sec: 19\n",
      "epoch: 2  batch: 231 / 721  loss: 0.00872131286271358  hr: 0  min: 6  sec: 18\n",
      "epoch: 2  batch: 232 / 721  loss: 0.008701188724569812  hr: 0  min: 6  sec: 17\n",
      "epoch: 2  batch: 233 / 721  loss: 0.008665835837918022  hr: 0  min: 6  sec: 16\n",
      "epoch: 2  batch: 234 / 721  loss: 0.008646910191569816  hr: 0  min: 6  sec: 15\n",
      "epoch: 2  batch: 235 / 721  loss: 0.008622816341069647  hr: 0  min: 6  sec: 15\n",
      "epoch: 2  batch: 236 / 721  loss: 0.008590053050777327  hr: 0  min: 6  sec: 14\n",
      "epoch: 2  batch: 237 / 721  loss: 0.008555564724897694  hr: 0  min: 6  sec: 13\n",
      "epoch: 2  batch: 238 / 721  loss: 0.008527475466011111  hr: 0  min: 6  sec: 12\n",
      "epoch: 2  batch: 239 / 721  loss: 0.008494303874278375  hr: 0  min: 6  sec: 11\n",
      "epoch: 2  batch: 240 / 721  loss: 0.00846466154744121  hr: 0  min: 6  sec: 11\n",
      "epoch: 2  batch: 241 / 721  loss: 0.008445961671677047  hr: 0  min: 6  sec: 10\n",
      "epoch: 2  batch: 242 / 721  loss: 0.008413187439386514  hr: 0  min: 6  sec: 9\n",
      "epoch: 2  batch: 243 / 721  loss: 0.00838346028018507  hr: 0  min: 6  sec: 9\n",
      "epoch: 2  batch: 244 / 721  loss: 0.008350465045519055  hr: 0  min: 6  sec: 9\n",
      "epoch: 2  batch: 245 / 721  loss: 0.008333147577145516  hr: 0  min: 6  sec: 8\n",
      "epoch: 2  batch: 246 / 721  loss: 0.00841399663222842  hr: 0  min: 6  sec: 8\n",
      "epoch: 2  batch: 247 / 721  loss: 0.008431403488187056  hr: 0  min: 6  sec: 7\n",
      "epoch: 2  batch: 248 / 721  loss: 0.008400182478468313  hr: 0  min: 6  sec: 6\n",
      "epoch: 2  batch: 249 / 721  loss: 0.008406866508562691  hr: 0  min: 6  sec: 5\n",
      "epoch: 2  batch: 250 / 721  loss: 0.008379114611831029  hr: 0  min: 6  sec: 4\n",
      "epoch: 2  batch: 251 / 721  loss: 0.008347476281856921  hr: 0  min: 6  sec: 3\n",
      "epoch: 2  batch: 252 / 721  loss: 0.00831748838824015  hr: 0  min: 6  sec: 2\n",
      "epoch: 2  batch: 253 / 721  loss: 0.008285983535380603  hr: 0  min: 6  sec: 2\n",
      "epoch: 2  batch: 254 / 721  loss: 0.008254938346586341  hr: 0  min: 6  sec: 1\n",
      "epoch: 2  batch: 255 / 721  loss: 0.008226803425121961  hr: 0  min: 6  sec: 0\n",
      "epoch: 2  batch: 256 / 721  loss: 0.008196214812699054  hr: 0  min: 5  sec: 59\n",
      "epoch: 2  batch: 257 / 721  loss: 0.008189689038524763  hr: 0  min: 5  sec: 58\n",
      "epoch: 2  batch: 258 / 721  loss: 0.008497501503553894  hr: 0  min: 5  sec: 57\n",
      "epoch: 2  batch: 259 / 721  loss: 0.00846999823988881  hr: 0  min: 5  sec: 56\n",
      "epoch: 2  batch: 260 / 721  loss: 0.00847097361989794  hr: 0  min: 5  sec: 56\n",
      "epoch: 2  batch: 261 / 721  loss: 0.00843953340080383  hr: 0  min: 5  sec: 55\n",
      "epoch: 2  batch: 262 / 721  loss: 0.008451263227387606  hr: 0  min: 5  sec: 54\n",
      "epoch: 2  batch: 263 / 721  loss: 0.0085160620713641  hr: 0  min: 5  sec: 54\n",
      "epoch: 2  batch: 264 / 721  loss: 0.00852753513991598  hr: 0  min: 5  sec: 53\n",
      "epoch: 2  batch: 265 / 721  loss: 0.008496972806433233  hr: 0  min: 5  sec: 52\n",
      "epoch: 2  batch: 266 / 721  loss: 0.008468811793191117  hr: 0  min: 5  sec: 51\n",
      "epoch: 2  batch: 267 / 721  loss: 0.008438711086724542  hr: 0  min: 5  sec: 51\n",
      "epoch: 2  batch: 268 / 721  loss: 0.00840901110924862  hr: 0  min: 5  sec: 50\n",
      "epoch: 2  batch: 269 / 721  loss: 0.008510329578144106  hr: 0  min: 5  sec: 49\n",
      "epoch: 2  batch: 270 / 721  loss: 0.008507013615141019  hr: 0  min: 5  sec: 48\n",
      "epoch: 2  batch: 271 / 721  loss: 0.008645897879979636  hr: 0  min: 5  sec: 47\n",
      "epoch: 2  batch: 272 / 721  loss: 0.008623147731026726  hr: 0  min: 5  sec: 47\n",
      "epoch: 2  batch: 273 / 721  loss: 0.008600559724559205  hr: 0  min: 5  sec: 46\n",
      "epoch: 2  batch: 274 / 721  loss: 0.008603238607032384  hr: 0  min: 5  sec: 45\n",
      "epoch: 2  batch: 275 / 721  loss: 0.008575658588405614  hr: 0  min: 5  sec: 45\n",
      "epoch: 2  batch: 276 / 721  loss: 0.008590811937935634  hr: 0  min: 5  sec: 44\n",
      "epoch: 2  batch: 277 / 721  loss: 0.00856205563258257  hr: 0  min: 5  sec: 43\n",
      "epoch: 2  batch: 278 / 721  loss: 0.00853546867841183  hr: 0  min: 5  sec: 42\n",
      "epoch: 2  batch: 279 / 721  loss: 0.008589914050794  hr: 0  min: 5  sec: 42\n",
      "epoch: 2  batch: 280 / 721  loss: 0.008562238788363175  hr: 0  min: 5  sec: 41\n",
      "epoch: 2  batch: 281 / 721  loss: 0.008535347388776805  hr: 0  min: 5  sec: 40\n",
      "epoch: 2  batch: 282 / 721  loss: 0.008611928883323641  hr: 0  min: 5  sec: 39\n",
      "epoch: 2  batch: 283 / 721  loss: 0.008583761675087093  hr: 0  min: 5  sec: 38\n",
      "epoch: 2  batch: 284 / 721  loss: 0.008559190171299762  hr: 0  min: 5  sec: 37\n",
      "epoch: 2  batch: 285 / 721  loss: 0.008530837095704076  hr: 0  min: 5  sec: 37\n",
      "epoch: 2  batch: 286 / 721  loss: 0.008504066489959687  hr: 0  min: 5  sec: 36\n",
      "epoch: 2  batch: 287 / 721  loss: 0.00847601820375058  hr: 0  min: 5  sec: 35\n",
      "epoch: 2  batch: 288 / 721  loss: 0.008447970545653839  hr: 0  min: 5  sec: 35\n",
      "epoch: 2  batch: 289 / 721  loss: 0.00842121366402787  hr: 0  min: 5  sec: 34\n",
      "epoch: 2  batch: 290 / 721  loss: 0.008393689856617632  hr: 0  min: 5  sec: 33\n",
      "epoch: 2  batch: 291 / 721  loss: 0.008367316447379616  hr: 0  min: 5  sec: 32\n",
      "epoch: 2  batch: 292 / 721  loss: 0.00842974868437398  hr: 0  min: 5  sec: 32\n",
      "epoch: 2  batch: 293 / 721  loss: 0.008406318958625023  hr: 0  min: 5  sec: 31\n",
      "epoch: 2  batch: 294 / 721  loss: 0.008379155462522608  hr: 0  min: 5  sec: 30\n",
      "epoch: 2  batch: 295 / 721  loss: 0.008355437911567436  hr: 0  min: 5  sec: 29\n",
      "epoch: 2  batch: 296 / 721  loss: 0.008403124247854214  hr: 0  min: 5  sec: 28\n",
      "epoch: 2  batch: 297 / 721  loss: 0.008663110269842315  hr: 0  min: 5  sec: 27\n",
      "epoch: 2  batch: 298 / 721  loss: 0.00863510238286225  hr: 0  min: 5  sec: 27\n",
      "epoch: 2  batch: 299 / 721  loss: 0.008615005781582438  hr: 0  min: 5  sec: 26\n",
      "epoch: 2  batch: 300 / 721  loss: 0.008608676174917491  hr: 0  min: 5  sec: 25\n",
      "epoch: 2  batch: 301 / 721  loss: 0.008581440360469053  hr: 0  min: 5  sec: 25\n",
      "epoch: 2  batch: 302 / 721  loss: 0.008554976043938612  hr: 0  min: 5  sec: 24\n",
      "epoch: 2  batch: 303 / 721  loss: 0.00852989990250922  hr: 0  min: 5  sec: 23\n",
      "epoch: 2  batch: 304 / 721  loss: 0.00850633506816896  hr: 0  min: 5  sec: 22\n",
      "epoch: 2  batch: 305 / 721  loss: 0.008480853355083171  hr: 0  min: 5  sec: 21\n",
      "epoch: 2  batch: 306 / 721  loss: 0.008746950284285347  hr: 0  min: 5  sec: 20\n",
      "epoch: 2  batch: 307 / 721  loss: 0.008727894614119588  hr: 0  min: 5  sec: 19\n",
      "epoch: 2  batch: 308 / 721  loss: 0.008790794817960213  hr: 0  min: 5  sec: 19\n",
      "epoch: 2  batch: 309 / 721  loss: 0.00876408199771966  hr: 0  min: 5  sec: 18\n",
      "epoch: 2  batch: 310 / 721  loss: 0.00893785133202102  hr: 0  min: 5  sec: 17\n",
      "epoch: 2  batch: 311 / 721  loss: 0.00891628062910845  hr: 0  min: 5  sec: 16\n",
      "epoch: 2  batch: 312 / 721  loss: 0.00888950351355351  hr: 0  min: 5  sec: 16\n",
      "epoch: 2  batch: 313 / 721  loss: 0.008869049233745579  hr: 0  min: 5  sec: 15\n",
      "epoch: 2  batch: 314 / 721  loss: 0.008869904359596096  hr: 0  min: 5  sec: 14\n",
      "epoch: 2  batch: 315 / 721  loss: 0.00884493061721081  hr: 0  min: 5  sec: 13\n",
      "epoch: 2  batch: 316 / 721  loss: 0.008923828929669704  hr: 0  min: 5  sec: 12\n",
      "epoch: 2  batch: 317 / 721  loss: 0.008898758626502957  hr: 0  min: 5  sec: 11\n",
      "epoch: 2  batch: 318 / 721  loss: 0.008874022099070481  hr: 0  min: 5  sec: 11\n",
      "epoch: 2  batch: 319 / 721  loss: 0.008847251480496723  hr: 0  min: 5  sec: 10\n",
      "epoch: 2  batch: 320 / 721  loss: 0.008824542096454025  hr: 0  min: 5  sec: 9\n",
      "epoch: 2  batch: 321 / 721  loss: 0.009179885979392566  hr: 0  min: 5  sec: 8\n",
      "epoch: 2  batch: 322 / 721  loss: 0.009160755916618994  hr: 0  min: 5  sec: 8\n",
      "epoch: 2  batch: 323 / 721  loss: 0.009323620191321495  hr: 0  min: 5  sec: 7\n",
      "epoch: 2  batch: 324 / 721  loss: 0.009299599751731292  hr: 0  min: 5  sec: 6\n",
      "epoch: 2  batch: 325 / 721  loss: 0.009341365645612732  hr: 0  min: 5  sec: 5\n",
      "epoch: 2  batch: 326 / 721  loss: 0.00931616397464735  hr: 0  min: 5  sec: 4\n",
      "epoch: 2  batch: 327 / 721  loss: 0.009293221819133708  hr: 0  min: 5  sec: 3\n",
      "epoch: 2  batch: 328 / 721  loss: 0.009285660040759688  hr: 0  min: 5  sec: 3\n",
      "epoch: 2  batch: 329 / 721  loss: 0.009264351916543666  hr: 0  min: 5  sec: 2\n",
      "epoch: 2  batch: 330 / 721  loss: 0.009305540170691639  hr: 0  min: 5  sec: 1\n",
      "epoch: 2  batch: 331 / 721  loss: 0.009287701045724654  hr: 0  min: 5  sec: 0\n",
      "epoch: 2  batch: 332 / 721  loss: 0.009264490568243508  hr: 0  min: 5  sec: 0\n",
      "epoch: 2  batch: 333 / 721  loss: 0.009240925146028044  hr: 0  min: 4  sec: 59\n",
      "epoch: 2  batch: 334 / 721  loss: 0.009260963098242569  hr: 0  min: 4  sec: 58\n",
      "epoch: 2  batch: 335 / 721  loss: 0.009235580086251095  hr: 0  min: 4  sec: 57\n",
      "epoch: 2  batch: 336 / 721  loss: 0.009210191422165994  hr: 0  min: 4  sec: 57\n",
      "epoch: 2  batch: 337 / 721  loss: 0.009416075018885717  hr: 0  min: 4  sec: 56\n",
      "epoch: 2  batch: 338 / 721  loss: 0.00939133859928539  hr: 0  min: 4  sec: 55\n",
      "epoch: 2  batch: 339 / 721  loss: 0.009365496077174032  hr: 0  min: 4  sec: 55\n",
      "epoch: 2  batch: 340 / 721  loss: 0.009345788861879983  hr: 0  min: 4  sec: 54\n",
      "epoch: 2  batch: 341 / 721  loss: 0.009329335744121614  hr: 0  min: 4  sec: 53\n",
      "epoch: 2  batch: 342 / 721  loss: 0.00930868860795224  hr: 0  min: 4  sec: 52\n",
      "epoch: 2  batch: 343 / 721  loss: 0.009340471192882479  hr: 0  min: 4  sec: 51\n",
      "epoch: 2  batch: 344 / 721  loss: 0.009319093562860575  hr: 0  min: 4  sec: 50\n",
      "epoch: 2  batch: 345 / 721  loss: 0.009294172381666705  hr: 0  min: 4  sec: 50\n",
      "epoch: 2  batch: 346 / 721  loss: 0.00927812654735834  hr: 0  min: 4  sec: 49\n",
      "epoch: 2  batch: 347 / 721  loss: 0.009262821357168851  hr: 0  min: 4  sec: 49\n",
      "epoch: 2  batch: 348 / 721  loss: 0.009243249248095385  hr: 0  min: 4  sec: 48\n",
      "epoch: 2  batch: 349 / 721  loss: 0.009220278277377102  hr: 0  min: 4  sec: 47\n",
      "epoch: 2  batch: 350 / 721  loss: 0.00919662350593301  hr: 0  min: 4  sec: 46\n",
      "epoch: 2  batch: 351 / 721  loss: 0.009249006019299252  hr: 0  min: 4  sec: 46\n",
      "epoch: 2  batch: 352 / 721  loss: 0.009226766450700035  hr: 0  min: 4  sec: 45\n",
      "epoch: 2  batch: 353 / 721  loss: 0.009202258540068425  hr: 0  min: 4  sec: 44\n",
      "epoch: 2  batch: 354 / 721  loss: 0.009339378073347514  hr: 0  min: 4  sec: 43\n",
      "epoch: 2  batch: 355 / 721  loss: 0.009368594148250389  hr: 0  min: 4  sec: 43\n",
      "epoch: 2  batch: 356 / 721  loss: 0.009345451891502389  hr: 0  min: 4  sec: 42\n",
      "epoch: 2  batch: 357 / 721  loss: 0.009387053662484853  hr: 0  min: 4  sec: 41\n",
      "epoch: 2  batch: 358 / 721  loss: 0.009496813409484717  hr: 0  min: 4  sec: 40\n",
      "epoch: 2  batch: 359 / 721  loss: 0.009495276006687405  hr: 0  min: 4  sec: 39\n",
      "epoch: 2  batch: 360 / 721  loss: 0.009471187372926376  hr: 0  min: 4  sec: 39\n",
      "epoch: 2  batch: 361 / 721  loss: 0.009477849775988808  hr: 0  min: 4  sec: 38\n",
      "epoch: 2  batch: 362 / 721  loss: 0.00945613559825733  hr: 0  min: 4  sec: 37\n",
      "epoch: 2  batch: 363 / 721  loss: 0.009470117997454118  hr: 0  min: 4  sec: 36\n",
      "epoch: 2  batch: 364 / 721  loss: 0.009474745092590168  hr: 0  min: 4  sec: 35\n",
      "epoch: 2  batch: 365 / 721  loss: 0.00947975063738764  hr: 0  min: 4  sec: 35\n",
      "epoch: 2  batch: 366 / 721  loss: 0.009457426876648103  hr: 0  min: 4  sec: 34\n",
      "epoch: 2  batch: 367 / 721  loss: 0.009434049065824032  hr: 0  min: 4  sec: 33\n",
      "epoch: 2  batch: 368 / 721  loss: 0.009414708343565113  hr: 0  min: 4  sec: 32\n",
      "epoch: 2  batch: 369 / 721  loss: 0.009392264035543323  hr: 0  min: 4  sec: 31\n",
      "epoch: 2  batch: 370 / 721  loss: 0.00937750397672455  hr: 0  min: 4  sec: 30\n",
      "epoch: 2  batch: 371 / 721  loss: 0.009362179434092933  hr: 0  min: 4  sec: 30\n",
      "epoch: 2  batch: 372 / 721  loss: 0.009351427396028665  hr: 0  min: 4  sec: 29\n",
      "epoch: 2  batch: 373 / 721  loss: 0.009327355253654681  hr: 0  min: 4  sec: 28\n",
      "epoch: 2  batch: 374 / 721  loss: 0.009306033292060338  hr: 0  min: 4  sec: 27\n",
      "epoch: 2  batch: 375 / 721  loss: 0.009291699051352529  hr: 0  min: 4  sec: 26\n",
      "epoch: 2  batch: 376 / 721  loss: 0.00927356190178661  hr: 0  min: 4  sec: 26\n",
      "epoch: 2  batch: 377 / 721  loss: 0.009250834698142515  hr: 0  min: 4  sec: 25\n",
      "epoch: 2  batch: 378 / 721  loss: 0.00923549326005972  hr: 0  min: 4  sec: 24\n",
      "epoch: 2  batch: 379 / 721  loss: 0.009212026776972295  hr: 0  min: 4  sec: 23\n",
      "epoch: 2  batch: 380 / 721  loss: 0.009188450257084027  hr: 0  min: 4  sec: 23\n",
      "epoch: 2  batch: 381 / 721  loss: 0.009167569138198599  hr: 0  min: 4  sec: 22\n",
      "epoch: 2  batch: 382 / 721  loss: 0.009144283348811164  hr: 0  min: 4  sec: 21\n",
      "epoch: 2  batch: 383 / 721  loss: 0.009168231364164206  hr: 0  min: 4  sec: 20\n",
      "epoch: 2  batch: 384 / 721  loss: 0.009148361368829683  hr: 0  min: 4  sec: 20\n",
      "epoch: 2  batch: 385 / 721  loss: 0.009129155698865833  hr: 0  min: 4  sec: 19\n",
      "epoch: 2  batch: 386 / 721  loss: 0.00910679518345139  hr: 0  min: 4  sec: 18\n",
      "epoch: 2  batch: 387 / 721  loss: 0.009086239129960361  hr: 0  min: 4  sec: 17\n",
      "epoch: 2  batch: 388 / 721  loss: 0.009064516940833125  hr: 0  min: 4  sec: 17\n",
      "epoch: 2  batch: 389 / 721  loss: 0.009050989172899926  hr: 0  min: 4  sec: 16\n",
      "epoch: 2  batch: 390 / 721  loss: 0.009028700537694535  hr: 0  min: 4  sec: 15\n",
      "epoch: 2  batch: 391 / 721  loss: 0.009006212718765158  hr: 0  min: 4  sec: 14\n",
      "epoch: 2  batch: 392 / 721  loss: 0.008983806966486499  hr: 0  min: 4  sec: 13\n",
      "epoch: 2  batch: 393 / 721  loss: 0.009048312990687746  hr: 0  min: 4  sec: 13\n",
      "epoch: 2  batch: 394 / 721  loss: 0.009050124958900648  hr: 0  min: 4  sec: 12\n",
      "epoch: 2  batch: 395 / 721  loss: 0.00904943953924724  hr: 0  min: 4  sec: 11\n",
      "epoch: 2  batch: 396 / 721  loss: 0.009036021499335706  hr: 0  min: 4  sec: 10\n",
      "epoch: 2  batch: 397 / 721  loss: 0.009052128816615516  hr: 0  min: 4  sec: 9\n",
      "epoch: 2  batch: 398 / 721  loss: 0.009056163260831097  hr: 0  min: 4  sec: 9\n",
      "epoch: 2  batch: 399 / 721  loss: 0.009034590457440914  hr: 0  min: 4  sec: 8\n",
      "epoch: 2  batch: 400 / 721  loss: 0.009012769684522937  hr: 0  min: 4  sec: 7\n",
      "epoch: 2  batch: 401 / 721  loss: 0.009320844922011818  hr: 0  min: 4  sec: 6\n",
      "epoch: 2  batch: 402 / 721  loss: 0.009551490458119162  hr: 0  min: 4  sec: 5\n",
      "epoch: 2  batch: 403 / 721  loss: 0.009530360131357246  hr: 0  min: 4  sec: 5\n",
      "epoch: 2  batch: 404 / 721  loss: 0.00954973805811179  hr: 0  min: 4  sec: 4\n",
      "epoch: 2  batch: 405 / 721  loss: 0.009527893773643702  hr: 0  min: 4  sec: 3\n",
      "epoch: 2  batch: 406 / 721  loss: 0.009505391825531584  hr: 0  min: 4  sec: 3\n",
      "epoch: 2  batch: 407 / 721  loss: 0.009596174356694383  hr: 0  min: 4  sec: 2\n",
      "epoch: 2  batch: 408 / 721  loss: 0.00957666110119973  hr: 0  min: 4  sec: 1\n",
      "epoch: 2  batch: 409 / 721  loss: 0.00956085531824766  hr: 0  min: 4  sec: 0\n",
      "epoch: 2  batch: 410 / 721  loss: 0.009543041484366546  hr: 0  min: 4  sec: 0\n",
      "epoch: 2  batch: 411 / 721  loss: 0.009522164224582122  hr: 0  min: 3  sec: 59\n",
      "epoch: 2  batch: 412 / 721  loss: 0.00950682159109596  hr: 0  min: 3  sec: 58\n",
      "epoch: 2  batch: 413 / 721  loss: 0.009495700785593406  hr: 0  min: 3  sec: 57\n",
      "epoch: 2  batch: 414 / 721  loss: 0.009482983332699568  hr: 0  min: 3  sec: 57\n",
      "epoch: 2  batch: 415 / 721  loss: 0.009464778982808376  hr: 0  min: 3  sec: 56\n",
      "epoch: 2  batch: 416 / 721  loss: 0.009446085692581199  hr: 0  min: 3  sec: 55\n",
      "epoch: 2  batch: 417 / 721  loss: 0.009428116946269834  hr: 0  min: 3  sec: 54\n",
      "epoch: 2  batch: 418 / 721  loss: 0.00941264442928914  hr: 0  min: 3  sec: 53\n",
      "epoch: 2  batch: 419 / 721  loss: 0.009394797337260086  hr: 0  min: 3  sec: 53\n",
      "epoch: 2  batch: 420 / 721  loss: 0.00945225475391323  hr: 0  min: 3  sec: 52\n",
      "epoch: 2  batch: 421 / 721  loss: 0.009432163550716166  hr: 0  min: 3  sec: 51\n",
      "epoch: 2  batch: 422 / 721  loss: 0.009411296063736996  hr: 0  min: 3  sec: 50\n",
      "epoch: 2  batch: 423 / 721  loss: 0.009570906086499808  hr: 0  min: 3  sec: 50\n",
      "epoch: 2  batch: 424 / 721  loss: 0.009560463855828033  hr: 0  min: 3  sec: 49\n",
      "epoch: 2  batch: 425 / 721  loss: 0.009541111524894778  hr: 0  min: 3  sec: 48\n",
      "epoch: 2  batch: 426 / 721  loss: 0.009520726723009184  hr: 0  min: 3  sec: 47\n",
      "epoch: 2  batch: 427 / 721  loss: 0.009500200406230268  hr: 0  min: 3  sec: 46\n",
      "epoch: 2  batch: 428 / 721  loss: 0.009481412165429144  hr: 0  min: 3  sec: 46\n",
      "epoch: 2  batch: 429 / 721  loss: 0.00946120315839646  hr: 0  min: 3  sec: 45\n",
      "epoch: 2  batch: 430 / 721  loss: 0.00945083742435388  hr: 0  min: 3  sec: 44\n",
      "epoch: 2  batch: 431 / 721  loss: 0.009441403106330104  hr: 0  min: 3  sec: 44\n",
      "epoch: 2  batch: 432 / 721  loss: 0.009450655588302147  hr: 0  min: 3  sec: 43\n",
      "epoch: 2  batch: 433 / 721  loss: 0.009429599304421908  hr: 0  min: 3  sec: 42\n",
      "epoch: 2  batch: 434 / 721  loss: 0.009408244405035413  hr: 0  min: 3  sec: 41\n",
      "epoch: 2  batch: 435 / 721  loss: 0.009394485090796344  hr: 0  min: 3  sec: 40\n",
      "epoch: 2  batch: 436 / 721  loss: 0.00937521598602078  hr: 0  min: 3  sec: 40\n",
      "epoch: 2  batch: 437 / 721  loss: 0.009358152300973201  hr: 0  min: 3  sec: 39\n",
      "epoch: 2  batch: 438 / 721  loss: 0.009341069714084627  hr: 0  min: 3  sec: 38\n",
      "epoch: 2  batch: 439 / 721  loss: 0.009326359665516927  hr: 0  min: 3  sec: 37\n",
      "epoch: 2  batch: 440 / 721  loss: 0.009462473106561281  hr: 0  min: 3  sec: 37\n",
      "epoch: 2  batch: 441 / 721  loss: 0.009442655173930985  hr: 0  min: 3  sec: 36\n",
      "epoch: 2  batch: 442 / 721  loss: 0.009421874428181373  hr: 0  min: 3  sec: 35\n",
      "epoch: 2  batch: 443 / 721  loss: 0.009402158116793279  hr: 0  min: 3  sec: 34\n",
      "epoch: 2  batch: 444 / 721  loss: 0.009381537100944935  hr: 0  min: 3  sec: 34\n",
      "epoch: 2  batch: 445 / 721  loss: 0.009364455289781626  hr: 0  min: 3  sec: 33\n",
      "epoch: 2  batch: 446 / 721  loss: 0.00936149463138826  hr: 0  min: 3  sec: 32\n",
      "epoch: 2  batch: 447 / 721  loss: 0.009344215965505545  hr: 0  min: 3  sec: 31\n",
      "epoch: 2  batch: 448 / 721  loss: 0.009325487893192985  hr: 0  min: 3  sec: 31\n",
      "epoch: 2  batch: 449 / 721  loss: 0.00930564528200096  hr: 0  min: 3  sec: 30\n",
      "epoch: 2  batch: 450 / 721  loss: 0.009286650556896347  hr: 0  min: 3  sec: 29\n",
      "epoch: 2  batch: 451 / 721  loss: 0.009266554966051143  hr: 0  min: 3  sec: 29\n",
      "epoch: 2  batch: 452 / 721  loss: 0.009247652057517604  hr: 0  min: 3  sec: 28\n",
      "epoch: 2  batch: 453 / 721  loss: 0.009229813379435502  hr: 0  min: 3  sec: 27\n",
      "epoch: 2  batch: 454 / 721  loss: 0.009215103357013352  hr: 0  min: 3  sec: 26\n",
      "epoch: 2  batch: 455 / 721  loss: 0.009325037300292464  hr: 0  min: 3  sec: 25\n",
      "epoch: 2  batch: 456 / 721  loss: 0.009305473977037125  hr: 0  min: 3  sec: 25\n",
      "epoch: 2  batch: 457 / 721  loss: 0.009286201212053846  hr: 0  min: 3  sec: 24\n",
      "epoch: 2  batch: 458 / 721  loss: 0.009314983141177444  hr: 0  min: 3  sec: 23\n",
      "epoch: 2  batch: 459 / 721  loss: 0.009296353901700931  hr: 0  min: 3  sec: 22\n",
      "epoch: 2  batch: 460 / 721  loss: 0.009276984139527801  hr: 0  min: 3  sec: 21\n",
      "epoch: 2  batch: 461 / 721  loss: 0.009265509415357302  hr: 0  min: 3  sec: 21\n",
      "epoch: 2  batch: 462 / 721  loss: 0.00924836122456059  hr: 0  min: 3  sec: 20\n",
      "epoch: 2  batch: 463 / 721  loss: 0.009243096742169533  hr: 0  min: 3  sec: 19\n",
      "epoch: 2  batch: 464 / 721  loss: 0.009227433921578581  hr: 0  min: 3  sec: 18\n",
      "epoch: 2  batch: 465 / 721  loss: 0.009214637125842274  hr: 0  min: 3  sec: 18\n",
      "epoch: 2  batch: 466 / 721  loss: 0.009218295710111365  hr: 0  min: 3  sec: 17\n",
      "epoch: 2  batch: 467 / 721  loss: 0.009200144718810956  hr: 0  min: 3  sec: 16\n",
      "epoch: 2  batch: 468 / 721  loss: 0.009181355269731253  hr: 0  min: 3  sec: 15\n",
      "epoch: 2  batch: 469 / 721  loss: 0.009163241110630889  hr: 0  min: 3  sec: 15\n",
      "epoch: 2  batch: 470 / 721  loss: 0.009161577831478194  hr: 0  min: 3  sec: 14\n",
      "epoch: 2  batch: 471 / 721  loss: 0.009158009847115938  hr: 0  min: 3  sec: 13\n",
      "epoch: 2  batch: 472 / 721  loss: 0.009139865463065827  hr: 0  min: 3  sec: 12\n",
      "epoch: 2  batch: 473 / 721  loss: 0.009122396230325496  hr: 0  min: 3  sec: 11\n",
      "epoch: 2  batch: 474 / 721  loss: 0.009127933659466998  hr: 0  min: 3  sec: 11\n",
      "epoch: 2  batch: 475 / 721  loss: 0.009118147940178843  hr: 0  min: 3  sec: 10\n",
      "epoch: 2  batch: 476 / 721  loss: 0.009142581123747166  hr: 0  min: 3  sec: 9\n",
      "epoch: 2  batch: 477 / 721  loss: 0.009131437047205053  hr: 0  min: 3  sec: 8\n",
      "epoch: 2  batch: 478 / 721  loss: 0.00911389070060825  hr: 0  min: 3  sec: 7\n",
      "epoch: 2  batch: 479 / 721  loss: 0.009095291649887002  hr: 0  min: 3  sec: 7\n",
      "epoch: 2  batch: 480 / 721  loss: 0.009077039802620371  hr: 0  min: 3  sec: 6\n",
      "epoch: 2  batch: 481 / 721  loss: 0.009070993314191976  hr: 0  min: 3  sec: 5\n",
      "epoch: 2  batch: 482 / 721  loss: 0.009057316648660574  hr: 0  min: 3  sec: 4\n",
      "epoch: 2  batch: 483 / 721  loss: 0.009039041907898938  hr: 0  min: 3  sec: 4\n",
      "epoch: 2  batch: 484 / 721  loss: 0.009020806028304885  hr: 0  min: 3  sec: 3\n",
      "epoch: 2  batch: 485 / 721  loss: 0.009135933258581532  hr: 0  min: 3  sec: 2\n",
      "epoch: 2  batch: 486 / 721  loss: 0.009117936397706965  hr: 0  min: 3  sec: 1\n",
      "epoch: 2  batch: 487 / 721  loss: 0.009100309171367511  hr: 0  min: 3  sec: 1\n",
      "epoch: 2  batch: 488 / 721  loss: 0.009085138423885286  hr: 0  min: 3  sec: 0\n",
      "epoch: 2  batch: 489 / 721  loss: 0.009067624163591772  hr: 0  min: 2  sec: 59\n",
      "epoch: 2  batch: 490 / 721  loss: 0.009049952815599236  hr: 0  min: 2  sec: 58\n",
      "epoch: 2  batch: 491 / 721  loss: 0.009051860065896268  hr: 0  min: 2  sec: 58\n",
      "epoch: 2  batch: 492 / 721  loss: 0.009034770519449706  hr: 0  min: 2  sec: 57\n",
      "epoch: 2  batch: 493 / 721  loss: 0.00902235050789958  hr: 0  min: 2  sec: 56\n",
      "epoch: 2  batch: 494 / 721  loss: 0.009035355899607158  hr: 0  min: 2  sec: 55\n",
      "epoch: 2  batch: 495 / 721  loss: 0.009024593959217939  hr: 0  min: 2  sec: 54\n",
      "epoch: 2  batch: 496 / 721  loss: 0.009020959114276328  hr: 0  min: 2  sec: 54\n",
      "epoch: 2  batch: 497 / 721  loss: 0.009003822520951239  hr: 0  min: 2  sec: 53\n",
      "epoch: 2  batch: 498 / 721  loss: 0.00899771671906655  hr: 0  min: 2  sec: 52\n",
      "epoch: 2  batch: 499 / 721  loss: 0.009047782275085486  hr: 0  min: 2  sec: 51\n",
      "epoch: 2  batch: 500 / 721  loss: 0.009037827169260708  hr: 0  min: 2  sec: 51\n",
      "epoch: 2  batch: 501 / 721  loss: 0.009020704079677068  hr: 0  min: 2  sec: 50\n",
      "epoch: 2  batch: 502 / 721  loss: 0.009004736668227353  hr: 0  min: 2  sec: 49\n",
      "epoch: 2  batch: 503 / 721  loss: 0.008987841425861868  hr: 0  min: 2  sec: 48\n",
      "epoch: 2  batch: 504 / 721  loss: 0.008971707306074446  hr: 0  min: 2  sec: 48\n",
      "epoch: 2  batch: 505 / 721  loss: 0.008972862167520182  hr: 0  min: 2  sec: 47\n",
      "epoch: 2  batch: 506 / 721  loss: 0.008955860914327268  hr: 0  min: 2  sec: 46\n",
      "epoch: 2  batch: 507 / 721  loss: 0.00893979717772872  hr: 0  min: 2  sec: 45\n",
      "epoch: 2  batch: 508 / 721  loss: 0.008922828965095985  hr: 0  min: 2  sec: 44\n",
      "epoch: 2  batch: 509 / 721  loss: 0.008909422362287079  hr: 0  min: 2  sec: 44\n",
      "epoch: 2  batch: 510 / 721  loss: 0.00889305672320131  hr: 0  min: 2  sec: 43\n",
      "epoch: 2  batch: 511 / 721  loss: 0.008876402758684059  hr: 0  min: 2  sec: 42\n",
      "epoch: 2  batch: 512 / 721  loss: 0.009243268843619035  hr: 0  min: 2  sec: 41\n",
      "epoch: 2  batch: 513 / 721  loss: 0.009226681374313653  hr: 0  min: 2  sec: 40\n",
      "epoch: 2  batch: 514 / 721  loss: 0.009209936058866938  hr: 0  min: 2  sec: 40\n",
      "epoch: 2  batch: 515 / 721  loss: 0.009193974643808934  hr: 0  min: 2  sec: 39\n",
      "epoch: 2  batch: 516 / 721  loss: 0.009179451982480566  hr: 0  min: 2  sec: 38\n",
      "epoch: 2  batch: 517 / 721  loss: 0.00916337135041964  hr: 0  min: 2  sec: 37\n",
      "epoch: 2  batch: 518 / 721  loss: 0.009148268975333435  hr: 0  min: 2  sec: 37\n",
      "epoch: 2  batch: 519 / 721  loss: 0.009133351330912531  hr: 0  min: 2  sec: 36\n",
      "epoch: 2  batch: 520 / 721  loss: 0.009117195032656757  hr: 0  min: 2  sec: 35\n",
      "epoch: 2  batch: 521 / 721  loss: 0.009100484012396185  hr: 0  min: 2  sec: 34\n",
      "epoch: 2  batch: 522 / 721  loss: 0.009084585441986297  hr: 0  min: 2  sec: 33\n",
      "epoch: 2  batch: 523 / 721  loss: 0.009070287503200374  hr: 0  min: 2  sec: 33\n",
      "epoch: 2  batch: 524 / 721  loss: 0.009083401163028445  hr: 0  min: 2  sec: 32\n",
      "epoch: 2  batch: 525 / 721  loss: 0.009073535913595974  hr: 0  min: 2  sec: 31\n",
      "epoch: 2  batch: 526 / 721  loss: 0.009105416999313363  hr: 0  min: 2  sec: 30\n",
      "epoch: 2  batch: 527 / 721  loss: 0.009125783212523355  hr: 0  min: 2  sec: 30\n",
      "epoch: 2  batch: 528 / 721  loss: 0.009110052693276057  hr: 0  min: 2  sec: 29\n",
      "epoch: 2  batch: 529 / 721  loss: 0.009093913103409582  hr: 0  min: 2  sec: 28\n",
      "epoch: 2  batch: 530 / 721  loss: 0.0090833564942103  hr: 0  min: 2  sec: 27\n",
      "epoch: 2  batch: 531 / 721  loss: 0.009067414778523917  hr: 0  min: 2  sec: 26\n",
      "epoch: 2  batch: 532 / 721  loss: 0.009050810893846998  hr: 0  min: 2  sec: 26\n",
      "epoch: 2  batch: 533 / 721  loss: 0.009035336419609751  hr: 0  min: 2  sec: 25\n",
      "epoch: 2  batch: 534 / 721  loss: 0.00911421222374199  hr: 0  min: 2  sec: 24\n",
      "epoch: 2  batch: 535 / 721  loss: 0.0091331293752034  hr: 0  min: 2  sec: 23\n",
      "epoch: 2  batch: 536 / 721  loss: 0.009117042336312345  hr: 0  min: 2  sec: 22\n",
      "epoch: 2  batch: 537 / 721  loss: 0.00910072125698478  hr: 0  min: 2  sec: 22\n",
      "epoch: 2  batch: 538 / 721  loss: 0.009085254084423174  hr: 0  min: 2  sec: 21\n",
      "epoch: 2  batch: 539 / 721  loss: 0.009072482616846676  hr: 0  min: 2  sec: 20\n",
      "epoch: 2  batch: 540 / 721  loss: 0.009076020750917787  hr: 0  min: 2  sec: 19\n",
      "epoch: 2  batch: 541 / 721  loss: 0.00906030233273095  hr: 0  min: 2  sec: 19\n",
      "epoch: 2  batch: 542 / 721  loss: 0.009045767813555405  hr: 0  min: 2  sec: 18\n",
      "epoch: 2  batch: 543 / 721  loss: 0.009030561151575129  hr: 0  min: 2  sec: 17\n",
      "epoch: 2  batch: 544 / 721  loss: 0.009019712946236028  hr: 0  min: 2  sec: 16\n",
      "epoch: 2  batch: 545 / 721  loss: 0.009008844635169527  hr: 0  min: 2  sec: 16\n",
      "epoch: 2  batch: 546 / 721  loss: 0.009029666399431839  hr: 0  min: 2  sec: 15\n",
      "epoch: 2  batch: 547 / 721  loss: 0.009016654884557187  hr: 0  min: 2  sec: 14\n",
      "epoch: 2  batch: 548 / 721  loss: 0.009003884136563348  hr: 0  min: 2  sec: 13\n",
      "epoch: 2  batch: 549 / 721  loss: 0.008988649900996284  hr: 0  min: 2  sec: 12\n",
      "epoch: 2  batch: 550 / 721  loss: 0.00897626933625857  hr: 0  min: 2  sec: 12\n",
      "epoch: 2  batch: 551 / 721  loss: 0.008978065237508133  hr: 0  min: 2  sec: 11\n",
      "epoch: 2  batch: 552 / 721  loss: 0.008963047911928024  hr: 0  min: 2  sec: 10\n",
      "epoch: 2  batch: 553 / 721  loss: 0.008949164163111163  hr: 0  min: 2  sec: 9\n",
      "epoch: 2  batch: 554 / 721  loss: 0.00893856566455022  hr: 0  min: 2  sec: 8\n",
      "epoch: 2  batch: 555 / 721  loss: 0.008922979104917848  hr: 0  min: 2  sec: 8\n",
      "epoch: 2  batch: 556 / 721  loss: 0.00890874256112524  hr: 0  min: 2  sec: 7\n",
      "epoch: 2  batch: 557 / 721  loss: 0.008893632916204841  hr: 0  min: 2  sec: 6\n",
      "epoch: 2  batch: 558 / 721  loss: 0.009400935589215271  hr: 0  min: 2  sec: 5\n",
      "epoch: 2  batch: 559 / 721  loss: 0.009385128658127843  hr: 0  min: 2  sec: 5\n",
      "epoch: 2  batch: 560 / 721  loss: 0.009373799348655407  hr: 0  min: 2  sec: 4\n",
      "epoch: 2  batch: 561 / 721  loss: 0.00935845430387365  hr: 0  min: 2  sec: 3\n",
      "epoch: 2  batch: 562 / 721  loss: 0.009357696336897477  hr: 0  min: 2  sec: 2\n",
      "epoch: 2  batch: 563 / 721  loss: 0.009377685470577267  hr: 0  min: 2  sec: 2\n",
      "epoch: 2  batch: 564 / 721  loss: 0.0096332356341473  hr: 0  min: 2  sec: 1\n",
      "epoch: 2  batch: 565 / 721  loss: 0.0096220296848281  hr: 0  min: 2  sec: 0\n",
      "epoch: 2  batch: 566 / 721  loss: 0.009678676910317905  hr: 0  min: 1  sec: 59\n",
      "epoch: 2  batch: 567 / 721  loss: 0.00967182716057222  hr: 0  min: 1  sec: 58\n",
      "epoch: 2  batch: 568 / 721  loss: 0.009671718275621714  hr: 0  min: 1  sec: 58\n",
      "epoch: 2  batch: 569 / 721  loss: 0.009673530153996886  hr: 0  min: 1  sec: 57\n",
      "epoch: 2  batch: 570 / 721  loss: 0.00969512957028775  hr: 0  min: 1  sec: 56\n",
      "epoch: 2  batch: 571 / 721  loss: 0.009686994864578862  hr: 0  min: 1  sec: 55\n",
      "epoch: 2  batch: 572 / 721  loss: 0.009677418203113871  hr: 0  min: 1  sec: 55\n",
      "epoch: 2  batch: 573 / 721  loss: 0.009675633267856676  hr: 0  min: 1  sec: 54\n",
      "epoch: 2  batch: 574 / 721  loss: 0.009673134547052662  hr: 0  min: 1  sec: 53\n",
      "epoch: 2  batch: 575 / 721  loss: 0.009670643382047749  hr: 0  min: 1  sec: 52\n",
      "epoch: 2  batch: 576 / 721  loss: 0.009667595048035259  hr: 0  min: 1  sec: 52\n",
      "epoch: 2  batch: 577 / 721  loss: 0.009673310666451185  hr: 0  min: 1  sec: 51\n",
      "epoch: 2  batch: 578 / 721  loss: 0.00966198110438772  hr: 0  min: 1  sec: 50\n",
      "epoch: 2  batch: 579 / 721  loss: 0.00967143325770269  hr: 0  min: 1  sec: 49\n",
      "epoch: 2  batch: 580 / 721  loss: 0.009675337735449109  hr: 0  min: 1  sec: 48\n",
      "epoch: 2  batch: 581 / 721  loss: 0.009667921328799728  hr: 0  min: 1  sec: 48\n",
      "epoch: 2  batch: 582 / 721  loss: 0.009740891554294862  hr: 0  min: 1  sec: 47\n",
      "epoch: 2  batch: 583 / 721  loss: 0.009724889764994363  hr: 0  min: 1  sec: 46\n",
      "epoch: 2  batch: 584 / 721  loss: 0.009711654426323485  hr: 0  min: 1  sec: 45\n",
      "epoch: 2  batch: 585 / 721  loss: 0.009696228283310198  hr: 0  min: 1  sec: 45\n",
      "epoch: 2  batch: 586 / 721  loss: 0.009682526059131656  hr: 0  min: 1  sec: 44\n",
      "epoch: 2  batch: 587 / 721  loss: 0.009680264767951901  hr: 0  min: 1  sec: 43\n",
      "epoch: 2  batch: 588 / 721  loss: 0.009664853357968909  hr: 0  min: 1  sec: 42\n",
      "epoch: 2  batch: 589 / 721  loss: 0.009649387586973323  hr: 0  min: 1  sec: 42\n",
      "epoch: 2  batch: 590 / 721  loss: 0.009635225347523436  hr: 0  min: 1  sec: 41\n",
      "epoch: 2  batch: 591 / 721  loss: 0.009619742693468556  hr: 0  min: 1  sec: 40\n",
      "epoch: 2  batch: 592 / 721  loss: 0.009617270069685293  hr: 0  min: 1  sec: 39\n",
      "epoch: 2  batch: 593 / 721  loss: 0.009604146035720278  hr: 0  min: 1  sec: 38\n",
      "epoch: 2  batch: 594 / 721  loss: 0.009588744126511926  hr: 0  min: 1  sec: 38\n",
      "epoch: 2  batch: 595 / 721  loss: 0.00957650806717653  hr: 0  min: 1  sec: 37\n",
      "epoch: 2  batch: 596 / 721  loss: 0.00956324743462067  hr: 0  min: 1  sec: 36\n",
      "epoch: 2  batch: 597 / 721  loss: 0.009547901498705658  hr: 0  min: 1  sec: 35\n",
      "epoch: 2  batch: 598 / 721  loss: 0.009532328319994158  hr: 0  min: 1  sec: 35\n",
      "epoch: 2  batch: 599 / 721  loss: 0.009517095072428348  hr: 0  min: 1  sec: 34\n",
      "epoch: 2  batch: 600 / 721  loss: 0.009501950760435042  hr: 0  min: 1  sec: 33\n",
      "epoch: 2  batch: 601 / 721  loss: 0.009581818018549668  hr: 0  min: 1  sec: 32\n",
      "epoch: 2  batch: 602 / 721  loss: 0.009567909127015634  hr: 0  min: 1  sec: 31\n",
      "epoch: 2  batch: 603 / 721  loss: 0.009623633409028056  hr: 0  min: 1  sec: 31\n",
      "epoch: 2  batch: 604 / 721  loss: 0.00960787685247526  hr: 0  min: 1  sec: 30\n",
      "epoch: 2  batch: 605 / 721  loss: 0.01001984830279469  hr: 0  min: 1  sec: 29\n",
      "epoch: 2  batch: 606 / 721  loss: 0.01000433638051714  hr: 0  min: 1  sec: 28\n",
      "epoch: 2  batch: 607 / 721  loss: 0.010173453939011316  hr: 0  min: 1  sec: 28\n",
      "epoch: 2  batch: 608 / 721  loss: 0.010159548833263742  hr: 0  min: 1  sec: 27\n",
      "epoch: 2  batch: 609 / 721  loss: 0.010144301690041754  hr: 0  min: 1  sec: 26\n",
      "epoch: 2  batch: 610 / 721  loss: 0.010136676117750152  hr: 0  min: 1  sec: 25\n",
      "epoch: 2  batch: 611 / 721  loss: 0.010126735049343909  hr: 0  min: 1  sec: 25\n",
      "epoch: 2  batch: 612 / 721  loss: 0.01011171012302595  hr: 0  min: 1  sec: 24\n",
      "epoch: 2  batch: 613 / 721  loss: 0.010097503127296816  hr: 0  min: 1  sec: 23\n",
      "epoch: 2  batch: 614 / 721  loss: 0.010082760008547041  hr: 0  min: 1  sec: 22\n",
      "epoch: 2  batch: 615 / 721  loss: 0.010070439422767178  hr: 0  min: 1  sec: 22\n",
      "epoch: 2  batch: 616 / 721  loss: 0.010054877006442947  hr: 0  min: 1  sec: 21\n",
      "epoch: 2  batch: 617 / 721  loss: 0.010039482582055788  hr: 0  min: 1  sec: 20\n",
      "epoch: 2  batch: 618 / 721  loss: 0.010024314361058731  hr: 0  min: 1  sec: 19\n",
      "epoch: 2  batch: 619 / 721  loss: 0.010013875594237174  hr: 0  min: 1  sec: 18\n",
      "epoch: 2  batch: 620 / 721  loss: 0.009999778811212426  hr: 0  min: 1  sec: 18\n",
      "epoch: 2  batch: 621 / 721  loss: 0.009984840326924694  hr: 0  min: 1  sec: 17\n",
      "epoch: 2  batch: 622 / 721  loss: 0.009976707887690749  hr: 0  min: 1  sec: 16\n",
      "epoch: 2  batch: 623 / 721  loss: 0.009961902350621757  hr: 0  min: 1  sec: 15\n",
      "epoch: 2  batch: 624 / 721  loss: 0.0099504593767844  hr: 0  min: 1  sec: 15\n",
      "epoch: 2  batch: 625 / 721  loss: 0.010050661650509574  hr: 0  min: 1  sec: 14\n",
      "epoch: 2  batch: 626 / 721  loss: 0.010035370234670616  hr: 0  min: 1  sec: 13\n",
      "epoch: 2  batch: 627 / 721  loss: 0.01001963082352743  hr: 0  min: 1  sec: 12\n",
      "epoch: 2  batch: 628 / 721  loss: 0.010004462646569791  hr: 0  min: 1  sec: 11\n",
      "epoch: 2  batch: 629 / 721  loss: 0.009990079604210117  hr: 0  min: 1  sec: 11\n",
      "epoch: 2  batch: 630 / 721  loss: 0.009974602522191754  hr: 0  min: 1  sec: 10\n",
      "epoch: 2  batch: 631 / 721  loss: 0.009960181129710356  hr: 0  min: 1  sec: 9\n",
      "epoch: 2  batch: 632 / 721  loss: 0.009947847717613294  hr: 0  min: 1  sec: 8\n",
      "epoch: 2  batch: 633 / 721  loss: 0.009934088441349808  hr: 0  min: 1  sec: 8\n",
      "epoch: 2  batch: 634 / 721  loss: 0.00992099825270424  hr: 0  min: 1  sec: 7\n",
      "epoch: 2  batch: 635 / 721  loss: 0.010074325829403343  hr: 0  min: 1  sec: 6\n",
      "epoch: 2  batch: 636 / 721  loss: 0.010059988108678112  hr: 0  min: 1  sec: 5\n",
      "epoch: 2  batch: 637 / 721  loss: 0.010051013078058819  hr: 0  min: 1  sec: 5\n",
      "epoch: 2  batch: 638 / 721  loss: 0.010036781529416432  hr: 0  min: 1  sec: 4\n",
      "epoch: 2  batch: 639 / 721  loss: 0.010023371134217864  hr: 0  min: 1  sec: 3\n",
      "epoch: 2  batch: 640 / 721  loss: 0.010008438539307463  hr: 0  min: 1  sec: 2\n",
      "epoch: 2  batch: 641 / 721  loss: 0.010025601357001123  hr: 0  min: 1  sec: 2\n",
      "epoch: 2  batch: 642 / 721  loss: 0.010015386672673465  hr: 0  min: 1  sec: 1\n",
      "epoch: 2  batch: 643 / 721  loss: 0.010001356292939289  hr: 0  min: 1  sec: 0\n",
      "epoch: 2  batch: 644 / 721  loss: 0.009993272542352253  hr: 0  min: 0  sec: 59\n",
      "epoch: 2  batch: 645 / 721  loss: 0.0099796699315342  hr: 0  min: 0  sec: 58\n",
      "epoch: 2  batch: 646 / 721  loss: 0.009965577949281428  hr: 0  min: 0  sec: 58\n",
      "epoch: 2  batch: 647 / 721  loss: 0.00995083198104618  hr: 0  min: 0  sec: 57\n",
      "epoch: 2  batch: 648 / 721  loss: 0.009936152561513827  hr: 0  min: 0  sec: 56\n",
      "epoch: 2  batch: 649 / 721  loss: 0.009922067461483674  hr: 0  min: 0  sec: 55\n",
      "epoch: 2  batch: 650 / 721  loss: 0.009907440373566575  hr: 0  min: 0  sec: 55\n",
      "epoch: 2  batch: 651 / 721  loss: 0.009934331119645633  hr: 0  min: 0  sec: 54\n",
      "epoch: 2  batch: 652 / 721  loss: 0.009922024305764174  hr: 0  min: 0  sec: 53\n",
      "epoch: 2  batch: 653 / 721  loss: 0.009907093875042938  hr: 0  min: 0  sec: 52\n",
      "epoch: 2  batch: 654 / 721  loss: 0.009892292661941788  hr: 0  min: 0  sec: 51\n",
      "epoch: 2  batch: 655 / 721  loss: 0.009878705353418343  hr: 0  min: 0  sec: 51\n",
      "epoch: 2  batch: 656 / 721  loss: 0.00986440173444241  hr: 0  min: 0  sec: 50\n",
      "epoch: 2  batch: 657 / 721  loss: 0.009849799362618492  hr: 0  min: 0  sec: 49\n",
      "epoch: 2  batch: 658 / 721  loss: 0.009851996534401204  hr: 0  min: 0  sec: 48\n",
      "epoch: 2  batch: 659 / 721  loss: 0.0098373250154436  hr: 0  min: 0  sec: 48\n",
      "epoch: 2  batch: 660 / 721  loss: 0.009822795329069557  hr: 0  min: 0  sec: 47\n",
      "epoch: 2  batch: 661 / 721  loss: 0.009808615968560372  hr: 0  min: 0  sec: 46\n",
      "epoch: 2  batch: 662 / 721  loss: 0.009946549218231524  hr: 0  min: 0  sec: 45\n",
      "epoch: 2  batch: 663 / 721  loss: 0.010310914802699852  hr: 0  min: 0  sec: 45\n",
      "epoch: 2  batch: 664 / 721  loss: 0.010295764819098425  hr: 0  min: 0  sec: 44\n",
      "epoch: 2  batch: 665 / 721  loss: 0.010295168417912936  hr: 0  min: 0  sec: 43\n",
      "epoch: 2  batch: 666 / 721  loss: 0.010281709357055201  hr: 0  min: 0  sec: 42\n",
      "epoch: 2  batch: 667 / 721  loss: 0.01027466225994327  hr: 0  min: 0  sec: 41\n",
      "epoch: 2  batch: 668 / 721  loss: 0.010261522040342178  hr: 0  min: 0  sec: 41\n",
      "epoch: 2  batch: 669 / 721  loss: 0.010265749861453715  hr: 0  min: 0  sec: 40\n",
      "epoch: 2  batch: 670 / 721  loss: 0.010271872267663068  hr: 0  min: 0  sec: 39\n",
      "epoch: 2  batch: 671 / 721  loss: 0.01029090084944107  hr: 0  min: 0  sec: 38\n",
      "epoch: 2  batch: 672 / 721  loss: 0.010276098972900357  hr: 0  min: 0  sec: 38\n",
      "epoch: 2  batch: 673 / 721  loss: 0.010265700935901081  hr: 0  min: 0  sec: 37\n",
      "epoch: 2  batch: 674 / 721  loss: 0.010306531609280778  hr: 0  min: 0  sec: 36\n",
      "epoch: 2  batch: 675 / 721  loss: 0.010314548547073542  hr: 0  min: 0  sec: 35\n",
      "epoch: 2  batch: 676 / 721  loss: 0.010380225301829213  hr: 0  min: 0  sec: 34\n",
      "epoch: 2  batch: 677 / 721  loss: 0.010368694348658499  hr: 0  min: 0  sec: 34\n",
      "epoch: 2  batch: 678 / 721  loss: 0.010363541795824247  hr: 0  min: 0  sec: 33\n",
      "epoch: 2  batch: 679 / 721  loss: 0.01035130495989139  hr: 0  min: 0  sec: 32\n",
      "epoch: 2  batch: 680 / 721  loss: 0.010345515393012594  hr: 0  min: 0  sec: 31\n",
      "epoch: 2  batch: 681 / 721  loss: 0.01033099264913104  hr: 0  min: 0  sec: 31\n",
      "epoch: 2  batch: 682 / 721  loss: 0.010317871346578987  hr: 0  min: 0  sec: 30\n",
      "epoch: 2  batch: 683 / 721  loss: 0.010304710928864244  hr: 0  min: 0  sec: 29\n",
      "epoch: 2  batch: 684 / 721  loss: 0.010308162246788042  hr: 0  min: 0  sec: 28\n",
      "epoch: 2  batch: 685 / 721  loss: 0.010300850448981233  hr: 0  min: 0  sec: 27\n",
      "epoch: 2  batch: 686 / 721  loss: 0.010287199476154273  hr: 0  min: 0  sec: 27\n",
      "epoch: 2  batch: 687 / 721  loss: 0.010272711159042394  hr: 0  min: 0  sec: 26\n",
      "epoch: 2  batch: 688 / 721  loss: 0.0102582610420858  hr: 0  min: 0  sec: 25\n",
      "epoch: 2  batch: 689 / 721  loss: 0.010244894450028083  hr: 0  min: 0  sec: 24\n",
      "epoch: 2  batch: 690 / 721  loss: 0.010233099788973155  hr: 0  min: 0  sec: 24\n",
      "epoch: 2  batch: 691 / 721  loss: 0.01021901511230628  hr: 0  min: 0  sec: 23\n",
      "epoch: 2  batch: 692 / 721  loss: 0.01020498271308408  hr: 0  min: 0  sec: 22\n",
      "epoch: 2  batch: 693 / 721  loss: 0.01019333985245341  hr: 0  min: 0  sec: 21\n",
      "epoch: 2  batch: 694 / 721  loss: 0.010179013719188246  hr: 0  min: 0  sec: 20\n",
      "epoch: 2  batch: 695 / 721  loss: 0.010164587397991826  hr: 0  min: 0  sec: 20\n",
      "epoch: 2  batch: 696 / 721  loss: 0.010165080353168566  hr: 0  min: 0  sec: 19\n",
      "epoch: 2  batch: 697 / 721  loss: 0.010155014089759475  hr: 0  min: 0  sec: 18\n",
      "epoch: 2  batch: 698 / 721  loss: 0.010141736528392379  hr: 0  min: 0  sec: 17\n",
      "epoch: 2  batch: 699 / 721  loss: 0.010127856758229458  hr: 0  min: 0  sec: 17\n",
      "epoch: 2  batch: 700 / 721  loss: 0.0101139105337331  hr: 0  min: 0  sec: 16\n",
      "epoch: 2  batch: 701 / 721  loss: 0.010104647355290168  hr: 0  min: 0  sec: 15\n",
      "epoch: 2  batch: 702 / 721  loss: 0.010095062609568757  hr: 0  min: 0  sec: 14\n",
      "epoch: 2  batch: 703 / 721  loss: 0.01015012779080254  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 704 / 721  loss: 0.01014101822939169  hr: 0  min: 0  sec: 13\n",
      "epoch: 2  batch: 705 / 721  loss: 0.010127175270380775  hr: 0  min: 0  sec: 12\n",
      "epoch: 2  batch: 706 / 721  loss: 0.010114059201256199  hr: 0  min: 0  sec: 11\n",
      "epoch: 2  batch: 707 / 721  loss: 0.010100816675675724  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 708 / 721  loss: 0.01008798911149984  hr: 0  min: 0  sec: 10\n",
      "epoch: 2  batch: 709 / 721  loss: 0.010075009709623266  hr: 0  min: 0  sec: 9\n",
      "epoch: 2  batch: 710 / 721  loss: 0.01006962111157047  hr: 0  min: 0  sec: 8\n",
      "epoch: 2  batch: 711 / 721  loss: 0.010058907371071827  hr: 0  min: 0  sec: 7\n",
      "epoch: 2  batch: 712 / 721  loss: 0.010049877589429681  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 713 / 721  loss: 0.010036728918178662  hr: 0  min: 0  sec: 6\n",
      "epoch: 2  batch: 714 / 721  loss: 0.010023596700448353  hr: 0  min: 0  sec: 5\n",
      "epoch: 2  batch: 715 / 721  loss: 0.01002953697410754  hr: 0  min: 0  sec: 4\n",
      "epoch: 2  batch: 716 / 721  loss: 0.010017770407382397  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 717 / 721  loss: 0.010006883745503058  hr: 0  min: 0  sec: 3\n",
      "epoch: 2  batch: 718 / 721  loss: 0.009994181476576056  hr: 0  min: 0  sec: 2\n",
      "epoch: 2  batch: 719 / 721  loss: 0.00998152049182819  hr: 0  min: 0  sec: 1\n",
      "epoch: 2  batch: 720 / 721  loss: 0.009969707912690258  hr: 0  min: 0  sec: 0\n",
      "epoch: 2  batch: 721 / 721  loss: 0.009956120650805358  hr: 0  min: 0  sec: 0\n",
      "CPU times: total: 7min 3s\n",
      "Wall time: 28min 18s\n"
     ]
    }
   ],
   "source": [
    "%time train_model_ATE(train_loader, 3)  #Tree epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emink\\AppData\\Local\\Temp\\ipykernel_23364\\1419488891.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path), strict=False)\n"
     ]
    }
   ],
   "source": [
    "model_ATE = load_model(model_ATE, 'bert_ATE.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 25.1 s\n",
      "Wall time: 34.9 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     64444\n",
      "           1       0.88      0.85      0.87      4022\n",
      "           2       0.84      0.76      0.79      2141\n",
      "\n",
      "    accuracy                           0.98     70607\n",
      "   macro avg       0.90      0.87      0.88     70607\n",
      "weighted avg       0.98      0.98      0.98     70607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time x, y = test_model_ATE(test_loader)\n",
    "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_ATE(sentence, tokenizer, model_ATE, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Test the ATE model with a single sentence and print the predictions.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "        tokenizer: The tokenizer used during training.\n",
    "        model_ATE: The trained ATE model.\n",
    "        device: The device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        List of predicted tags for the input sentence.\n",
    "    \"\"\"\n",
    "    model_ATE.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids_tensor = torch.tensor([input_ids]).to(device)\n",
    "    attention_mask = torch.tensor([[1] * len(input_ids)]).to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ATE(ids_tensors=input_ids_tensor, tags_tensors=None, masks_tensors=attention_mask)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "    # Convert predictions to tags\n",
    "    predicted_tags = predictions.squeeze().tolist()\n",
    "    token_tag_pairs = list(zip(tokens, predicted_tags))\n",
    "\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Predictions:\")\n",
    "    for token, tag in token_tag_pairs:\n",
    "        print(f\"Token: {token}, Predicted Tag: {tag}\")\n",
    "\n",
    "    return token_tag_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The food was amazing but the service was terrible.\n",
      "Predictions:\n",
      "Token: the, Predicted Tag: 0\n",
      "Token: food, Predicted Tag: 1\n",
      "Token: was, Predicted Tag: 0\n",
      "Token: amazing, Predicted Tag: 0\n",
      "Token: but, Predicted Tag: 0\n",
      "Token: the, Predicted Tag: 0\n",
      "Token: service, Predicted Tag: 1\n",
      "Token: was, Predicted Tag: 0\n",
      "Token: terrible, Predicted Tag: 0\n",
      "Token: ., Predicted Tag: 0\n",
      "Sentence: The pasta was delicious, but the service was slow.\n",
      "Predictions:\n",
      "Token: the, Predicted Tag: 0\n",
      "Token: pasta, Predicted Tag: 1\n",
      "Token: was, Predicted Tag: 0\n",
      "Token: delicious, Predicted Tag: 0\n",
      "Token: ,, Predicted Tag: 0\n",
      "Token: but, Predicted Tag: 0\n",
      "Token: the, Predicted Tag: 0\n",
      "Token: service, Predicted Tag: 1\n",
      "Token: was, Predicted Tag: 0\n",
      "Token: slow, Predicted Tag: 0\n",
      "Token: ., Predicted Tag: 0\n",
      "Sentence: The ambiance was fantastic, but the food was overpriced.\n",
      "Predictions:\n",
      "Token: the, Predicted Tag: 0\n",
      "Token: am, Predicted Tag: 1\n",
      "Token: ##bian, Predicted Tag: 1\n",
      "Token: ##ce, Predicted Tag: 1\n",
      "Token: was, Predicted Tag: 0\n",
      "Token: fantastic, Predicted Tag: 0\n",
      "Token: ,, Predicted Tag: 0\n",
      "Token: but, Predicted Tag: 0\n",
      "Token: the, Predicted Tag: 0\n",
      "Token: food, Predicted Tag: 1\n",
      "Token: was, Predicted Tag: 0\n",
      "Token: over, Predicted Tag: 0\n",
      "Token: ##pr, Predicted Tag: 0\n",
      "Token: ##ice, Predicted Tag: 0\n",
      "Token: ##d, Predicted Tag: 0\n",
      "Token: ., Predicted Tag: 0\n",
      "Sentence: The waiter was very friendly, and the desserts were outstanding\n",
      "Predictions:\n",
      "Token: the, Predicted Tag: 0\n",
      "Token: waiter, Predicted Tag: 1\n",
      "Token: was, Predicted Tag: 0\n",
      "Token: very, Predicted Tag: 0\n",
      "Token: friendly, Predicted Tag: 0\n",
      "Token: ,, Predicted Tag: 0\n",
      "Token: and, Predicted Tag: 0\n",
      "Token: the, Predicted Tag: 0\n",
      "Token: dessert, Predicted Tag: 1\n",
      "Token: ##s, Predicted Tag: 1\n",
      "Token: were, Predicted Tag: 0\n",
      "Token: outstanding, Predicted Tag: 0\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The food was amazing but the service was terrible.\"\n",
    "token_tag_pairs = test_sentence_ATE(sentence, tokenizer, model_ATE)\n",
    "\n",
    "sentence = \"The pasta was delicious, but the service was slow.\"\n",
    "token_tag_pairs= test_sentence_ATE(sentence,tokenizer,model_ATE)\n",
    "\n",
    "sentence = \"The ambiance was fantastic, but the food was overpriced.\"\n",
    "token_tag_pairs= test_sentence_ATE(sentence,tokenizer,model_ATE)\n",
    "\n",
    "sentence = \"The waiter was very friendly, and the desserts were outstanding\"\n",
    "token_tag_pairs= test_sentence_ATE(sentence,tokenizer,model_ATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# restaurants_train_ds = dataset_ABSA(pd.read_csv(\"data/restaurants_train.csv\"), tokenizer)\n",
    "restaurants_train_ds = dataset_ABSA(pd.read_csv(\"data/ABSA5restaurants_train.csv\"), tokenizer)\n",
    "restaurants_test_ds = dataset_ABSA(pd.read_csv(\"data/restaurants_test.csv\"), tokenizer)\n",
    "restaurants_val_ds = dataset_ABSA(pd.read_csv(\"data/V1restaurants_test.csv\"), tokenizer)\n",
    "\n",
    "\n",
    "# restaurants_train_ds = dataset_ABSA(pd.read_csv(\"data/train_test.csv\"), tokenizer)\n",
    "# restaurants_test_ds = dataset_ABSA(pd.read_csv(\"data/test_test.csv\"), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'i', 'went', 'there', 'in', 'late', 'afternoon', 'for', 'some', 'bite', 'size', 'food', 'and', 'ref', '##les', '##hm', '##ent', 'with', 'my', 'date', '.', '[sep]', 'food']\n",
      "23\n",
      "tensor([  100,  1045,  2253,  2045,  1999,  2397,  5027,  2005,  2070,  6805,\n",
      "         2946,  2833,  1998, 25416,  4244, 14227,  4765,  2007,  2026,  3058,\n",
      "         1012,   100,  2833])\n",
      "23\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "23\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "w,x,y,z = restaurants_train_ds.__getitem__(121)\n",
    "print(w)\n",
    "print(len(w))\n",
    "print(x)\n",
    "print(len(x))\n",
    "print(y)\n",
    "print(len(y))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch2(samples):\n",
    "    # Find the max length in the current batch\n",
    "    max_len = max([len(s[1]) for s in samples])\n",
    "\n",
    "    ids_tensors = [torch.cat([s[1], torch.zeros(max_len - len(s[1]), dtype=torch.long)]) for s in samples]\n",
    "    ids_tensors = torch.stack(ids_tensors)\n",
    "\n",
    "    segments_tensors = [torch.cat([s[2], torch.zeros(max_len - len(s[2]), dtype=torch.long)]) for s in samples]\n",
    "    segments_tensors = torch.stack(segments_tensors)\n",
    "\n",
    "    label_ids = torch.stack([s[3] for s in samples])\n",
    "    \n",
    "    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)\n",
    "\n",
    "    return ids_tensors, segments_tensors, masks_tensors, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = ConcatDataset([laptops_train_ds, restaurants_train_ds, twitter_train_ds])\n",
    "# test_ds = ConcatDataset([laptops_test_ds, restaurants_test_ds, twitter_test_ds])\n",
    "\n",
    "# train_ds = restaurants_train_ds\n",
    "# test_ds = restaurants_test_ds\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=4, collate_fn=create_mini_batch2, shuffle = True)\n",
    "# test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch2, shuffle = True)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "\n",
    "\n",
    "# test_size = int(0.5 * len(restaurants_test_ds))\n",
    "# val_size = len(restaurants_test_ds) - test_size\n",
    "# test_ds, validation_ds = random_split(restaurants_test_ds, [test_size, val_size])\n",
    "\n",
    "train_ds = restaurants_train_ds\n",
    "test_ds = restaurants_test_ds\n",
    "validation_ds = restaurants_val_ds\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=16, collate_fn=create_mini_batch2, shuffle=True)\n",
    "validation_loader = DataLoader(validation_ds, batch_size=32, collate_fn=create_mini_batch2, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     w,x,y,z = batch\n",
    "#     print(w)\n",
    "#     print(w.size())\n",
    "#     print(x)\n",
    "#     print(x.size())\n",
    "#     print(y)\n",
    "#     print(y.size())\n",
    "#     print(z)\n",
    "#     print(z.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# def train_model_ABSA(loader, epochs):\n",
    "#     all_data = len(loader)\n",
    "#     for epoch in range(epochs):\n",
    "#         finish_data = 0\n",
    "#         losses = []\n",
    "#         current_times = []\n",
    "#         correct_predictions = 0\n",
    "        \n",
    "#         for data in loader:\n",
    "#             t0 = time.time()\n",
    "#             ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "#             ids_tensors = ids_tensors.to(DEVICE)\n",
    "#             segments_tensors = segments_tensors.to(DEVICE)\n",
    "#             label_ids = label_ids.to(DEVICE)\n",
    "#             masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "#             loss = model_ABSA(ids_tensors=ids_tensors, lable_tensors=label_ids, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
    "#             losses.append(loss.item())\n",
    "#             loss.backward()\n",
    "#             optimizer_ABSA.step()\n",
    "#             optimizer_ABSA.zero_grad()\n",
    "\n",
    "#             finish_data += 1\n",
    "#             current_times.append(round(time.time()-t0,3))\n",
    "#             current = np.mean(current_times)\n",
    "#             hr, min, sec = evl_time(current*(all_data-finish_data) + current*all_data*(epochs-epoch-1))\n",
    "#             print('epoch:', epoch, \" batch:\", finish_data, \"/\" , all_data, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min,\" sec:\", sec)         \n",
    "\n",
    "#         save_model(model_ABSA, 'bert_ABSA3.pkl')\n",
    "#############################################################################\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def validate_model_ABSA(loader):\n",
    "    model_ABSA.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            segments_tensors = segments_tensors.to(DEVICE)\n",
    "            label_ids = label_ids.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            # Compute the loss during validation\n",
    "            loss = model_ABSA(ids_tensors=ids_tensors, lable_tensors=label_ids, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Return the average validation loss\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def train_model_ABSA(loader, validation_loader, epochs):\n",
    "\n",
    "    total_steps = len(loader) * epochs\n",
    "    warmup_steps = int(0.1 * total_steps)  # 10% warmup\n",
    "\n",
    "    # Create the scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer_ABSA,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    all_data = len(loader)\n",
    "    best_loss = float('inf')\n",
    "    patience = 10\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Scheduler\n",
    "    #scheduler = StepLR(optimizer_ABSA, step_size=1, gamma=0.9)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        finish_data = 0\n",
    "        losses = []\n",
    "        current_times = []\n",
    "        \n",
    "        model_ABSA.train()  # Ensure model is in training mode\n",
    "        for data in loader:\n",
    "            t0 = time.time()\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            segments_tensors = segments_tensors.to(DEVICE)\n",
    "            label_ids = label_ids.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            loss = model_ABSA(ids_tensors=ids_tensors, lable_tensors=label_ids, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model_ABSA.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer_ABSA.step()\n",
    "\n",
    "            scheduler.step() #get_linear_schedule_with_warmup\n",
    "\n",
    "            optimizer_ABSA.zero_grad()\n",
    "\n",
    "            finish_data += 1\n",
    "            current_times.append(round(time.time() - t0, 3))\n",
    "            current = np.mean(current_times)\n",
    "            hr, min, sec = evl_time(current * (all_data - finish_data) + current * all_data * (epochs - epoch - 1))\n",
    "            print('epoch:', epoch, \" batch:\", finish_data, \"/\", all_data, \" loss:\", np.mean(losses), \" hr:\", hr, \" min:\", min, \" sec:\", sec)\n",
    "\n",
    "            # Periodic validation within the epoch\n",
    "            if finish_data % 100 == 0:\n",
    "                val_loss = validate_model_ABSA(validation_loader)\n",
    "                print(f\"Validation Loss after {finish_data} batches: {val_loss}\")\n",
    "\n",
    "        # Print learning rate\n",
    "        for param_group in optimizer_ABSA.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = validate_model_ABSA(validation_loader)\n",
    "        print(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            save_model(model_ABSA, 'bert_ABSA8.pkl')\n",
    "            print(\"Saving...\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "        # Step scheduler\n",
    "        #sscheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "def test_model_ABSA(loader):\n",
    "    pred = []\n",
    "    trueth = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "\n",
    "            ids_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "            ids_tensors = ids_tensors.to(DEVICE)\n",
    "            segments_tensors = segments_tensors.to(DEVICE)\n",
    "            masks_tensors = masks_tensors.to(DEVICE)\n",
    "\n",
    "            outputs = model_ABSA(ids_tensors, None, masks_tensors=masks_tensors, segments_tensors=segments_tensors)\n",
    "            \n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "\n",
    "            pred += list([int(i) for i in predictions])\n",
    "            trueth += list([int(i) for i in label_ids])\n",
    "\n",
    "    return trueth, pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  batch: 1 / 122  loss: 1.09127676486969  hr: 0  min: 46  sec: 58\n",
      "epoch: 0  batch: 2 / 122  loss: 1.1679109334945679  hr: 0  min: 38  sec: 22\n",
      "epoch: 0  batch: 3 / 122  loss: 1.147869348526001  hr: 0  min: 37  sec: 59\n",
      "epoch: 0  batch: 4 / 122  loss: 1.1286798119544983  hr: 0  min: 40  sec: 17\n",
      "epoch: 0  batch: 5 / 122  loss: 1.1413341999053954  hr: 0  min: 45  sec: 30\n",
      "epoch: 0  batch: 6 / 122  loss: 1.131715714931488  hr: 0  min: 45  sec: 12\n",
      "epoch: 0  batch: 7 / 122  loss: 1.1377011707850866  hr: 0  min: 44  sec: 8\n",
      "epoch: 0  batch: 8 / 122  loss: 1.1354601085186005  hr: 0  min: 43  sec: 40\n",
      "epoch: 0  batch: 9 / 122  loss: 1.133635891808404  hr: 0  min: 44  sec: 44\n",
      "epoch: 0  batch: 10 / 122  loss: 1.1357030868530273  hr: 0  min: 42  sec: 47\n",
      "epoch: 0  batch: 11 / 122  loss: 1.1381762569600886  hr: 0  min: 42  sec: 5\n",
      "epoch: 0  batch: 12 / 122  loss: 1.1279943585395813  hr: 0  min: 41  sec: 15\n",
      "epoch: 0  batch: 13 / 122  loss: 1.1256739634733934  hr: 0  min: 40  sec: 41\n",
      "epoch: 0  batch: 14 / 122  loss: 1.1230278866631644  hr: 0  min: 40  sec: 15\n",
      "epoch: 0  batch: 15 / 122  loss: 1.1189436833063762  hr: 0  min: 39  sec: 38\n",
      "epoch: 0  batch: 16 / 122  loss: 1.1192502602934837  hr: 0  min: 39  sec: 43\n",
      "epoch: 0  batch: 17 / 122  loss: 1.1199337314156925  hr: 0  min: 39  sec: 11\n",
      "epoch: 0  batch: 18 / 122  loss: 1.117158399687873  hr: 0  min: 39  sec: 7\n",
      "epoch: 0  batch: 19 / 122  loss: 1.1143472069188167  hr: 0  min: 39  sec: 0\n",
      "epoch: 0  batch: 20 / 122  loss: 1.1169472098350526  hr: 0  min: 38  sec: 37\n",
      "epoch: 0  batch: 21 / 122  loss: 1.114015125093006  hr: 0  min: 39  sec: 2\n",
      "epoch: 0  batch: 22 / 122  loss: 1.1119217439131304  hr: 0  min: 38  sec: 37\n",
      "epoch: 0  batch: 23 / 122  loss: 1.1114202478657598  hr: 0  min: 38  sec: 13\n",
      "epoch: 0  batch: 24 / 122  loss: 1.1132579147815704  hr: 0  min: 38  sec: 11\n",
      "epoch: 0  batch: 25 / 122  loss: 1.109910159111023  hr: 0  min: 38  sec: 28\n",
      "epoch: 0  batch: 26 / 122  loss: 1.1083473012997553  hr: 0  min: 38  sec: 11\n",
      "epoch: 0  batch: 27 / 122  loss: 1.108750617062604  hr: 0  min: 38  sec: 6\n",
      "epoch: 0  batch: 28 / 122  loss: 1.105158269405365  hr: 0  min: 38  sec: 6\n",
      "epoch: 0  batch: 29 / 122  loss: 1.1089426237961342  hr: 0  min: 38  sec: 25\n",
      "epoch: 0  batch: 30 / 122  loss: 1.1089704434076946  hr: 0  min: 38  sec: 8\n",
      "epoch: 0  batch: 31 / 122  loss: 1.1062774581293906  hr: 0  min: 38  sec: 8\n",
      "epoch: 0  batch: 32 / 122  loss: 1.1087924800813198  hr: 0  min: 38  sec: 25\n",
      "epoch: 0  batch: 33 / 122  loss: 1.111500418547428  hr: 0  min: 38  sec: 10\n",
      "epoch: 0  batch: 34 / 122  loss: 1.1125426958588993  hr: 0  min: 38  sec: 6\n",
      "epoch: 0  batch: 35 / 122  loss: 1.112704474585397  hr: 0  min: 38  sec: 2\n",
      "epoch: 0  batch: 36 / 122  loss: 1.1118709213203855  hr: 0  min: 38  sec: 0\n",
      "epoch: 0  batch: 37 / 122  loss: 1.111958155760894  hr: 0  min: 37  sec: 54\n",
      "epoch: 0  batch: 38 / 122  loss: 1.1130063000478243  hr: 0  min: 37  sec: 35\n",
      "epoch: 0  batch: 39 / 122  loss: 1.1139640838671954  hr: 0  min: 37  sec: 23\n",
      "epoch: 0  batch: 40 / 122  loss: 1.1149084359407424  hr: 0  min: 37  sec: 17\n",
      "epoch: 0  batch: 41 / 122  loss: 1.114611680914716  hr: 0  min: 37  sec: 10\n",
      "epoch: 0  batch: 42 / 122  loss: 1.116535586970193  hr: 0  min: 37  sec: 41\n",
      "epoch: 0  batch: 43 / 122  loss: 1.1180769620939743  hr: 0  min: 38  sec: 7\n",
      "epoch: 0  batch: 44 / 122  loss: 1.1161619939587333  hr: 0  min: 38  sec: 11\n",
      "epoch: 0  batch: 45 / 122  loss: 1.1164437188042535  hr: 0  min: 38  sec: 10\n",
      "epoch: 0  batch: 46 / 122  loss: 1.116064965724945  hr: 0  min: 38  sec: 4\n",
      "epoch: 0  batch: 47 / 122  loss: 1.117168109467689  hr: 0  min: 38  sec: 3\n",
      "epoch: 0  batch: 48 / 122  loss: 1.115791621307532  hr: 0  min: 37  sec: 45\n",
      "epoch: 0  batch: 49 / 122  loss: 1.1156184454353488  hr: 0  min: 37  sec: 37\n",
      "epoch: 0  batch: 50 / 122  loss: 1.1151279950141906  hr: 0  min: 37  sec: 26\n",
      "epoch: 0  batch: 51 / 122  loss: 1.1156188810572905  hr: 0  min: 37  sec: 17\n",
      "epoch: 0  batch: 52 / 122  loss: 1.1140081240580633  hr: 0  min: 37  sec: 14\n",
      "epoch: 0  batch: 53 / 122  loss: 1.1130868578856845  hr: 0  min: 37  sec: 11\n",
      "epoch: 0  batch: 54 / 122  loss: 1.1126977646792378  hr: 0  min: 37  sec: 8\n",
      "epoch: 0  batch: 55 / 122  loss: 1.1121654597195711  hr: 0  min: 36  sec: 59\n",
      "epoch: 0  batch: 56 / 122  loss: 1.1116328601326262  hr: 0  min: 36  sec: 51\n",
      "epoch: 0  batch: 57 / 122  loss: 1.1103043786266393  hr: 0  min: 36  sec: 54\n",
      "epoch: 0  batch: 58 / 122  loss: 1.1103579463629887  hr: 0  min: 36  sec: 50\n",
      "epoch: 0  batch: 59 / 122  loss: 1.1100641771898432  hr: 0  min: 36  sec: 40\n",
      "epoch: 0  batch: 60 / 122  loss: 1.110539682706197  hr: 0  min: 36  sec: 38\n",
      "epoch: 0  batch: 61 / 122  loss: 1.10914951660594  hr: 0  min: 36  sec: 35\n",
      "epoch: 0  batch: 62 / 122  loss: 1.1093818545341492  hr: 0  min: 36  sec: 30\n",
      "epoch: 0  batch: 63 / 122  loss: 1.1091263256375752  hr: 0  min: 36  sec: 47\n",
      "epoch: 0  batch: 64 / 122  loss: 1.1082085743546486  hr: 0  min: 36  sec: 44\n",
      "epoch: 0  batch: 65 / 122  loss: 1.1069822366421038  hr: 0  min: 36  sec: 46\n",
      "epoch: 0  batch: 66 / 122  loss: 1.1068710955706509  hr: 0  min: 36  sec: 39\n",
      "epoch: 0  batch: 67 / 122  loss: 1.1065008924968207  hr: 0  min: 36  sec: 34\n",
      "epoch: 0  batch: 68 / 122  loss: 1.1065001592916601  hr: 0  min: 36  sec: 27\n",
      "epoch: 0  batch: 69 / 122  loss: 1.1057163960691812  hr: 0  min: 36  sec: 21\n",
      "epoch: 0  batch: 70 / 122  loss: 1.1059746844427927  hr: 0  min: 36  sec: 14\n",
      "epoch: 0  batch: 71 / 122  loss: 1.1048511468188864  hr: 0  min: 36  sec: 28\n",
      "epoch: 0  batch: 72 / 122  loss: 1.103792475329505  hr: 0  min: 36  sec: 22\n",
      "epoch: 0  batch: 73 / 122  loss: 1.1052689225706336  hr: 0  min: 36  sec: 14\n",
      "epoch: 0  batch: 74 / 122  loss: 1.1041535574036676  hr: 0  min: 36  sec: 9\n",
      "epoch: 0  batch: 75 / 122  loss: 1.1036988592147827  hr: 0  min: 36  sec: 5\n",
      "epoch: 0  batch: 76 / 122  loss: 1.1034554164660604  hr: 0  min: 35  sec: 57\n",
      "epoch: 0  batch: 77 / 122  loss: 1.1024130304138382  hr: 0  min: 35  sec: 50\n",
      "epoch: 0  batch: 78 / 122  loss: 1.1022148224023671  hr: 0  min: 36  sec: 4\n",
      "epoch: 0  batch: 79 / 122  loss: 1.1021945431262632  hr: 0  min: 35  sec: 56\n",
      "epoch: 0  batch: 80 / 122  loss: 1.1026368409395217  hr: 0  min: 35  sec: 55\n",
      "epoch: 0  batch: 81 / 122  loss: 1.102901549986851  hr: 0  min: 35  sec: 52\n",
      "epoch: 0  batch: 82 / 122  loss: 1.1026321068042662  hr: 0  min: 35  sec: 57\n",
      "epoch: 0  batch: 83 / 122  loss: 1.1013394306941204  hr: 0  min: 35  sec: 52\n",
      "epoch: 0  batch: 84 / 122  loss: 1.0995457285926455  hr: 0  min: 35  sec: 47\n",
      "epoch: 0  batch: 85 / 122  loss: 1.0992831580779132  hr: 0  min: 35  sec: 47\n",
      "epoch: 0  batch: 86 / 122  loss: 1.0991658000058906  hr: 0  min: 35  sec: 41\n",
      "epoch: 0  batch: 87 / 122  loss: 1.0986264327476765  hr: 0  min: 35  sec: 38\n",
      "epoch: 0  batch: 88 / 122  loss: 1.0990060838786038  hr: 0  min: 35  sec: 33\n",
      "epoch: 0  batch: 89 / 122  loss: 1.098642726962486  hr: 0  min: 35  sec: 33\n",
      "epoch: 0  batch: 90 / 122  loss: 1.0990542160140144  hr: 0  min: 35  sec: 37\n",
      "epoch: 0  batch: 91 / 122  loss: 1.0985880749566215  hr: 0  min: 35  sec: 30\n",
      "epoch: 0  batch: 92 / 122  loss: 1.09833996321844  hr: 0  min: 35  sec: 25\n",
      "epoch: 0  batch: 93 / 122  loss: 1.0970900885520443  hr: 0  min: 35  sec: 22\n",
      "epoch: 0  batch: 94 / 122  loss: 1.0967755920075355  hr: 0  min: 35  sec: 16\n",
      "epoch: 0  batch: 95 / 122  loss: 1.0963411249612507  hr: 0  min: 35  sec: 19\n",
      "epoch: 0  batch: 96 / 122  loss: 1.0970480137815077  hr: 0  min: 35  sec: 17\n",
      "epoch: 0  batch: 97 / 122  loss: 1.0969522066952027  hr: 0  min: 35  sec: 14\n",
      "epoch: 0  batch: 98 / 122  loss: 1.0973606176522313  hr: 0  min: 35  sec: 9\n",
      "epoch: 0  batch: 99 / 122  loss: 1.096882483573875  hr: 0  min: 35  sec: 5\n",
      "epoch: 0  batch: 100 / 122  loss: 1.0969774943590165  hr: 0  min: 34  sec: 59\n",
      "Validation Loss after 100 batches: 1.0180278688669204\n",
      "epoch: 0  batch: 101 / 122  loss: 1.0961881173719275  hr: 0  min: 34  sec: 56\n",
      "epoch: 0  batch: 102 / 122  loss: 1.0953919905073501  hr: 0  min: 34  sec: 51\n",
      "epoch: 0  batch: 103 / 122  loss: 1.0956823160347429  hr: 0  min: 34  sec: 48\n",
      "epoch: 0  batch: 104 / 122  loss: 1.0948895301956396  hr: 0  min: 34  sec: 43\n",
      "epoch: 0  batch: 105 / 122  loss: 1.0942316901116145  hr: 0  min: 34  sec: 38\n",
      "epoch: 0  batch: 106 / 122  loss: 1.0934484156797517  hr: 0  min: 34  sec: 31\n",
      "epoch: 0  batch: 107 / 122  loss: 1.0930083861974913  hr: 0  min: 34  sec: 28\n",
      "epoch: 0  batch: 108 / 122  loss: 1.0918330368068483  hr: 0  min: 34  sec: 20\n",
      "epoch: 0  batch: 109 / 122  loss: 1.0918831480752438  hr: 0  min: 34  sec: 18\n",
      "epoch: 0  batch: 110 / 122  loss: 1.0917404559525576  hr: 0  min: 34  sec: 13\n",
      "epoch: 0  batch: 111 / 122  loss: 1.0909104229093671  hr: 0  min: 34  sec: 8\n",
      "epoch: 0  batch: 112 / 122  loss: 1.0903590811150414  hr: 0  min: 34  sec: 14\n",
      "epoch: 0  batch: 113 / 122  loss: 1.0897732139688678  hr: 0  min: 34  sec: 9\n",
      "epoch: 0  batch: 114 / 122  loss: 1.0887235821339123  hr: 0  min: 34  sec: 4\n",
      "epoch: 0  batch: 115 / 122  loss: 1.0877318382263184  hr: 0  min: 34  sec: 4\n",
      "epoch: 0  batch: 116 / 122  loss: 1.086273562291573  hr: 0  min: 34  sec: 0\n",
      "epoch: 0  batch: 117 / 122  loss: 1.0850750661303854  hr: 0  min: 34  sec: 3\n",
      "epoch: 0  batch: 118 / 122  loss: 1.083311799219099  hr: 0  min: 34  sec: 2\n",
      "epoch: 0  batch: 119 / 122  loss: 1.0825770627550717  hr: 0  min: 33  sec: 58\n",
      "epoch: 0  batch: 120 / 122  loss: 1.0806062320868175  hr: 0  min: 33  sec: 57\n",
      "epoch: 0  batch: 121 / 122  loss: 1.0796076965726111  hr: 0  min: 33  sec: 53\n",
      "epoch: 0  batch: 122 / 122  loss: 1.0783842254857547  hr: 0  min: 33  sec: 48\n",
      "Learning rate: 2e-05\n",
      "0.89602090716362\n",
      "Saving...\n",
      "epoch: 1  batch: 1 / 122  loss: 0.9124557971954346  hr: 0  min: 44  sec: 41\n",
      "epoch: 1  batch: 2 / 122  loss: 0.8716066479682922  hr: 0  min: 39  sec: 27\n",
      "epoch: 1  batch: 3 / 122  loss: 0.8911266326904297  hr: 0  min: 36  sec: 12\n",
      "epoch: 1  batch: 4 / 122  loss: 0.9024171084165573  hr: 0  min: 40  sec: 34\n",
      "epoch: 1  batch: 5 / 122  loss: 0.9051136612892151  hr: 0  min: 43  sec: 34\n",
      "epoch: 1  batch: 6 / 122  loss: 0.8813177148501078  hr: 0  min: 41  sec: 48\n",
      "epoch: 1  batch: 7 / 122  loss: 0.8714734401021685  hr: 0  min: 41  sec: 0\n",
      "epoch: 1  batch: 8 / 122  loss: 0.8719179555773735  hr: 0  min: 38  sec: 48\n",
      "epoch: 1  batch: 9 / 122  loss: 0.9018126659923129  hr: 0  min: 38  sec: 16\n",
      "epoch: 1  batch: 10 / 122  loss: 0.8941596567630767  hr: 0  min: 36  sec: 45\n",
      "epoch: 1  batch: 11 / 122  loss: 0.8974447250366211  hr: 0  min: 35  sec: 53\n",
      "epoch: 1  batch: 12 / 122  loss: 0.9021536509195963  hr: 0  min: 35  sec: 25\n",
      "epoch: 1  batch: 13 / 122  loss: 0.9077228124325092  hr: 0  min: 35  sec: 32\n",
      "epoch: 1  batch: 14 / 122  loss: 0.9131214490958622  hr: 0  min: 35  sec: 20\n",
      "epoch: 1  batch: 15 / 122  loss: 0.9025117913881938  hr: 0  min: 34  sec: 48\n",
      "epoch: 1  batch: 16 / 122  loss: 0.9078868925571442  hr: 0  min: 34  sec: 50\n",
      "epoch: 1  batch: 17 / 122  loss: 0.9280959367752075  hr: 0  min: 35  sec: 15\n",
      "epoch: 1  batch: 18 / 122  loss: 0.9323760271072388  hr: 0  min: 35  sec: 2\n",
      "epoch: 1  batch: 19 / 122  loss: 0.9444142454548886  hr: 0  min: 34  sec: 47\n",
      "epoch: 1  batch: 20 / 122  loss: 0.9342889130115509  hr: 0  min: 34  sec: 22\n",
      "epoch: 1  batch: 21 / 122  loss: 0.9219307218279157  hr: 0  min: 34  sec: 6\n",
      "epoch: 1  batch: 22 / 122  loss: 0.917502760887146  hr: 0  min: 33  sec: 52\n",
      "epoch: 1  batch: 23 / 122  loss: 0.9142200221186099  hr: 0  min: 33  sec: 38\n",
      "epoch: 1  batch: 24 / 122  loss: 0.908065639436245  hr: 0  min: 33  sec: 40\n",
      "epoch: 1  batch: 25 / 122  loss: 0.9045174670219421  hr: 0  min: 33  sec: 35\n",
      "epoch: 1  batch: 26 / 122  loss: 0.8958665361771216  hr: 0  min: 33  sec: 31\n",
      "epoch: 1  batch: 27 / 122  loss: 0.8951136823053714  hr: 0  min: 33  sec: 38\n",
      "epoch: 1  batch: 28 / 122  loss: 0.8944888412952423  hr: 0  min: 33  sec: 47\n",
      "epoch: 1  batch: 29 / 122  loss: 0.8991877662724462  hr: 0  min: 34  sec: 0\n",
      "epoch: 1  batch: 30 / 122  loss: 0.893598872423172  hr: 0  min: 33  sec: 57\n",
      "epoch: 1  batch: 31 / 122  loss: 0.8904144687037314  hr: 0  min: 33  sec: 39\n",
      "epoch: 1  batch: 32 / 122  loss: 0.8882049340754747  hr: 0  min: 33  sec: 29\n",
      "epoch: 1  batch: 33 / 122  loss: 0.8851319226351652  hr: 0  min: 33  sec: 25\n",
      "epoch: 1  batch: 34 / 122  loss: 0.8874943922547733  hr: 0  min: 33  sec: 14\n",
      "epoch: 1  batch: 35 / 122  loss: 0.8892012136323112  hr: 0  min: 32  sec: 58\n",
      "epoch: 1  batch: 36 / 122  loss: 0.8854586531718572  hr: 0  min: 32  sec: 49\n",
      "epoch: 1  batch: 37 / 122  loss: 0.8875471820702424  hr: 0  min: 32  sec: 46\n",
      "epoch: 1  batch: 38 / 122  loss: 0.8839724252098485  hr: 0  min: 32  sec: 35\n",
      "epoch: 1  batch: 39 / 122  loss: 0.8857085567254287  hr: 0  min: 32  sec: 49\n",
      "epoch: 1  batch: 40 / 122  loss: 0.8882784530520439  hr: 0  min: 32  sec: 57\n",
      "epoch: 1  batch: 41 / 122  loss: 0.8887917733773952  hr: 0  min: 33  sec: 12\n",
      "epoch: 1  batch: 42 / 122  loss: 0.8881887665816716  hr: 0  min: 33  sec: 10\n",
      "epoch: 1  batch: 43 / 122  loss: 0.8845543237619622  hr: 0  min: 33  sec: 0\n",
      "epoch: 1  batch: 44 / 122  loss: 0.8868992653760043  hr: 0  min: 32  sec: 55\n",
      "epoch: 1  batch: 45 / 122  loss: 0.8845014837053087  hr: 0  min: 32  sec: 50\n",
      "epoch: 1  batch: 46 / 122  loss: 0.8782762055811675  hr: 0  min: 32  sec: 39\n",
      "epoch: 1  batch: 47 / 122  loss: 0.8742345774427374  hr: 0  min: 33  sec: 2\n",
      "epoch: 1  batch: 48 / 122  loss: 0.8775293206175169  hr: 0  min: 33  sec: 19\n",
      "epoch: 1  batch: 49 / 122  loss: 0.8724711318405307  hr: 0  min: 33  sec: 13\n",
      "epoch: 1  batch: 50 / 122  loss: 0.8683985400199891  hr: 0  min: 33  sec: 6\n",
      "epoch: 1  batch: 51 / 122  loss: 0.868519732765123  hr: 0  min: 32  sec: 56\n",
      "epoch: 1  batch: 52 / 122  loss: 0.8757667736365244  hr: 0  min: 32  sec: 48\n",
      "epoch: 1  batch: 53 / 122  loss: 0.876394158264376  hr: 0  min: 32  sec: 39\n",
      "epoch: 1  batch: 54 / 122  loss: 0.873972847505852  hr: 0  min: 32  sec: 32\n",
      "epoch: 1  batch: 55 / 122  loss: 0.8720316984436729  hr: 0  min: 32  sec: 25\n",
      "epoch: 1  batch: 56 / 122  loss: 0.8728729358741215  hr: 0  min: 32  sec: 15\n",
      "epoch: 1  batch: 57 / 122  loss: 0.8708798299755967  hr: 0  min: 32  sec: 14\n",
      "epoch: 1  batch: 58 / 122  loss: 0.8675043911769472  hr: 0  min: 32  sec: 13\n",
      "epoch: 1  batch: 59 / 122  loss: 0.8653099688432985  hr: 0  min: 32  sec: 14\n",
      "epoch: 1  batch: 60 / 122  loss: 0.8675415923198064  hr: 0  min: 32  sec: 5\n",
      "epoch: 1  batch: 61 / 122  loss: 0.8659074023121693  hr: 0  min: 32  sec: 3\n",
      "epoch: 1  batch: 62 / 122  loss: 0.8623578798386359  hr: 0  min: 32  sec: 4\n",
      "epoch: 1  batch: 63 / 122  loss: 0.8590668752079919  hr: 0  min: 31  sec: 59\n",
      "epoch: 1  batch: 64 / 122  loss: 0.8575767884030938  hr: 0  min: 32  sec: 0\n",
      "epoch: 1  batch: 65 / 122  loss: 0.8548614281874437  hr: 0  min: 31  sec: 56\n",
      "epoch: 1  batch: 66 / 122  loss: 0.8514202706741564  hr: 0  min: 31  sec: 59\n",
      "epoch: 1  batch: 67 / 122  loss: 0.8509232446328917  hr: 0  min: 31  sec: 52\n",
      "epoch: 1  batch: 68 / 122  loss: 0.8491711940835504  hr: 0  min: 31  sec: 46\n",
      "epoch: 1  batch: 69 / 122  loss: 0.8491719693377398  hr: 0  min: 31  sec: 44\n",
      "epoch: 1  batch: 70 / 122  loss: 0.8443021586963109  hr: 0  min: 31  sec: 42\n",
      "epoch: 1  batch: 71 / 122  loss: 0.8459066463188386  hr: 0  min: 31  sec: 40\n",
      "epoch: 1  batch: 72 / 122  loss: 0.84564894106653  hr: 0  min: 31  sec: 53\n",
      "epoch: 1  batch: 73 / 122  loss: 0.8471649052345589  hr: 0  min: 31  sec: 50\n",
      "epoch: 1  batch: 74 / 122  loss: 0.8442035454350549  hr: 0  min: 31  sec: 50\n",
      "epoch: 1  batch: 75 / 122  loss: 0.8424873638153076  hr: 0  min: 31  sec: 43\n",
      "epoch: 1  batch: 76 / 122  loss: 0.8422798632006896  hr: 0  min: 31  sec: 35\n",
      "epoch: 1  batch: 77 / 122  loss: 0.8382455079586475  hr: 0  min: 31  sec: 34\n",
      "epoch: 1  batch: 78 / 122  loss: 0.8380864300788977  hr: 0  min: 31  sec: 29\n",
      "epoch: 1  batch: 79 / 122  loss: 0.8403804430478736  hr: 0  min: 31  sec: 26\n",
      "epoch: 1  batch: 80 / 122  loss: 0.8401024550199508  hr: 0  min: 31  sec: 23\n",
      "epoch: 1  batch: 81 / 122  loss: 0.841808397092937  hr: 0  min: 31  sec: 19\n",
      "epoch: 1  batch: 82 / 122  loss: 0.842017939904841  hr: 0  min: 31  sec: 16\n",
      "epoch: 1  batch: 83 / 122  loss: 0.8430260527564819  hr: 0  min: 31  sec: 18\n",
      "epoch: 1  batch: 84 / 122  loss: 0.8438892392885118  hr: 0  min: 31  sec: 15\n",
      "epoch: 1  batch: 85 / 122  loss: 0.8407029993393842  hr: 0  min: 31  sec: 15\n",
      "epoch: 1  batch: 86 / 122  loss: 0.8365646929934968  hr: 0  min: 31  sec: 7\n",
      "epoch: 1  batch: 87 / 122  loss: 0.8364629591333455  hr: 0  min: 31  sec: 12\n",
      "epoch: 1  batch: 88 / 122  loss: 0.837392517450181  hr: 0  min: 31  sec: 8\n",
      "epoch: 1  batch: 89 / 122  loss: 0.8365126851569401  hr: 0  min: 31  sec: 6\n",
      "epoch: 1  batch: 90 / 122  loss: 0.8354471958345837  hr: 0  min: 31  sec: 6\n",
      "epoch: 1  batch: 91 / 122  loss: 0.837240626851281  hr: 0  min: 31  sec: 4\n",
      "epoch: 1  batch: 92 / 122  loss: 0.8329821135038915  hr: 0  min: 31  sec: 1\n",
      "epoch: 1  batch: 93 / 122  loss: 0.832413112924945  hr: 0  min: 30  sec: 58\n",
      "epoch: 1  batch: 94 / 122  loss: 0.83003405909589  hr: 0  min: 30  sec: 55\n",
      "epoch: 1  batch: 95 / 122  loss: 0.8273202780045961  hr: 0  min: 30  sec: 51\n",
      "epoch: 1  batch: 96 / 122  loss: 0.8281372161582112  hr: 0  min: 30  sec: 45\n",
      "epoch: 1  batch: 97 / 122  loss: 0.8270751136479918  hr: 0  min: 30  sec: 45\n",
      "epoch: 1  batch: 98 / 122  loss: 0.8259606504318665  hr: 0  min: 30  sec: 54\n",
      "epoch: 1  batch: 99 / 122  loss: 0.8227558602588345  hr: 0  min: 30  sec: 52\n",
      "epoch: 1  batch: 100 / 122  loss: 0.8197160282731056  hr: 0  min: 30  sec: 56\n",
      "Validation Loss after 100 batches: 0.6932248950004578\n",
      "epoch: 1  batch: 101 / 122  loss: 0.81713536086649  hr: 0  min: 30  sec: 51\n",
      "epoch: 1  batch: 102 / 122  loss: 0.8167987983016407  hr: 0  min: 30  sec: 50\n",
      "epoch: 1  batch: 103 / 122  loss: 0.8183325792400582  hr: 0  min: 30  sec: 44\n",
      "epoch: 1  batch: 104 / 122  loss: 0.8150350213623964  hr: 0  min: 30  sec: 40\n",
      "epoch: 1  batch: 105 / 122  loss: 0.8125602492264339  hr: 0  min: 30  sec: 36\n",
      "epoch: 1  batch: 106 / 122  loss: 0.8090922998369865  hr: 0  min: 30  sec: 34\n",
      "epoch: 1  batch: 107 / 122  loss: 0.8069021364795828  hr: 0  min: 30  sec: 36\n",
      "epoch: 1  batch: 108 / 122  loss: 0.8061994361104788  hr: 0  min: 30  sec: 34\n",
      "epoch: 1  batch: 109 / 122  loss: 0.8020895159025805  hr: 0  min: 30  sec: 29\n",
      "epoch: 1  batch: 110 / 122  loss: 0.801680435646664  hr: 0  min: 30  sec: 36\n",
      "epoch: 1  batch: 111 / 122  loss: 0.798458015328055  hr: 0  min: 30  sec: 34\n",
      "epoch: 1  batch: 112 / 122  loss: 0.7972689060760396  hr: 0  min: 30  sec: 31\n",
      "epoch: 1  batch: 113 / 122  loss: 0.7994056752825205  hr: 0  min: 30  sec: 34\n",
      "epoch: 1  batch: 114 / 122  loss: 0.7990266254596543  hr: 0  min: 30  sec: 30\n",
      "epoch: 1  batch: 115 / 122  loss: 0.7995629914428877  hr: 0  min: 30  sec: 25\n",
      "epoch: 1  batch: 116 / 122  loss: 0.7978890869638016  hr: 0  min: 30  sec: 21\n",
      "epoch: 1  batch: 117 / 122  loss: 0.7966693606641557  hr: 0  min: 30  sec: 16\n",
      "epoch: 1  batch: 118 / 122  loss: 0.796100429306596  hr: 0  min: 30  sec: 12\n",
      "epoch: 1  batch: 119 / 122  loss: 0.7943166216381458  hr: 0  min: 30  sec: 11\n",
      "epoch: 1  batch: 120 / 122  loss: 0.7947515639166037  hr: 0  min: 30  sec: 9\n",
      "epoch: 1  batch: 121 / 122  loss: 0.7958485476734224  hr: 0  min: 30  sec: 8\n",
      "epoch: 1  batch: 122 / 122  loss: 0.7969978327145342  hr: 0  min: 30  sec: 2\n",
      "Learning rate: 1.7777777777777777e-05\n",
      "0.7552885591983796\n",
      "Saving...\n",
      "epoch: 2  batch: 1 / 122  loss: 0.7533192038536072  hr: 0  min: 31  sec: 40\n",
      "epoch: 2  batch: 2 / 122  loss: 0.7426623404026031  hr: 0  min: 31  sec: 3\n",
      "epoch: 2  batch: 3 / 122  loss: 0.714948038260142  hr: 0  min: 28  sec: 40\n",
      "epoch: 2  batch: 4 / 122  loss: 0.6902493685483932  hr: 0  min: 28  sec: 51\n",
      "epoch: 2  batch: 5 / 122  loss: 0.6713191390037536  hr: 0  min: 27  sec: 49\n",
      "epoch: 2  batch: 6 / 122  loss: 0.6338628232479095  hr: 0  min: 28  sec: 27\n",
      "epoch: 2  batch: 7 / 122  loss: 0.6561259712491717  hr: 0  min: 27  sec: 58\n",
      "epoch: 2  batch: 8 / 122  loss: 0.6645019724965096  hr: 0  min: 28  sec: 5\n",
      "epoch: 2  batch: 9 / 122  loss: 0.6511458224720426  hr: 0  min: 28  sec: 22\n",
      "epoch: 2  batch: 10 / 122  loss: 0.6540570259094238  hr: 0  min: 28  sec: 9\n",
      "epoch: 2  batch: 11 / 122  loss: 0.6629700443961404  hr: 0  min: 29  sec: 5\n",
      "epoch: 2  batch: 12 / 122  loss: 0.676670620838801  hr: 0  min: 30  sec: 34\n",
      "epoch: 2  batch: 13 / 122  loss: 0.6783517644955561  hr: 0  min: 30  sec: 5\n",
      "epoch: 2  batch: 14 / 122  loss: 0.6820724265916007  hr: 0  min: 31  sec: 4\n",
      "epoch: 2  batch: 15 / 122  loss: 0.6697442134221395  hr: 0  min: 31  sec: 1\n",
      "epoch: 2  batch: 16 / 122  loss: 0.6728627942502499  hr: 0  min: 30  sec: 40\n",
      "epoch: 2  batch: 17 / 122  loss: 0.6854221154661739  hr: 0  min: 30  sec: 5\n",
      "epoch: 2  batch: 18 / 122  loss: 0.691819257206387  hr: 0  min: 29  sec: 50\n",
      "epoch: 2  batch: 19 / 122  loss: 0.6951258339379963  hr: 0  min: 29  sec: 46\n",
      "epoch: 2  batch: 20 / 122  loss: 0.6946758508682251  hr: 0  min: 29  sec: 33\n",
      "epoch: 2  batch: 21 / 122  loss: 0.6837124001412165  hr: 0  min: 29  sec: 55\n",
      "epoch: 2  batch: 22 / 122  loss: 0.6838859753175215  hr: 0  min: 29  sec: 32\n",
      "epoch: 2  batch: 23 / 122  loss: 0.678684993930485  hr: 0  min: 29  sec: 29\n",
      "epoch: 2  batch: 24 / 122  loss: 0.6821212321519852  hr: 0  min: 29  sec: 13\n",
      "epoch: 2  batch: 25 / 122  loss: 0.67841956615448  hr: 0  min: 29  sec: 11\n",
      "epoch: 2  batch: 26 / 122  loss: 0.6781808756864988  hr: 0  min: 28  sec: 57\n",
      "epoch: 2  batch: 27 / 122  loss: 0.6719183303691723  hr: 0  min: 29  sec: 38\n",
      "epoch: 2  batch: 28 / 122  loss: 0.6795910745859146  hr: 0  min: 29  sec: 27\n",
      "epoch: 2  batch: 29 / 122  loss: 0.6827951423053084  hr: 0  min: 29  sec: 33\n",
      "epoch: 2  batch: 30 / 122  loss: 0.67482950091362  hr: 0  min: 29  sec: 20\n",
      "epoch: 2  batch: 31 / 122  loss: 0.6715396354275365  hr: 0  min: 29  sec: 21\n",
      "epoch: 2  batch: 32 / 122  loss: 0.6669298093765974  hr: 0  min: 29  sec: 18\n",
      "epoch: 2  batch: 33 / 122  loss: 0.6585071095914552  hr: 0  min: 29  sec: 1\n",
      "epoch: 2  batch: 34 / 122  loss: 0.6520618875237072  hr: 0  min: 28  sec: 58\n",
      "epoch: 2  batch: 35 / 122  loss: 0.649200006042208  hr: 0  min: 28  sec: 56\n",
      "epoch: 2  batch: 36 / 122  loss: 0.6537576475077205  hr: 0  min: 29  sec: 4\n",
      "epoch: 2  batch: 37 / 122  loss: 0.659801115055342  hr: 0  min: 29  sec: 0\n",
      "epoch: 2  batch: 38 / 122  loss: 0.6635908476616207  hr: 0  min: 28  sec: 57\n",
      "epoch: 2  batch: 39 / 122  loss: 0.653694483714226  hr: 0  min: 28  sec: 54\n",
      "epoch: 2  batch: 40 / 122  loss: 0.6461660817265511  hr: 0  min: 28  sec: 47\n",
      "epoch: 2  batch: 41 / 122  loss: 0.6413255770031999  hr: 0  min: 28  sec: 46\n",
      "epoch: 2  batch: 42 / 122  loss: 0.642372788417907  hr: 0  min: 28  sec: 40\n",
      "epoch: 2  batch: 43 / 122  loss: 0.6367849374926368  hr: 0  min: 28  sec: 33\n",
      "epoch: 2  batch: 44 / 122  loss: 0.6371100409464403  hr: 0  min: 28  sec: 24\n",
      "epoch: 2  batch: 45 / 122  loss: 0.6394356674618191  hr: 0  min: 28  sec: 16\n",
      "epoch: 2  batch: 46 / 122  loss: 0.6337483110635177  hr: 0  min: 28  sec: 10\n",
      "epoch: 2  batch: 47 / 122  loss: 0.6282048574153413  hr: 0  min: 28  sec: 16\n",
      "epoch: 2  batch: 48 / 122  loss: 0.6292343940585852  hr: 0  min: 28  sec: 18\n",
      "epoch: 2  batch: 49 / 122  loss: 0.6286930642565902  hr: 0  min: 28  sec: 14\n",
      "epoch: 2  batch: 50 / 122  loss: 0.6323444682359696  hr: 0  min: 28  sec: 8\n",
      "epoch: 2  batch: 51 / 122  loss: 0.6275672462641024  hr: 0  min: 28  sec: 2\n",
      "epoch: 2  batch: 52 / 122  loss: 0.6222987759571809  hr: 0  min: 27  sec: 56\n",
      "epoch: 2  batch: 53 / 122  loss: 0.6308788072388127  hr: 0  min: 27  sec: 58\n",
      "epoch: 2  batch: 54 / 122  loss: 0.6251243695064828  hr: 0  min: 27  sec: 59\n",
      "epoch: 2  batch: 55 / 122  loss: 0.6222253934903579  hr: 0  min: 27  sec: 57\n",
      "epoch: 2  batch: 56 / 122  loss: 0.6190069967082569  hr: 0  min: 27  sec: 55\n",
      "epoch: 2  batch: 57 / 122  loss: 0.6171163090488367  hr: 0  min: 27  sec: 47\n",
      "epoch: 2  batch: 58 / 122  loss: 0.6145875613237249  hr: 0  min: 27  sec: 43\n",
      "epoch: 2  batch: 59 / 122  loss: 0.6107591749247858  hr: 0  min: 27  sec: 51\n",
      "epoch: 2  batch: 60 / 122  loss: 0.6108285660545031  hr: 0  min: 27  sec: 44\n",
      "epoch: 2  batch: 61 / 122  loss: 0.6089964318470876  hr: 0  min: 27  sec: 42\n",
      "epoch: 2  batch: 62 / 122  loss: 0.6103899512560137  hr: 0  min: 27  sec: 39\n",
      "epoch: 2  batch: 63 / 122  loss: 0.6116207927938492  hr: 0  min: 27  sec: 36\n",
      "epoch: 2  batch: 64 / 122  loss: 0.6119445492513478  hr: 0  min: 27  sec: 45\n",
      "epoch: 2  batch: 65 / 122  loss: 0.6112939509061667  hr: 0  min: 27  sec: 49\n",
      "epoch: 2  batch: 66 / 122  loss: 0.6088164480346622  hr: 0  min: 27  sec: 44\n",
      "epoch: 2  batch: 67 / 122  loss: 0.6098909613801472  hr: 0  min: 27  sec: 38\n",
      "epoch: 2  batch: 68 / 122  loss: 0.610012020696612  hr: 0  min: 27  sec: 40\n",
      "epoch: 2  batch: 69 / 122  loss: 0.610126802454824  hr: 0  min: 27  sec: 37\n",
      "epoch: 2  batch: 70 / 122  loss: 0.6096079311200551  hr: 0  min: 27  sec: 34\n",
      "epoch: 2  batch: 71 / 122  loss: 0.605910214739786  hr: 0  min: 27  sec: 34\n",
      "epoch: 2  batch: 72 / 122  loss: 0.6055348672800593  hr: 0  min: 27  sec: 30\n",
      "epoch: 2  batch: 73 / 122  loss: 0.6028859688811106  hr: 0  min: 27  sec: 24\n",
      "epoch: 2  batch: 74 / 122  loss: 0.6018987040261965  hr: 0  min: 27  sec: 23\n",
      "epoch: 2  batch: 75 / 122  loss: 0.6004300638039907  hr: 0  min: 27  sec: 23\n",
      "epoch: 2  batch: 76 / 122  loss: 0.6057636780958426  hr: 0  min: 27  sec: 19\n",
      "epoch: 2  batch: 77 / 122  loss: 0.6038794935523689  hr: 0  min: 27  sec: 18\n",
      "epoch: 2  batch: 78 / 122  loss: 0.6008766904855386  hr: 0  min: 27  sec: 18\n",
      "epoch: 2  batch: 79 / 122  loss: 0.6034070985226692  hr: 0  min: 27  sec: 14\n",
      "epoch: 2  batch: 80 / 122  loss: 0.6007718972861766  hr: 0  min: 27  sec: 16\n",
      "epoch: 2  batch: 81 / 122  loss: 0.6017645928594801  hr: 0  min: 27  sec: 19\n",
      "epoch: 2  batch: 82 / 122  loss: 0.6011125256375569  hr: 0  min: 27  sec: 27\n",
      "epoch: 2  batch: 83 / 122  loss: 0.5992721990648523  hr: 0  min: 27  sec: 24\n",
      "epoch: 2  batch: 84 / 122  loss: 0.5971231960824558  hr: 0  min: 27  sec: 22\n",
      "epoch: 2  batch: 85 / 122  loss: 0.5968892633914947  hr: 0  min: 27  sec: 17\n",
      "epoch: 2  batch: 86 / 122  loss: 0.5966630505267964  hr: 0  min: 27  sec: 11\n",
      "epoch: 2  batch: 87 / 122  loss: 0.5998329126286781  hr: 0  min: 27  sec: 9\n",
      "epoch: 2  batch: 88 / 122  loss: 0.597653618929061  hr: 0  min: 27  sec: 7\n",
      "epoch: 2  batch: 89 / 122  loss: 0.5982429843940092  hr: 0  min: 27  sec: 8\n",
      "epoch: 2  batch: 90 / 122  loss: 0.6008157223463059  hr: 0  min: 27  sec: 5\n",
      "epoch: 2  batch: 91 / 122  loss: 0.6019524358786069  hr: 0  min: 27  sec: 1\n",
      "epoch: 2  batch: 92 / 122  loss: 0.599921251444713  hr: 0  min: 27  sec: 1\n",
      "epoch: 2  batch: 93 / 122  loss: 0.6013123806445829  hr: 0  min: 27  sec: 4\n",
      "epoch: 2  batch: 94 / 122  loss: 0.6007555948292955  hr: 0  min: 27  sec: 3\n",
      "epoch: 2  batch: 95 / 122  loss: 0.6010771641605779  hr: 0  min: 26  sec: 58\n",
      "epoch: 2  batch: 96 / 122  loss: 0.6015127801025907  hr: 0  min: 26  sec: 53\n",
      "epoch: 2  batch: 97 / 122  loss: 0.6034373939037323  hr: 0  min: 26  sec: 49\n",
      "epoch: 2  batch: 98 / 122  loss: 0.6030461620919558  hr: 0  min: 26  sec: 47\n",
      "epoch: 2  batch: 99 / 122  loss: 0.6056761937309997  hr: 0  min: 26  sec: 43\n",
      "epoch: 2  batch: 100 / 122  loss: 0.6046094390749931  hr: 0  min: 26  sec: 42\n",
      "Validation Loss after 100 batches: 0.677165238559246\n",
      "epoch: 2  batch: 101 / 122  loss: 0.6036133409136593  hr: 0  min: 26  sec: 48\n",
      "epoch: 2  batch: 102 / 122  loss: 0.6043223665625441  hr: 0  min: 26  sec: 44\n",
      "epoch: 2  batch: 103 / 122  loss: 0.6039783864924051  hr: 0  min: 26  sec: 40\n",
      "epoch: 2  batch: 104 / 122  loss: 0.6017320124575725  hr: 0  min: 26  sec: 43\n",
      "epoch: 2  batch: 105 / 122  loss: 0.5994540021533058  hr: 0  min: 26  sec: 43\n",
      "epoch: 2  batch: 106 / 122  loss: 0.5967422451050777  hr: 0  min: 26  sec: 38\n",
      "epoch: 2  batch: 107 / 122  loss: 0.5962634223086811  hr: 0  min: 26  sec: 44\n",
      "epoch: 2  batch: 108 / 122  loss: 0.5932800913298572  hr: 0  min: 26  sec: 41\n",
      "epoch: 2  batch: 109 / 122  loss: 0.5929015688939926  hr: 0  min: 26  sec: 39\n",
      "epoch: 2  batch: 110 / 122  loss: 0.5907075266946445  hr: 0  min: 26  sec: 32\n",
      "epoch: 2  batch: 111 / 122  loss: 0.5905471607908472  hr: 0  min: 26  sec: 31\n",
      "epoch: 2  batch: 112 / 122  loss: 0.5889114213309118  hr: 0  min: 26  sec: 26\n",
      "epoch: 2  batch: 113 / 122  loss: 0.5871146527011838  hr: 0  min: 26  sec: 32\n",
      "epoch: 2  batch: 114 / 122  loss: 0.5875296022808343  hr: 0  min: 26  sec: 29\n",
      "epoch: 2  batch: 115 / 122  loss: 0.585401461435401  hr: 0  min: 26  sec: 25\n",
      "epoch: 2  batch: 116 / 122  loss: 0.584921437604674  hr: 0  min: 26  sec: 27\n",
      "epoch: 2  batch: 117 / 122  loss: 0.5859118501345316  hr: 0  min: 26  sec: 23\n",
      "epoch: 2  batch: 118 / 122  loss: 0.5857902250047458  hr: 0  min: 26  sec: 20\n",
      "epoch: 2  batch: 119 / 122  loss: 0.5865020872164173  hr: 0  min: 26  sec: 22\n",
      "epoch: 2  batch: 120 / 122  loss: 0.587090882162253  hr: 0  min: 26  sec: 19\n",
      "epoch: 2  batch: 121 / 122  loss: 0.585568713255165  hr: 0  min: 26  sec: 16\n",
      "epoch: 2  batch: 122 / 122  loss: 0.5842710452490165  hr: 0  min: 26  sec: 13\n",
      "Learning rate: 1.555555555555556e-05\n",
      "0.6435546755790711\n",
      "Saving...\n",
      "epoch: 3  batch: 1 / 122  loss: 0.2166702002286911  hr: 0  min: 22  sec: 22\n",
      "epoch: 3  batch: 2 / 122  loss: 0.34835194796323776  hr: 0  min: 24  sec: 7\n",
      "epoch: 3  batch: 3 / 122  loss: 0.3174203783273697  hr: 0  min: 22  sec: 55\n",
      "epoch: 3  batch: 4 / 122  loss: 0.2928892560303211  hr: 0  min: 22  sec: 48\n",
      "epoch: 3  batch: 5 / 122  loss: 0.3420900732278824  hr: 0  min: 22  sec: 38\n",
      "epoch: 3  batch: 6 / 122  loss: 0.33866530905167264  hr: 0  min: 22  sec: 9\n",
      "epoch: 3  batch: 7 / 122  loss: 0.3583740357841764  hr: 0  min: 22  sec: 25\n",
      "epoch: 3  batch: 8 / 122  loss: 0.3553981687873602  hr: 0  min: 23  sec: 18\n",
      "epoch: 3  batch: 9 / 122  loss: 0.35783527460363174  hr: 0  min: 24  sec: 12\n",
      "epoch: 3  batch: 10 / 122  loss: 0.39722381979227067  hr: 0  min: 24  sec: 51\n",
      "epoch: 3  batch: 11 / 122  loss: 0.3925658152862029  hr: 0  min: 24  sec: 46\n",
      "epoch: 3  batch: 12 / 122  loss: 0.40339797859390575  hr: 0  min: 24  sec: 42\n",
      "epoch: 3  batch: 13 / 122  loss: 0.4000219095211763  hr: 0  min: 24  sec: 28\n",
      "epoch: 3  batch: 14 / 122  loss: 0.4094101946268763  hr: 0  min: 24  sec: 28\n",
      "epoch: 3  batch: 15 / 122  loss: 0.40977632304032646  hr: 0  min: 24  sec: 39\n",
      "epoch: 3  batch: 16 / 122  loss: 0.4040048820897937  hr: 0  min: 24  sec: 32\n",
      "epoch: 3  batch: 17 / 122  loss: 0.4018171929261264  hr: 0  min: 24  sec: 19\n",
      "epoch: 3  batch: 18 / 122  loss: 0.402333017024729  hr: 0  min: 24  sec: 39\n",
      "epoch: 3  batch: 19 / 122  loss: 0.39189617727932174  hr: 0  min: 24  sec: 24\n",
      "epoch: 3  batch: 20 / 122  loss: 0.39557415843009947  hr: 0  min: 24  sec: 33\n",
      "epoch: 3  batch: 21 / 122  loss: 0.39942550517263864  hr: 0  min: 24  sec: 43\n",
      "epoch: 3  batch: 22 / 122  loss: 0.3975260623476722  hr: 0  min: 25  sec: 16\n",
      "epoch: 3  batch: 23 / 122  loss: 0.4076175339843916  hr: 0  min: 25  sec: 8\n",
      "epoch: 3  batch: 24 / 122  loss: 0.4002258287121852  hr: 0  min: 25  sec: 3\n",
      "epoch: 3  batch: 25 / 122  loss: 0.39543069779872897  hr: 0  min: 25  sec: 29\n",
      "epoch: 3  batch: 26 / 122  loss: 0.39193205019602406  hr: 0  min: 25  sec: 48\n",
      "epoch: 3  batch: 27 / 122  loss: 0.38951048420535195  hr: 0  min: 25  sec: 57\n",
      "epoch: 3  batch: 28 / 122  loss: 0.37986259242253645  hr: 0  min: 26  sec: 12\n",
      "epoch: 3  batch: 29 / 122  loss: 0.38587364734246815  hr: 0  min: 26  sec: 36\n",
      "epoch: 3  batch: 30 / 122  loss: 0.3922410177687804  hr: 0  min: 27  sec: 25\n",
      "epoch: 3  batch: 31 / 122  loss: 0.3985586231273989  hr: 0  min: 27  sec: 31\n",
      "epoch: 3  batch: 32 / 122  loss: 0.39728841255418956  hr: 0  min: 27  sec: 24\n",
      "epoch: 3  batch: 33 / 122  loss: 0.3924378664656119  hr: 0  min: 27  sec: 21\n",
      "epoch: 3  batch: 34 / 122  loss: 0.38995900158496466  hr: 0  min: 27  sec: 21\n",
      "epoch: 3  batch: 35 / 122  loss: 0.3920792164547103  hr: 0  min: 28  sec: 2\n",
      "epoch: 3  batch: 36 / 122  loss: 0.3998442633698384  hr: 0  min: 28  sec: 15\n",
      "epoch: 3  batch: 37 / 122  loss: 0.39358143528571  hr: 0  min: 28  sec: 9\n",
      "epoch: 3  batch: 38 / 122  loss: 0.3936509762547518  hr: 0  min: 28  sec: 27\n",
      "epoch: 3  batch: 39 / 122  loss: 0.39953108533070636  hr: 0  min: 28  sec: 13\n",
      "epoch: 3  batch: 40 / 122  loss: 0.40371664371341465  hr: 0  min: 28  sec: 6\n",
      "epoch: 3  batch: 41 / 122  loss: 0.4043596545007171  hr: 0  min: 28  sec: 6\n",
      "epoch: 3  batch: 42 / 122  loss: 0.39961426332592964  hr: 0  min: 28  sec: 34\n",
      "epoch: 3  batch: 43 / 122  loss: 0.3934369691929152  hr: 0  min: 28  sec: 47\n",
      "epoch: 3  batch: 44 / 122  loss: 0.3960190654139627  hr: 0  min: 28  sec: 46\n",
      "epoch: 3  batch: 45 / 122  loss: 0.3924414662851228  hr: 0  min: 28  sec: 48\n",
      "epoch: 3  batch: 46 / 122  loss: 0.38847828899388726  hr: 0  min: 28  sec: 52\n",
      "epoch: 3  batch: 47 / 122  loss: 0.385886364319223  hr: 0  min: 29  sec: 4\n",
      "epoch: 3  batch: 48 / 122  loss: 0.3868616019996504  hr: 0  min: 28  sec: 57\n",
      "epoch: 3  batch: 49 / 122  loss: 0.3862255107687444  hr: 0  min: 28  sec: 49\n",
      "epoch: 3  batch: 50 / 122  loss: 0.3902529336512089  hr: 0  min: 28  sec: 49\n",
      "epoch: 3  batch: 51 / 122  loss: 0.39186151778581096  hr: 0  min: 28  sec: 50\n",
      "epoch: 3  batch: 52 / 122  loss: 0.39649867509993225  hr: 0  min: 29  sec: 1\n",
      "epoch: 3  batch: 53 / 122  loss: 0.3979946201983488  hr: 0  min: 29  sec: 10\n",
      "epoch: 3  batch: 54 / 122  loss: 0.3984871590854945  hr: 0  min: 29  sec: 5\n",
      "epoch: 3  batch: 55 / 122  loss: 0.3993170858784155  hr: 0  min: 29  sec: 4\n",
      "epoch: 3  batch: 56 / 122  loss: 0.40637548600456547  hr: 0  min: 29  sec: 4\n",
      "epoch: 3  batch: 57 / 122  loss: 0.4072929020774992  hr: 0  min: 29  sec: 3\n",
      "epoch: 3  batch: 58 / 122  loss: 0.4063694041607709  hr: 0  min: 28  sec: 55\n",
      "epoch: 3  batch: 59 / 122  loss: 0.4055490267731376  hr: 0  min: 28  sec: 47\n",
      "epoch: 3  batch: 60 / 122  loss: 0.40360954515635966  hr: 0  min: 28  sec: 44\n",
      "epoch: 3  batch: 61 / 122  loss: 0.4011509874072231  hr: 0  min: 28  sec: 38\n",
      "epoch: 3  batch: 62 / 122  loss: 0.39849090972735035  hr: 0  min: 28  sec: 40\n",
      "epoch: 3  batch: 63 / 122  loss: 0.397619691751306  hr: 0  min: 28  sec: 41\n",
      "epoch: 3  batch: 64 / 122  loss: 0.39624955982435495  hr: 0  min: 28  sec: 39\n",
      "epoch: 3  batch: 65 / 122  loss: 0.3931445219195806  hr: 0  min: 28  sec: 43\n",
      "epoch: 3  batch: 66 / 122  loss: 0.38895382912773074  hr: 0  min: 28  sec: 59\n",
      "epoch: 3  batch: 67 / 122  loss: 0.3864374645610354  hr: 0  min: 28  sec: 52\n",
      "epoch: 3  batch: 68 / 122  loss: 0.39214637305806666  hr: 0  min: 28  sec: 52\n",
      "epoch: 3  batch: 69 / 122  loss: 0.39043753993683966  hr: 0  min: 28  sec: 46\n",
      "epoch: 3  batch: 70 / 122  loss: 0.38773337155580523  hr: 0  min: 28  sec: 45\n",
      "epoch: 3  batch: 71 / 122  loss: 0.38542664659694886  hr: 0  min: 28  sec: 43\n",
      "epoch: 3  batch: 72 / 122  loss: 0.3878115036835273  hr: 0  min: 28  sec: 42\n",
      "epoch: 3  batch: 73 / 122  loss: 0.38875179809250243  hr: 0  min: 28  sec: 41\n",
      "epoch: 3  batch: 74 / 122  loss: 0.38735243657956253  hr: 0  min: 28  sec: 37\n",
      "epoch: 3  batch: 75 / 122  loss: 0.38956058005491895  hr: 0  min: 28  sec: 37\n",
      "epoch: 3  batch: 76 / 122  loss: 0.38895791631780174  hr: 0  min: 28  sec: 31\n",
      "epoch: 3  batch: 77 / 122  loss: 0.3906894049474171  hr: 0  min: 28  sec: 26\n",
      "epoch: 3  batch: 78 / 122  loss: 0.3904792492588361  hr: 0  min: 28  sec: 22\n",
      "epoch: 3  batch: 79 / 122  loss: 0.3901541910216778  hr: 0  min: 28  sec: 18\n",
      "epoch: 3  batch: 80 / 122  loss: 0.3898051207885146  hr: 0  min: 28  sec: 13\n",
      "epoch: 3  batch: 81 / 122  loss: 0.3873285077236317  hr: 0  min: 28  sec: 14\n",
      "epoch: 3  batch: 82 / 122  loss: 0.39308381444070395  hr: 0  min: 28  sec: 18\n",
      "epoch: 3  batch: 83 / 122  loss: 0.395216152610549  hr: 0  min: 28  sec: 15\n",
      "epoch: 3  batch: 84 / 122  loss: 0.39281071811204865  hr: 0  min: 28  sec: 10\n",
      "epoch: 3  batch: 85 / 122  loss: 0.3965582077994066  hr: 0  min: 28  sec: 4\n",
      "epoch: 3  batch: 86 / 122  loss: 0.3937054077206656  hr: 0  min: 28  sec: 0\n",
      "epoch: 3  batch: 87 / 122  loss: 0.3920487215121587  hr: 0  min: 27  sec: 59\n",
      "epoch: 3  batch: 88 / 122  loss: 0.390630753710866  hr: 0  min: 28  sec: 10\n",
      "epoch: 3  batch: 89 / 122  loss: 0.38975774957222886  hr: 0  min: 28  sec: 6\n",
      "epoch: 3  batch: 90 / 122  loss: 0.3899672960241636  hr: 0  min: 28  sec: 1\n",
      "epoch: 3  batch: 91 / 122  loss: 0.39583825229943453  hr: 0  min: 27  sec: 57\n",
      "epoch: 3  batch: 92 / 122  loss: 0.3941510995121106  hr: 0  min: 28  sec: 3\n",
      "epoch: 3  batch: 93 / 122  loss: 0.39544299485221984  hr: 0  min: 27  sec: 56\n",
      "epoch: 3  batch: 94 / 122  loss: 0.3970582271192936  hr: 0  min: 28  sec: 6\n",
      "epoch: 3  batch: 95 / 122  loss: 0.4006089105417854  hr: 0  min: 28  sec: 3\n",
      "epoch: 3  batch: 96 / 122  loss: 0.3989063431508839  hr: 0  min: 27  sec: 57\n",
      "epoch: 3  batch: 97 / 122  loss: 0.3987858969833433  hr: 0  min: 27  sec: 53\n",
      "epoch: 3  batch: 98 / 122  loss: 0.3995034966845902  hr: 0  min: 27  sec: 49\n",
      "epoch: 3  batch: 99 / 122  loss: 0.40142059190706775  hr: 0  min: 27  sec: 48\n",
      "epoch: 3  batch: 100 / 122  loss: 0.402990550249815  hr: 0  min: 27  sec: 47\n",
      "Validation Loss after 100 batches: 0.6734457731246948\n",
      "epoch: 3  batch: 101 / 122  loss: 0.40220283768554727  hr: 0  min: 27  sec: 42\n",
      "epoch: 3  batch: 102 / 122  loss: 0.4004129920812214  hr: 0  min: 27  sec: 40\n",
      "epoch: 3  batch: 103 / 122  loss: 0.39904125468823515  hr: 0  min: 27  sec: 37\n",
      "epoch: 3  batch: 104 / 122  loss: 0.39711045731718725  hr: 0  min: 27  sec: 39\n",
      "epoch: 3  batch: 105 / 122  loss: 0.39657953949201674  hr: 0  min: 27  sec: 36\n",
      "epoch: 3  batch: 106 / 122  loss: 0.39558601604317717  hr: 0  min: 27  sec: 34\n",
      "epoch: 3  batch: 107 / 122  loss: 0.39585205856884753  hr: 0  min: 27  sec: 32\n",
      "epoch: 3  batch: 108 / 122  loss: 0.3937786046277594  hr: 0  min: 27  sec: 28\n",
      "epoch: 3  batch: 109 / 122  loss: 0.3917956617447214  hr: 0  min: 27  sec: 23\n",
      "epoch: 3  batch: 110 / 122  loss: 0.3906154800545086  hr: 0  min: 27  sec: 18\n",
      "epoch: 3  batch: 111 / 122  loss: 0.39082842420887304  hr: 0  min: 27  sec: 15\n",
      "epoch: 3  batch: 112 / 122  loss: 0.3899387123861483  hr: 0  min: 27  sec: 9\n",
      "epoch: 3  batch: 113 / 122  loss: 0.3904150603091822  hr: 0  min: 27  sec: 5\n",
      "epoch: 3  batch: 114 / 122  loss: 0.393220982007813  hr: 0  min: 27  sec: 4\n",
      "epoch: 3  batch: 115 / 122  loss: 0.3924541590006455  hr: 0  min: 27  sec: 10\n",
      "epoch: 3  batch: 116 / 122  loss: 0.39055864577149524  hr: 0  min: 27  sec: 9\n",
      "epoch: 3  batch: 117 / 122  loss: 0.39021801681090623  hr: 0  min: 27  sec: 5\n",
      "epoch: 3  batch: 118 / 122  loss: 0.3892391183871334  hr: 0  min: 27  sec: 3\n",
      "epoch: 3  batch: 119 / 122  loss: 0.3886853788329774  hr: 0  min: 26  sec: 59\n",
      "epoch: 3  batch: 120 / 122  loss: 0.38739039860665797  hr: 0  min: 27  sec: 3\n",
      "epoch: 3  batch: 121 / 122  loss: 0.38641969268479626  hr: 0  min: 26  sec: 59\n",
      "epoch: 3  batch: 122 / 122  loss: 0.38433592815379625  hr: 0  min: 26  sec: 55\n",
      "Learning rate: 1.3333333333333333e-05\n",
      "0.6131754368543625\n",
      "Saving...\n",
      "epoch: 4  batch: 1 / 122  loss: 0.4948870539665222  hr: 0  min: 21  sec: 7\n",
      "epoch: 4  batch: 2 / 122  loss: 0.34275516122579575  hr: 0  min: 18  sec: 51\n",
      "epoch: 4  batch: 3 / 122  loss: 0.29585038125514984  hr: 0  min: 19  sec: 29\n",
      "epoch: 4  batch: 4 / 122  loss: 0.32258081063628197  hr: 0  min: 19  sec: 59\n",
      "epoch: 4  batch: 5 / 122  loss: 0.3035606026649475  hr: 0  min: 23  sec: 5\n",
      "epoch: 4  batch: 6 / 122  loss: 0.3280957539876302  hr: 0  min: 22  sec: 58\n",
      "epoch: 4  batch: 7 / 122  loss: 0.3066743718726294  hr: 0  min: 23  sec: 13\n",
      "epoch: 4  batch: 8 / 122  loss: 0.2849506065249443  hr: 0  min: 23  sec: 3\n",
      "epoch: 4  batch: 9 / 122  loss: 0.30206695199012756  hr: 0  min: 23  sec: 2\n",
      "epoch: 4  batch: 10 / 122  loss: 0.3106054037809372  hr: 0  min: 22  sec: 33\n",
      "epoch: 4  batch: 11 / 122  loss: 0.3004779531197114  hr: 0  min: 22  sec: 21\n",
      "epoch: 4  batch: 12 / 122  loss: 0.29727228606740635  hr: 0  min: 21  sec: 55\n",
      "epoch: 4  batch: 13 / 122  loss: 0.2866425376672011  hr: 0  min: 22  sec: 12\n",
      "epoch: 4  batch: 14 / 122  loss: 0.29769361657755716  hr: 0  min: 21  sec: 59\n",
      "epoch: 4  batch: 15 / 122  loss: 0.29724544485410054  hr: 0  min: 21  sec: 37\n",
      "epoch: 4  batch: 16 / 122  loss: 0.2933368720114231  hr: 0  min: 21  sec: 45\n",
      "epoch: 4  batch: 17 / 122  loss: 0.2795601259259617  hr: 0  min: 22  sec: 5\n",
      "epoch: 4  batch: 18 / 122  loss: 0.27843277156352997  hr: 0  min: 21  sec: 52\n",
      "epoch: 4  batch: 19 / 122  loss: 0.2750398171575446  hr: 0  min: 22  sec: 9\n",
      "epoch: 4  batch: 20 / 122  loss: 0.2704990513622761  hr: 0  min: 21  sec: 54\n",
      "epoch: 4  batch: 21 / 122  loss: 0.27596894970961977  hr: 0  min: 21  sec: 44\n",
      "epoch: 4  batch: 22 / 122  loss: 0.2717340663075447  hr: 0  min: 21  sec: 43\n",
      "epoch: 4  batch: 23 / 122  loss: 0.26847107254940533  hr: 0  min: 21  sec: 47\n",
      "epoch: 4  batch: 24 / 122  loss: 0.2602950107927124  hr: 0  min: 21  sec: 35\n",
      "epoch: 4  batch: 25 / 122  loss: 0.25569981783628465  hr: 0  min: 21  sec: 27\n",
      "epoch: 4  batch: 26 / 122  loss: 0.24803057126700878  hr: 0  min: 21  sec: 21\n",
      "epoch: 4  batch: 27 / 122  loss: 0.24847289519729437  hr: 0  min: 21  sec: 13\n",
      "epoch: 4  batch: 28 / 122  loss: 0.24471668140696629  hr: 0  min: 21  sec: 6\n",
      "epoch: 4  batch: 29 / 122  loss: 0.24055534588365718  hr: 0  min: 21  sec: 3\n",
      "epoch: 4  batch: 30 / 122  loss: 0.24492557334403198  hr: 0  min: 21  sec: 1\n",
      "epoch: 4  batch: 31 / 122  loss: 0.24818911175093344  hr: 0  min: 20  sec: 54\n",
      "epoch: 4  batch: 32 / 122  loss: 0.2489718123106286  hr: 0  min: 20  sec: 51\n",
      "epoch: 4  batch: 33 / 122  loss: 0.2638729719275778  hr: 0  min: 20  sec: 44\n",
      "epoch: 4  batch: 34 / 122  loss: 0.26298444049761577  hr: 0  min: 20  sec: 43\n",
      "epoch: 4  batch: 35 / 122  loss: 0.264554650336504  hr: 0  min: 20  sec: 40\n",
      "epoch: 4  batch: 36 / 122  loss: 0.25830819364637136  hr: 0  min: 20  sec: 37\n",
      "epoch: 4  batch: 37 / 122  loss: 0.25856264287958275  hr: 0  min: 20  sec: 33\n",
      "epoch: 4  batch: 38 / 122  loss: 0.25624559378545536  hr: 0  min: 20  sec: 33\n",
      "epoch: 4  batch: 39 / 122  loss: 0.25110230738153827  hr: 0  min: 20  sec: 34\n",
      "epoch: 4  batch: 40 / 122  loss: 0.24624297237023712  hr: 0  min: 20  sec: 36\n",
      "epoch: 4  batch: 41 / 122  loss: 0.24199942472140965  hr: 0  min: 20  sec: 53\n",
      "epoch: 4  batch: 42 / 122  loss: 0.2428796843936046  hr: 0  min: 20  sec: 45\n",
      "epoch: 4  batch: 43 / 122  loss: 0.24409697490722634  hr: 0  min: 20  sec: 44\n",
      "epoch: 4  batch: 44 / 122  loss: 0.24865106552500615  hr: 0  min: 20  sec: 41\n",
      "epoch: 4  batch: 45 / 122  loss: 0.2512109086745315  hr: 0  min: 20  sec: 35\n",
      "epoch: 4  batch: 46 / 122  loss: 0.25021484388929344  hr: 0  min: 20  sec: 36\n",
      "epoch: 4  batch: 47 / 122  loss: 0.2521294281679265  hr: 0  min: 20  sec: 37\n",
      "epoch: 4  batch: 48 / 122  loss: 0.2572779564652592  hr: 0  min: 20  sec: 37\n",
      "epoch: 4  batch: 49 / 122  loss: 0.2629869917065513  hr: 0  min: 20  sec: 44\n",
      "epoch: 4  batch: 50 / 122  loss: 0.2597267768532038  hr: 0  min: 20  sec: 38\n",
      "epoch: 4  batch: 51 / 122  loss: 0.2610684777153473  hr: 0  min: 20  sec: 31\n",
      "epoch: 4  batch: 52 / 122  loss: 0.2662384570934452  hr: 0  min: 20  sec: 30\n",
      "epoch: 4  batch: 53 / 122  loss: 0.2644916374306634  hr: 0  min: 20  sec: 22\n",
      "epoch: 4  batch: 54 / 122  loss: 0.26611974345589123  hr: 0  min: 20  sec: 29\n",
      "epoch: 4  batch: 55 / 122  loss: 0.26427353112535046  hr: 0  min: 20  sec: 24\n",
      "epoch: 4  batch: 56 / 122  loss: 0.26426605115245494  hr: 0  min: 20  sec: 19\n",
      "epoch: 4  batch: 57 / 122  loss: 0.26545499167159986  hr: 0  min: 20  sec: 18\n",
      "epoch: 4  batch: 58 / 122  loss: 0.26473092679576626  hr: 0  min: 20  sec: 11\n",
      "epoch: 4  batch: 59 / 122  loss: 0.2612149428134247  hr: 0  min: 20  sec: 13\n",
      "epoch: 4  batch: 60 / 122  loss: 0.26229877676814795  hr: 0  min: 20  sec: 9\n",
      "epoch: 4  batch: 61 / 122  loss: 0.2609584769386737  hr: 0  min: 20  sec: 8\n",
      "epoch: 4  batch: 62 / 122  loss: 0.2630291281328086  hr: 0  min: 20  sec: 20\n",
      "epoch: 4  batch: 63 / 122  loss: 0.2629872979744086  hr: 0  min: 20  sec: 15\n",
      "epoch: 4  batch: 64 / 122  loss: 0.26828298933105543  hr: 0  min: 20  sec: 13\n",
      "epoch: 4  batch: 65 / 122  loss: 0.2667037132267769  hr: 0  min: 20  sec: 13\n",
      "epoch: 4  batch: 66 / 122  loss: 0.265465244029959  hr: 0  min: 20  sec: 8\n",
      "epoch: 4  batch: 67 / 122  loss: 0.2625735899739301  hr: 0  min: 20  sec: 5\n",
      "epoch: 4  batch: 68 / 122  loss: 0.2610033401562011  hr: 0  min: 20  sec: 1\n",
      "epoch: 4  batch: 69 / 122  loss: 0.2582844837528208  hr: 0  min: 19  sec: 59\n",
      "epoch: 4  batch: 70 / 122  loss: 0.26385578646191526  hr: 0  min: 19  sec: 59\n",
      "epoch: 4  batch: 71 / 122  loss: 0.2610709508225112  hr: 0  min: 19  sec: 55\n",
      "epoch: 4  batch: 72 / 122  loss: 0.25856689079147244  hr: 0  min: 19  sec: 51\n",
      "epoch: 4  batch: 73 / 122  loss: 0.2629449739541909  hr: 0  min: 19  sec: 46\n",
      "epoch: 4  batch: 74 / 122  loss: 0.2607149132904974  hr: 0  min: 19  sec: 42\n",
      "epoch: 4  batch: 75 / 122  loss: 0.26269693349798523  hr: 0  min: 19  sec: 38\n",
      "epoch: 4  batch: 76 / 122  loss: 0.2630820358172059  hr: 0  min: 19  sec: 47\n",
      "epoch: 4  batch: 77 / 122  loss: 0.2641590873819667  hr: 0  min: 19  sec: 44\n",
      "epoch: 4  batch: 78 / 122  loss: 0.2666495554626752  hr: 0  min: 19  sec: 42\n",
      "epoch: 4  batch: 79 / 122  loss: 0.26599475698946395  hr: 0  min: 19  sec: 39\n",
      "epoch: 4  batch: 80 / 122  loss: 0.2644786294084042  hr: 0  min: 19  sec: 39\n",
      "epoch: 4  batch: 81 / 122  loss: 0.2671169233450919  hr: 0  min: 19  sec: 39\n",
      "epoch: 4  batch: 82 / 122  loss: 0.2685274761170149  hr: 0  min: 19  sec: 36\n",
      "epoch: 4  batch: 83 / 122  loss: 0.26906034238187665  hr: 0  min: 19  sec: 37\n",
      "epoch: 4  batch: 84 / 122  loss: 0.2689347841466467  hr: 0  min: 19  sec: 32\n",
      "epoch: 4  batch: 85 / 122  loss: 0.26823319350095354  hr: 0  min: 19  sec: 31\n",
      "epoch: 4  batch: 86 / 122  loss: 0.26601613629176174  hr: 0  min: 19  sec: 29\n",
      "epoch: 4  batch: 87 / 122  loss: 0.2668866044574085  hr: 0  min: 19  sec: 25\n",
      "epoch: 4  batch: 88 / 122  loss: 0.2659011056900702  hr: 0  min: 19  sec: 27\n",
      "epoch: 4  batch: 89 / 122  loss: 0.26720448227578336  hr: 0  min: 19  sec: 28\n",
      "epoch: 4  batch: 90 / 122  loss: 0.26910062734451556  hr: 0  min: 19  sec: 25\n",
      "epoch: 4  batch: 91 / 122  loss: 0.2747165367907875  hr: 0  min: 19  sec: 23\n",
      "epoch: 4  batch: 92 / 122  loss: 0.2737349850409057  hr: 0  min: 19  sec: 28\n",
      "epoch: 4  batch: 93 / 122  loss: 0.2804538898009767  hr: 0  min: 19  sec: 25\n",
      "epoch: 4  batch: 94 / 122  loss: 0.28033194187632265  hr: 0  min: 19  sec: 21\n",
      "epoch: 4  batch: 95 / 122  loss: 0.28142046979383417  hr: 0  min: 19  sec: 20\n",
      "epoch: 4  batch: 96 / 122  loss: 0.2816490567056462  hr: 0  min: 19  sec: 19\n",
      "epoch: 4  batch: 97 / 122  loss: 0.28016145162514805  hr: 0  min: 19  sec: 16\n",
      "epoch: 4  batch: 98 / 122  loss: 0.28013444178718694  hr: 0  min: 19  sec: 13\n",
      "epoch: 4  batch: 99 / 122  loss: 0.277832366940048  hr: 0  min: 19  sec: 12\n",
      "epoch: 4  batch: 100 / 122  loss: 0.2755553648248315  hr: 0  min: 19  sec: 9\n",
      "Validation Loss after 100 batches: 0.7041571661829948\n",
      "epoch: 4  batch: 101 / 122  loss: 0.2784587750119148  hr: 0  min: 19  sec: 12\n",
      "epoch: 4  batch: 102 / 122  loss: 0.2771677194053636  hr: 0  min: 19  sec: 15\n",
      "epoch: 4  batch: 103 / 122  loss: 0.2753010011341387  hr: 0  min: 19  sec: 12\n",
      "epoch: 4  batch: 104 / 122  loss: 0.2736295317299664  hr: 0  min: 19  sec: 8\n",
      "epoch: 4  batch: 105 / 122  loss: 0.27444141837103025  hr: 0  min: 19  sec: 6\n",
      "epoch: 4  batch: 106 / 122  loss: 0.27278629461971093  hr: 0  min: 19  sec: 3\n",
      "epoch: 4  batch: 107 / 122  loss: 0.2729691354401201  hr: 0  min: 19  sec: 0\n",
      "epoch: 4  batch: 108 / 122  loss: 0.2707398224070116  hr: 0  min: 18  sec: 59\n",
      "epoch: 4  batch: 109 / 122  loss: 0.27179153653186394  hr: 0  min: 18  sec: 57\n",
      "epoch: 4  batch: 110 / 122  loss: 0.271651055528359  hr: 0  min: 18  sec: 53\n",
      "epoch: 4  batch: 111 / 122  loss: 0.27204915337465907  hr: 0  min: 18  sec: 50\n",
      "epoch: 4  batch: 112 / 122  loss: 0.27052835122283014  hr: 0  min: 18  sec: 54\n",
      "epoch: 4  batch: 113 / 122  loss: 0.2714330246754452  hr: 0  min: 18  sec: 54\n",
      "epoch: 4  batch: 114 / 122  loss: 0.27053214857975644  hr: 0  min: 18  sec: 50\n",
      "epoch: 4  batch: 115 / 122  loss: 0.26858213002914966  hr: 0  min: 18  sec: 47\n",
      "epoch: 4  batch: 116 / 122  loss: 0.26913985459069756  hr: 0  min: 18  sec: 44\n",
      "epoch: 4  batch: 117 / 122  loss: 0.26895785946239775  hr: 0  min: 18  sec: 43\n",
      "epoch: 4  batch: 118 / 122  loss: 0.26715614807681515  hr: 0  min: 18  sec: 44\n",
      "epoch: 4  batch: 119 / 122  loss: 0.26682962499120655  hr: 0  min: 18  sec: 43\n",
      "epoch: 4  batch: 120 / 122  loss: 0.2649138863819341  hr: 0  min: 18  sec: 44\n",
      "epoch: 4  batch: 121 / 122  loss: 0.2629508670450242  hr: 0  min: 18  sec: 42\n",
      "epoch: 4  batch: 122 / 122  loss: 0.2610179297778694  hr: 0  min: 18  sec: 39\n",
      "Learning rate: 1.1111111111111113e-05\n",
      "0.7738149136304855\n",
      "epoch: 5  batch: 1 / 122  loss: 0.09011439979076385  hr: 0  min: 14  sec: 36\n",
      "epoch: 5  batch: 2 / 122  loss: 0.0675143301486969  hr: 0  min: 15  sec: 12\n",
      "epoch: 5  batch: 3 / 122  loss: 0.1037772645552953  hr: 0  min: 15  sec: 56\n",
      "epoch: 5  batch: 4 / 122  loss: 0.08706631790846586  hr: 0  min: 15  sec: 27\n",
      "epoch: 5  batch: 5 / 122  loss: 0.1451871834695339  hr: 0  min: 15  sec: 51\n",
      "epoch: 5  batch: 6 / 122  loss: 0.16410785603026548  hr: 0  min: 16  sec: 39\n",
      "epoch: 5  batch: 7 / 122  loss: 0.21215953731111117  hr: 0  min: 16  sec: 25\n",
      "epoch: 5  batch: 8 / 122  loss: 0.2128342124633491  hr: 0  min: 18  sec: 13\n",
      "epoch: 5  batch: 9 / 122  loss: 0.2419409482843346  hr: 0  min: 18  sec: 12\n",
      "epoch: 5  batch: 10 / 122  loss: 0.22487102411687374  hr: 0  min: 17  sec: 41\n",
      "epoch: 5  batch: 11 / 122  loss: 0.20869066125967287  hr: 0  min: 18  sec: 16\n",
      "epoch: 5  batch: 12 / 122  loss: 0.19300213983903328  hr: 0  min: 18  sec: 2\n",
      "epoch: 5  batch: 13 / 122  loss: 0.18891645652743486  hr: 0  min: 17  sec: 50\n",
      "epoch: 5  batch: 14 / 122  loss: 0.18138484789856843  hr: 0  min: 17  sec: 40\n",
      "epoch: 5  batch: 15 / 122  loss: 0.1931904765466849  hr: 0  min: 17  sec: 54\n",
      "epoch: 5  batch: 16 / 122  loss: 0.1991316054482013  hr: 0  min: 18  sec: 6\n",
      "epoch: 5  batch: 17 / 122  loss: 0.19062518920092023  hr: 0  min: 18  sec: 7\n",
      "epoch: 5  batch: 18 / 122  loss: 0.1877672550164991  hr: 0  min: 17  sec: 55\n",
      "epoch: 5  batch: 19 / 122  loss: 0.1922805679864005  hr: 0  min: 17  sec: 46\n",
      "epoch: 5  batch: 20 / 122  loss: 0.18933214489370584  hr: 0  min: 17  sec: 44\n",
      "epoch: 5  batch: 21 / 122  loss: 0.19989302559267907  hr: 0  min: 18  sec: 11\n",
      "epoch: 5  batch: 22 / 122  loss: 0.20832042616199364  hr: 0  min: 18  sec: 6\n",
      "epoch: 5  batch: 23 / 122  loss: 0.2127632778947768  hr: 0  min: 18  sec: 0\n",
      "epoch: 5  batch: 24 / 122  loss: 0.20676967424030104  hr: 0  min: 17  sec: 51\n",
      "epoch: 5  batch: 25 / 122  loss: 0.20495739087462425  hr: 0  min: 17  sec: 49\n",
      "epoch: 5  batch: 26 / 122  loss: 0.20608781894239095  hr: 0  min: 17  sec: 59\n",
      "epoch: 5  batch: 27 / 122  loss: 0.19956773905842393  hr: 0  min: 18  sec: 0\n",
      "epoch: 5  batch: 28 / 122  loss: 0.2000464770410742  hr: 0  min: 17  sec: 59\n",
      "epoch: 5  batch: 29 / 122  loss: 0.2050617260151896  hr: 0  min: 17  sec: 54\n",
      "epoch: 5  batch: 30 / 122  loss: 0.2065880849957466  hr: 0  min: 17  sec: 54\n",
      "epoch: 5  batch: 31 / 122  loss: 0.20212460092959866  hr: 0  min: 17  sec: 47\n",
      "epoch: 5  batch: 32 / 122  loss: 0.2073974027298391  hr: 0  min: 17  sec: 41\n",
      "epoch: 5  batch: 33 / 122  loss: 0.2122353148279768  hr: 0  min: 17  sec: 58\n",
      "epoch: 5  batch: 34 / 122  loss: 0.20674962901017246  hr: 0  min: 17  sec: 50\n",
      "epoch: 5  batch: 35 / 122  loss: 0.20307082831859588  hr: 0  min: 17  sec: 42\n",
      "epoch: 5  batch: 36 / 122  loss: 0.19937127911382252  hr: 0  min: 17  sec: 38\n",
      "epoch: 5  batch: 37 / 122  loss: 0.19663862038303065  hr: 0  min: 17  sec: 36\n",
      "epoch: 5  batch: 38 / 122  loss: 0.1925668998768455  hr: 0  min: 17  sec: 30\n",
      "epoch: 5  batch: 39 / 122  loss: 0.18899872364142004  hr: 0  min: 17  sec: 36\n",
      "epoch: 5  batch: 40 / 122  loss: 0.1851250858977437  hr: 0  min: 17  sec: 40\n",
      "epoch: 5  batch: 41 / 122  loss: 0.1841376938107537  hr: 0  min: 17  sec: 38\n",
      "epoch: 5  batch: 42 / 122  loss: 0.180696010678297  hr: 0  min: 17  sec: 34\n",
      "epoch: 5  batch: 43 / 122  loss: 0.1770565652310155  hr: 0  min: 17  sec: 34\n",
      "epoch: 5  batch: 44 / 122  loss: 0.17748599876226348  hr: 0  min: 17  sec: 29\n",
      "epoch: 5  batch: 45 / 122  loss: 0.1769718690464894  hr: 0  min: 17  sec: 28\n",
      "epoch: 5  batch: 46 / 122  loss: 0.18682004483011755  hr: 0  min: 17  sec: 26\n",
      "epoch: 5  batch: 47 / 122  loss: 0.1837422444703097  hr: 0  min: 17  sec: 20\n",
      "epoch: 5  batch: 48 / 122  loss: 0.18674997391644865  hr: 0  min: 17  sec: 20\n",
      "epoch: 5  batch: 49 / 122  loss: 0.18864795288109049  hr: 0  min: 17  sec: 21\n",
      "epoch: 5  batch: 50 / 122  loss: 0.1956470527872443  hr: 0  min: 17  sec: 19\n",
      "epoch: 5  batch: 51 / 122  loss: 0.20106509178146428  hr: 0  min: 17  sec: 17\n",
      "epoch: 5  batch: 52 / 122  loss: 0.20710541676873198  hr: 0  min: 17  sec: 18\n",
      "epoch: 5  batch: 53 / 122  loss: 0.20674149323043958  hr: 0  min: 17  sec: 13\n",
      "epoch: 5  batch: 54 / 122  loss: 0.21084438316110107  hr: 0  min: 17  sec: 11\n",
      "epoch: 5  batch: 55 / 122  loss: 0.20920931639319115  hr: 0  min: 17  sec: 9\n",
      "epoch: 5  batch: 56 / 122  loss: 0.20737298321910203  hr: 0  min: 17  sec: 7\n",
      "epoch: 5  batch: 57 / 122  loss: 0.2042151551628322  hr: 0  min: 17  sec: 3\n",
      "epoch: 5  batch: 58 / 122  loss: 0.20124068361674918  hr: 0  min: 16  sec: 56\n",
      "epoch: 5  batch: 59 / 122  loss: 0.19988512456164523  hr: 0  min: 16  sec: 51\n",
      "epoch: 5  batch: 60 / 122  loss: 0.19719250078002612  hr: 0  min: 16  sec: 50\n",
      "epoch: 5  batch: 61 / 122  loss: 0.1942678903397478  hr: 0  min: 16  sec: 50\n",
      "epoch: 5  batch: 62 / 122  loss: 0.1918553845476239  hr: 0  min: 16  sec: 46\n",
      "epoch: 5  batch: 63 / 122  loss: 0.18914019159736142  hr: 0  min: 16  sec: 45\n",
      "epoch: 5  batch: 64 / 122  loss: 0.18839910739916377  hr: 0  min: 16  sec: 50\n",
      "epoch: 5  batch: 65 / 122  loss: 0.1891134342035422  hr: 0  min: 16  sec: 57\n",
      "epoch: 5  batch: 66 / 122  loss: 0.1917466288432479  hr: 0  min: 16  sec: 55\n",
      "epoch: 5  batch: 67 / 122  loss: 0.1893595386185308  hr: 0  min: 16  sec: 53\n",
      "epoch: 5  batch: 68 / 122  loss: 0.1978438628344413  hr: 0  min: 16  sec: 49\n",
      "epoch: 5  batch: 69 / 122  loss: 0.19536345592443494  hr: 0  min: 16  sec: 48\n",
      "epoch: 5  batch: 70 / 122  loss: 0.1934737156544413  hr: 0  min: 16  sec: 42\n",
      "epoch: 5  batch: 71 / 122  loss: 0.1951583483269517  hr: 0  min: 16  sec: 41\n",
      "epoch: 5  batch: 72 / 122  loss: 0.19312246433562702  hr: 0  min: 16  sec: 37\n",
      "epoch: 5  batch: 73 / 122  loss: 0.19222624538695976  hr: 0  min: 16  sec: 35\n",
      "epoch: 5  batch: 74 / 122  loss: 0.19284323542504697  hr: 0  min: 16  sec: 31\n",
      "epoch: 5  batch: 75 / 122  loss: 0.1906897454460462  hr: 0  min: 16  sec: 25\n",
      "epoch: 5  batch: 76 / 122  loss: 0.18934733450020613  hr: 0  min: 16  sec: 32\n",
      "epoch: 5  batch: 77 / 122  loss: 0.18857842945046238  hr: 0  min: 16  sec: 31\n",
      "epoch: 5  batch: 78 / 122  loss: 0.1871213267246882  hr: 0  min: 16  sec: 29\n",
      "epoch: 5  batch: 79 / 122  loss: 0.18912900965425034  hr: 0  min: 16  sec: 23\n",
      "epoch: 5  batch: 80 / 122  loss: 0.1888077264651656  hr: 0  min: 16  sec: 19\n",
      "epoch: 5  batch: 81 / 122  loss: 0.1902169594426214  hr: 0  min: 16  sec: 16\n",
      "epoch: 5  batch: 82 / 122  loss: 0.18977162369140765  hr: 0  min: 16  sec: 12\n",
      "epoch: 5  batch: 83 / 122  loss: 0.18975541767585707  hr: 0  min: 16  sec: 10\n",
      "epoch: 5  batch: 84 / 122  loss: 0.18776402657940275  hr: 0  min: 16  sec: 5\n",
      "epoch: 5  batch: 85 / 122  loss: 0.1876400202512741  hr: 0  min: 16  sec: 1\n",
      "epoch: 5  batch: 86 / 122  loss: 0.18867029682841413  hr: 0  min: 16  sec: 0\n",
      "epoch: 5  batch: 87 / 122  loss: 0.18906893521204762  hr: 0  min: 15  sec: 59\n",
      "epoch: 5  batch: 88 / 122  loss: 0.18983691270378503  hr: 0  min: 15  sec: 56\n",
      "epoch: 5  batch: 89 / 122  loss: 0.18895262722553832  hr: 0  min: 15  sec: 52\n",
      "epoch: 5  batch: 90 / 122  loss: 0.18806061521172523  hr: 0  min: 15  sec: 57\n",
      "epoch: 5  batch: 91 / 122  loss: 0.18928414591393627  hr: 0  min: 15  sec: 54\n",
      "epoch: 5  batch: 92 / 122  loss: 0.19526095761229162  hr: 0  min: 15  sec: 52\n",
      "epoch: 5  batch: 93 / 122  loss: 0.19923151060137698  hr: 0  min: 15  sec: 48\n",
      "epoch: 5  batch: 94 / 122  loss: 0.1980683301833082  hr: 0  min: 15  sec: 50\n",
      "epoch: 5  batch: 95 / 122  loss: 0.19698529321896402  hr: 0  min: 15  sec: 48\n",
      "epoch: 5  batch: 96 / 122  loss: 0.19525976094882935  hr: 0  min: 15  sec: 47\n",
      "epoch: 5  batch: 97 / 122  loss: 0.19781473451822074  hr: 0  min: 15  sec: 44\n",
      "epoch: 5  batch: 98 / 122  loss: 0.19636341769780433  hr: 0  min: 15  sec: 42\n",
      "epoch: 5  batch: 99 / 122  loss: 0.19458002772069338  hr: 0  min: 15  sec: 39\n",
      "epoch: 5  batch: 100 / 122  loss: 0.1944136935658753  hr: 0  min: 15  sec: 42\n",
      "Validation Loss after 100 batches: 0.8464251428842544\n",
      "epoch: 5  batch: 101 / 122  loss: 0.19272017775859573  hr: 0  min: 15  sec: 41\n",
      "epoch: 5  batch: 102 / 122  loss: 0.19098354164766623  hr: 0  min: 15  sec: 37\n",
      "epoch: 5  batch: 103 / 122  loss: 0.1895766250373235  hr: 0  min: 15  sec: 35\n",
      "epoch: 5  batch: 104 / 122  loss: 0.18812728330242232  hr: 0  min: 15  sec: 35\n",
      "epoch: 5  batch: 105 / 122  loss: 0.18652346722249474  hr: 0  min: 15  sec: 32\n",
      "epoch: 5  batch: 106 / 122  loss: 0.18533134803984244  hr: 0  min: 15  sec: 30\n",
      "epoch: 5  batch: 107 / 122  loss: 0.18384678318017275  hr: 0  min: 15  sec: 26\n",
      "epoch: 5  batch: 108 / 122  loss: 0.18362403541147984  hr: 0  min: 15  sec: 23\n",
      "epoch: 5  batch: 109 / 122  loss: 0.18212469257926994  hr: 0  min: 15  sec: 20\n",
      "epoch: 5  batch: 110 / 122  loss: 0.18180532408878208  hr: 0  min: 15  sec: 19\n",
      "epoch: 5  batch: 111 / 122  loss: 0.1821387198812387  hr: 0  min: 15  sec: 16\n",
      "epoch: 5  batch: 112 / 122  loss: 0.1842063086057481  hr: 0  min: 15  sec: 13\n",
      "epoch: 5  batch: 113 / 122  loss: 0.18337782446822498  hr: 0  min: 15  sec: 13\n",
      "epoch: 5  batch: 114 / 122  loss: 0.18219555183232092  hr: 0  min: 15  sec: 11\n",
      "epoch: 5  batch: 115 / 122  loss: 0.1809647367457333  hr: 0  min: 15  sec: 8\n",
      "epoch: 5  batch: 116 / 122  loss: 0.1795807829656606  hr: 0  min: 15  sec: 4\n",
      "epoch: 5  batch: 117 / 122  loss: 0.17878050880076793  hr: 0  min: 15  sec: 4\n",
      "epoch: 5  batch: 118 / 122  loss: 0.1777439967697581  hr: 0  min: 15  sec: 4\n",
      "epoch: 5  batch: 119 / 122  loss: 0.17834749871849262  hr: 0  min: 15  sec: 2\n",
      "epoch: 5  batch: 120 / 122  loss: 0.17756759627566984  hr: 0  min: 14  sec: 59\n",
      "epoch: 5  batch: 121 / 122  loss: 0.17670412642558006  hr: 0  min: 14  sec: 56\n",
      "epoch: 5  batch: 122 / 122  loss: 0.1754685329716103  hr: 0  min: 14  sec: 58\n",
      "Learning rate: 8.888888888888888e-06\n",
      "0.980083404481411\n",
      "epoch: 6  batch: 1 / 122  loss: 0.16203922033309937  hr: 0  min: 19  sec: 29\n",
      "epoch: 6  batch: 2 / 122  loss: 0.1091580931097269  hr: 0  min: 17  sec: 4\n",
      "epoch: 6  batch: 3 / 122  loss: 0.07642366426686446  hr: 0  min: 15  sec: 49\n",
      "epoch: 6  batch: 4 / 122  loss: 0.06360640469938517  hr: 0  min: 15  sec: 46\n",
      "epoch: 6  batch: 5 / 122  loss: 0.061825239658355714  hr: 0  min: 15  sec: 13\n",
      "epoch: 6  batch: 6 / 122  loss: 0.11783380309740703  hr: 0  min: 14  sec: 35\n",
      "epoch: 6  batch: 7 / 122  loss: 0.10272542613425426  hr: 0  min: 15  sec: 25\n",
      "epoch: 6  batch: 8 / 122  loss: 0.13729966164100915  hr: 0  min: 15  sec: 26\n",
      "epoch: 6  batch: 9 / 122  loss: 0.17338548859374392  hr: 0  min: 15  sec: 11\n",
      "epoch: 6  batch: 10 / 122  loss: 0.17244638139382004  hr: 0  min: 14  sec: 53\n",
      "epoch: 6  batch: 11 / 122  loss: 0.1720649621195414  hr: 0  min: 14  sec: 52\n",
      "epoch: 6  batch: 12 / 122  loss: 0.1598472302624335  hr: 0  min: 14  sec: 51\n",
      "epoch: 6  batch: 13 / 122  loss: 0.15259789324437195  hr: 0  min: 14  sec: 38\n",
      "epoch: 6  batch: 14 / 122  loss: 0.14725849983681524  hr: 0  min: 14  sec: 29\n",
      "epoch: 6  batch: 15 / 122  loss: 0.1416248936826984  hr: 0  min: 14  sec: 24\n",
      "epoch: 6  batch: 16 / 122  loss: 0.15261922957142815  hr: 0  min: 14  sec: 13\n",
      "epoch: 6  batch: 17 / 122  loss: 0.14558916664956248  hr: 0  min: 14  sec: 44\n",
      "epoch: 6  batch: 18 / 122  loss: 0.13938391886444557  hr: 0  min: 14  sec: 45\n",
      "epoch: 6  batch: 19 / 122  loss: 0.15005344167155654  hr: 0  min: 14  sec: 39\n",
      "epoch: 6  batch: 20 / 122  loss: 0.14440997648052872  hr: 0  min: 14  sec: 38\n",
      "epoch: 6  batch: 21 / 122  loss: 0.15294841459641853  hr: 0  min: 14  sec: 59\n",
      "epoch: 6  batch: 22 / 122  loss: 0.1505556035024876  hr: 0  min: 14  sec: 47\n",
      "epoch: 6  batch: 23 / 122  loss: 0.14516262446894593  hr: 0  min: 14  sec: 50\n",
      "epoch: 6  batch: 24 / 122  loss: 0.14024368716248622  hr: 0  min: 14  sec: 34\n",
      "epoch: 6  batch: 25 / 122  loss: 0.13812865387648343  hr: 0  min: 14  sec: 45\n",
      "epoch: 6  batch: 26 / 122  loss: 0.13392362740034094  hr: 0  min: 14  sec: 52\n",
      "epoch: 6  batch: 27 / 122  loss: 0.1388733193485273  hr: 0  min: 14  sec: 48\n",
      "epoch: 6  batch: 28 / 122  loss: 0.13730643952398427  hr: 0  min: 14  sec: 42\n",
      "epoch: 6  batch: 29 / 122  loss: 0.13461131368089338  hr: 0  min: 14  sec: 36\n",
      "epoch: 6  batch: 30 / 122  loss: 0.13392621325328946  hr: 0  min: 14  sec: 33\n",
      "epoch: 6  batch: 31 / 122  loss: 0.13118918861953482  hr: 0  min: 14  sec: 26\n",
      "epoch: 6  batch: 32 / 122  loss: 0.1294154831848573  hr: 0  min: 14  sec: 40\n",
      "epoch: 6  batch: 33 / 122  loss: 0.12683173254922483  hr: 0  min: 14  sec: 32\n",
      "epoch: 6  batch: 34 / 122  loss: 0.12375235653427594  hr: 0  min: 14  sec: 44\n",
      "epoch: 6  batch: 35 / 122  loss: 0.12269283541079079  hr: 0  min: 14  sec: 38\n",
      "epoch: 6  batch: 36 / 122  loss: 0.12035338940202361  hr: 0  min: 14  sec: 36\n",
      "epoch: 6  batch: 37 / 122  loss: 0.12172704962761821  hr: 0  min: 14  sec: 30\n",
      "epoch: 6  batch: 38 / 122  loss: 0.12104691964525141  hr: 0  min: 14  sec: 22\n",
      "epoch: 6  batch: 39 / 122  loss: 0.118377712316429  hr: 0  min: 14  sec: 23\n",
      "epoch: 6  batch: 40 / 122  loss: 0.11656719448510558  hr: 0  min: 14  sec: 24\n",
      "epoch: 6  batch: 41 / 122  loss: 0.11861546485253223  hr: 0  min: 14  sec: 21\n",
      "epoch: 6  batch: 42 / 122  loss: 0.1163181594484264  hr: 0  min: 14  sec: 14\n",
      "epoch: 6  batch: 43 / 122  loss: 0.11386529325919095  hr: 0  min: 14  sec: 17\n",
      "epoch: 6  batch: 44 / 122  loss: 0.11166807640851899  hr: 0  min: 14  sec: 11\n",
      "epoch: 6  batch: 45 / 122  loss: 0.11371585035489665  hr: 0  min: 14  sec: 5\n",
      "epoch: 6  batch: 46 / 122  loss: 0.11179632673282987  hr: 0  min: 14  sec: 4\n",
      "epoch: 6  batch: 47 / 122  loss: 0.1125858179987111  hr: 0  min: 13  sec: 59\n",
      "epoch: 6  batch: 48 / 122  loss: 0.1159218029351905  hr: 0  min: 13  sec: 58\n",
      "epoch: 6  batch: 49 / 122  loss: 0.12399485049655243  hr: 0  min: 13  sec: 55\n",
      "epoch: 6  batch: 50 / 122  loss: 0.12195541352033615  hr: 0  min: 13  sec: 50\n",
      "epoch: 6  batch: 51 / 122  loss: 0.11976013142688602  hr: 0  min: 13  sec: 47\n",
      "epoch: 6  batch: 52 / 122  loss: 0.12065959291962478  hr: 0  min: 13  sec: 53\n",
      "epoch: 6  batch: 53 / 122  loss: 0.12706964505168628  hr: 0  min: 13  sec: 49\n",
      "epoch: 6  batch: 54 / 122  loss: 0.1285804052043844  hr: 0  min: 13  sec: 45\n",
      "epoch: 6  batch: 55 / 122  loss: 0.12643539466979828  hr: 0  min: 13  sec: 43\n",
      "epoch: 6  batch: 56 / 122  loss: 0.1255352929001674  hr: 0  min: 13  sec: 38\n",
      "epoch: 6  batch: 57 / 122  loss: 0.12518437044989122  hr: 0  min: 13  sec: 43\n",
      "epoch: 6  batch: 58 / 122  loss: 0.12519254787536016  hr: 0  min: 13  sec: 43\n",
      "epoch: 6  batch: 59 / 122  loss: 0.12591022374581987  hr: 0  min: 13  sec: 39\n",
      "epoch: 6  batch: 60 / 122  loss: 0.1257562042058756  hr: 0  min: 13  sec: 34\n",
      "epoch: 6  batch: 61 / 122  loss: 0.12789830035667438  hr: 0  min: 13  sec: 29\n",
      "epoch: 6  batch: 62 / 122  loss: 0.12678345023924786  hr: 0  min: 13  sec: 28\n",
      "epoch: 6  batch: 63 / 122  loss: 0.12498584321685254  hr: 0  min: 13  sec: 24\n",
      "epoch: 6  batch: 64 / 122  loss: 0.12739586297539063  hr: 0  min: 13  sec: 20\n",
      "epoch: 6  batch: 65 / 122  loss: 0.1259789280879956  hr: 0  min: 13  sec: 17\n",
      "epoch: 6  batch: 66 / 122  loss: 0.12440699784818923  hr: 0  min: 13  sec: 22\n",
      "epoch: 6  batch: 67 / 122  loss: 0.1227781744951855  hr: 0  min: 13  sec: 21\n",
      "epoch: 6  batch: 68 / 122  loss: 0.12720845235676012  hr: 0  min: 13  sec: 16\n",
      "epoch: 6  batch: 69 / 122  loss: 0.1266158925153423  hr: 0  min: 13  sec: 12\n",
      "epoch: 6  batch: 70 / 122  loss: 0.12510530085169844  hr: 0  min: 13  sec: 10\n",
      "epoch: 6  batch: 71 / 122  loss: 0.12490497244863023  hr: 0  min: 13  sec: 11\n",
      "epoch: 6  batch: 72 / 122  loss: 0.12592981702699843  hr: 0  min: 13  sec: 9\n",
      "epoch: 6  batch: 73 / 122  loss: 0.12451218689906679  hr: 0  min: 13  sec: 5\n",
      "epoch: 6  batch: 74 / 122  loss: 0.1229734087262202  hr: 0  min: 13  sec: 4\n",
      "epoch: 6  batch: 75 / 122  loss: 0.12220805980265141  hr: 0  min: 13  sec: 2\n",
      "epoch: 6  batch: 76 / 122  loss: 0.12089381893900664  hr: 0  min: 12  sec: 59\n",
      "epoch: 6  batch: 77 / 122  loss: 0.11969137061145399  hr: 0  min: 12  sec: 56\n",
      "epoch: 6  batch: 78 / 122  loss: 0.11959145643199101  hr: 0  min: 12  sec: 58\n",
      "epoch: 6  batch: 79 / 122  loss: 0.12077172444779662  hr: 0  min: 12  sec: 56\n",
      "epoch: 6  batch: 80 / 122  loss: 0.11970773334614933  hr: 0  min: 12  sec: 53\n",
      "epoch: 6  batch: 81 / 122  loss: 0.11856922868317292  hr: 0  min: 12  sec: 51\n",
      "epoch: 6  batch: 82 / 122  loss: 0.11743000424580603  hr: 0  min: 12  sec: 50\n",
      "epoch: 6  batch: 83 / 122  loss: 0.11625762438648436  hr: 0  min: 12  sec: 50\n",
      "epoch: 6  batch: 84 / 122  loss: 0.1165660844361853  hr: 0  min: 12  sec: 49\n",
      "epoch: 6  batch: 85 / 122  loss: 0.11706213488736573  hr: 0  min: 12  sec: 49\n",
      "epoch: 6  batch: 86 / 122  loss: 0.11675858413150837  hr: 0  min: 12  sec: 46\n",
      "epoch: 6  batch: 87 / 122  loss: 0.11569630962678756  hr: 0  min: 12  sec: 41\n",
      "epoch: 6  batch: 88 / 122  loss: 0.11468880433081226  hr: 0  min: 12  sec: 40\n",
      "epoch: 6  batch: 89 / 122  loss: 0.11432952052924071  hr: 0  min: 12  sec: 36\n",
      "epoch: 6  batch: 90 / 122  loss: 0.11526969915462865  hr: 0  min: 12  sec: 34\n",
      "epoch: 6  batch: 91 / 122  loss: 0.11430998072846905  hr: 0  min: 12  sec: 32\n",
      "epoch: 6  batch: 92 / 122  loss: 0.11326052762729966  hr: 0  min: 12  sec: 29\n",
      "epoch: 6  batch: 93 / 122  loss: 0.11213228954703257  hr: 0  min: 12  sec: 27\n",
      "epoch: 6  batch: 94 / 122  loss: 0.11106039902710534  hr: 0  min: 12  sec: 24\n",
      "epoch: 6  batch: 95 / 122  loss: 0.11016061258943458  hr: 0  min: 12  sec: 23\n",
      "epoch: 6  batch: 96 / 122  loss: 0.10984654491767287  hr: 0  min: 12  sec: 22\n",
      "epoch: 6  batch: 97 / 122  loss: 0.11218456985409726  hr: 0  min: 12  sec: 19\n",
      "epoch: 6  batch: 98 / 122  loss: 0.11295229242164262  hr: 0  min: 12  sec: 16\n",
      "epoch: 6  batch: 99 / 122  loss: 0.1120020060075654  hr: 0  min: 12  sec: 14\n",
      "epoch: 6  batch: 100 / 122  loss: 0.11247801102697849  hr: 0  min: 12  sec: 10\n",
      "Validation Loss after 100 batches: 1.0146594271063805\n",
      "epoch: 6  batch: 101 / 122  loss: 0.11178506612040029  hr: 0  min: 12  sec: 8\n",
      "epoch: 6  batch: 102 / 122  loss: 0.11082997437858698  hr: 0  min: 12  sec: 6\n",
      "epoch: 6  batch: 103 / 122  loss: 0.10992384814087627  hr: 0  min: 12  sec: 2\n",
      "epoch: 6  batch: 104 / 122  loss: 0.11088939430192113  hr: 0  min: 11  sec: 59\n",
      "epoch: 6  batch: 105 / 122  loss: 0.11208782341508639  hr: 0  min: 11  sec: 56\n",
      "epoch: 6  batch: 106 / 122  loss: 0.1110910302036846  hr: 0  min: 11  sec: 53\n",
      "epoch: 6  batch: 107 / 122  loss: 0.11015245944661932  hr: 0  min: 11  sec: 50\n",
      "epoch: 6  batch: 108 / 122  loss: 0.10920553609159672  hr: 0  min: 11  sec: 47\n",
      "epoch: 6  batch: 109 / 122  loss: 0.1083726423895072  hr: 0  min: 11  sec: 45\n",
      "epoch: 6  batch: 110 / 122  loss: 0.10743463952517644  hr: 0  min: 11  sec: 42\n",
      "epoch: 6  batch: 111 / 122  loss: 0.10658332490286715  hr: 0  min: 11  sec: 40\n",
      "epoch: 6  batch: 112 / 122  loss: 0.10567868623599809  hr: 0  min: 11  sec: 37\n",
      "epoch: 6  batch: 113 / 122  loss: 0.10477759682499202  hr: 0  min: 11  sec: 34\n",
      "epoch: 6  batch: 114 / 122  loss: 0.1039882680286833  hr: 0  min: 11  sec: 30\n",
      "epoch: 6  batch: 115 / 122  loss: 0.10314604933576091  hr: 0  min: 11  sec: 28\n",
      "epoch: 6  batch: 116 / 122  loss: 0.10412991851776966  hr: 0  min: 11  sec: 25\n",
      "epoch: 6  batch: 117 / 122  loss: 0.1033162052392896  hr: 0  min: 11  sec: 23\n",
      "epoch: 6  batch: 118 / 122  loss: 0.10258826998188712  hr: 0  min: 11  sec: 19\n",
      "epoch: 6  batch: 119 / 122  loss: 0.10177133316095888  hr: 0  min: 11  sec: 19\n",
      "epoch: 6  batch: 120 / 122  loss: 0.10110239440109581  hr: 0  min: 11  sec: 16\n",
      "epoch: 6  batch: 121 / 122  loss: 0.10033087481259684  hr: 0  min: 11  sec: 12\n",
      "epoch: 6  batch: 122 / 122  loss: 0.09956002349919471  hr: 0  min: 11  sec: 10\n",
      "Learning rate: 6.666666666666667e-06\n",
      "1.0287454321980476\n",
      "epoch: 7  batch: 1 / 122  loss: 0.3419751822948456  hr: 0  min: 10  sec: 33\n",
      "epoch: 7  batch: 2 / 122  loss: 0.18267710134387016  hr: 0  min: 12  sec: 56\n",
      "epoch: 7  batch: 3 / 122  loss: 0.12934143034120402  hr: 0  min: 13  sec: 29\n",
      "epoch: 7  batch: 4 / 122  loss: 0.09998126165010035  hr: 0  min: 12  sec: 9\n",
      "epoch: 7  batch: 5 / 122  loss: 0.09857912082225084  hr: 0  min: 12  sec: 14\n",
      "epoch: 7  batch: 6 / 122  loss: 0.10737831906105082  hr: 0  min: 12  sec: 8\n",
      "epoch: 7  batch: 7 / 122  loss: 0.12672041795615638  hr: 0  min: 12  sec: 57\n",
      "epoch: 7  batch: 8 / 122  loss: 0.11202699632849544  hr: 0  min: 12  sec: 53\n",
      "epoch: 7  batch: 9 / 122  loss: 0.1030604722392228  hr: 0  min: 12  sec: 29\n",
      "epoch: 7  batch: 10 / 122  loss: 0.09332217094488442  hr: 0  min: 12  sec: 2\n",
      "epoch: 7  batch: 11 / 122  loss: 0.08558948570862412  hr: 0  min: 11  sec: 36\n",
      "epoch: 7  batch: 12 / 122  loss: 0.10804840018196653  hr: 0  min: 11  sec: 41\n",
      "epoch: 7  batch: 13 / 122  loss: 0.10020915511995554  hr: 0  min: 11  sec: 23\n",
      "epoch: 7  batch: 14 / 122  loss: 0.09398147976025939  hr: 0  min: 11  sec: 18\n",
      "epoch: 7  batch: 15 / 122  loss: 0.08818727942804495  hr: 0  min: 11  sec: 4\n",
      "epoch: 7  batch: 16 / 122  loss: 0.102826859918423  hr: 0  min: 10  sec: 58\n",
      "epoch: 7  batch: 17 / 122  loss: 0.09735311600653564  hr: 0  min: 10  sec: 47\n",
      "epoch: 7  batch: 18 / 122  loss: 0.09267998998984694  hr: 0  min: 10  sec: 42\n",
      "epoch: 7  batch: 19 / 122  loss: 0.08819022071302722  hr: 0  min: 10  sec: 39\n",
      "epoch: 7  batch: 20 / 122  loss: 0.08443782168906182  hr: 0  min: 10  sec: 31\n",
      "epoch: 7  batch: 21 / 122  loss: 0.08357575169897505  hr: 0  min: 10  sec: 29\n",
      "epoch: 7  batch: 22 / 122  loss: 0.08197836381044578  hr: 0  min: 10  sec: 29\n",
      "epoch: 7  batch: 23 / 122  loss: 0.07879804117281151  hr: 0  min: 10  sec: 25\n",
      "epoch: 7  batch: 24 / 122  loss: 0.07606777698189642  hr: 0  min: 10  sec: 24\n",
      "epoch: 7  batch: 25 / 122  loss: 0.07338535284623504  hr: 0  min: 10  sec: 22\n",
      "epoch: 7  batch: 26 / 122  loss: 0.0711897631498197  hr: 0  min: 10  sec: 19\n",
      "epoch: 7  batch: 27 / 122  loss: 0.06928609544411302  hr: 0  min: 10  sec: 16\n",
      "epoch: 7  batch: 28 / 122  loss: 0.06713239248243294  hr: 0  min: 10  sec: 11\n",
      "epoch: 7  batch: 29 / 122  loss: 0.0654017900694804  hr: 0  min: 10  sec: 9\n",
      "epoch: 7  batch: 30 / 122  loss: 0.0670755794737488  hr: 0  min: 10  sec: 4\n",
      "epoch: 7  batch: 31 / 122  loss: 0.0672468529683688  hr: 0  min: 9  sec: 59\n",
      "epoch: 7  batch: 32 / 122  loss: 0.06555807594850194  hr: 0  min: 9  sec: 57\n",
      "epoch: 7  batch: 33 / 122  loss: 0.0637942974342767  hr: 0  min: 9  sec: 53\n",
      "epoch: 7  batch: 34 / 122  loss: 0.06222569691839025  hr: 0  min: 9  sec: 49\n",
      "epoch: 7  batch: 35 / 122  loss: 0.060563236754387614  hr: 0  min: 9  sec: 53\n",
      "epoch: 7  batch: 36 / 122  loss: 0.05908507218636158  hr: 0  min: 9  sec: 51\n",
      "epoch: 7  batch: 37 / 122  loss: 0.05764139228424913  hr: 0  min: 9  sec: 48\n",
      "epoch: 7  batch: 38 / 122  loss: 0.060898585992522146  hr: 0  min: 9  sec: 46\n",
      "epoch: 7  batch: 39 / 122  loss: 0.05943985196212546  hr: 0  min: 9  sec: 45\n",
      "epoch: 7  batch: 40 / 122  loss: 0.058215510833542795  hr: 0  min: 9  sec: 46\n",
      "epoch: 7  batch: 41 / 122  loss: 0.07483688940680246  hr: 0  min: 9  sec: 42\n",
      "epoch: 7  batch: 42 / 122  loss: 0.07334018158282907  hr: 0  min: 9  sec: 42\n",
      "epoch: 7  batch: 43 / 122  loss: 0.07206477711063831  hr: 0  min: 9  sec: 42\n",
      "epoch: 7  batch: 44 / 122  loss: 0.07057925006798045  hr: 0  min: 9  sec: 40\n",
      "epoch: 7  batch: 45 / 122  loss: 0.06910605784505605  hr: 0  min: 9  sec: 38\n",
      "epoch: 7  batch: 46 / 122  loss: 0.07529391824146328  hr: 0  min: 9  sec: 34\n",
      "epoch: 7  batch: 47 / 122  loss: 0.0737844936728002  hr: 0  min: 9  sec: 33\n",
      "epoch: 7  batch: 48 / 122  loss: 0.07244438766307819  hr: 0  min: 9  sec: 31\n",
      "epoch: 7  batch: 49 / 122  loss: 0.07105430712619303  hr: 0  min: 9  sec: 28\n",
      "epoch: 7  batch: 50 / 122  loss: 0.06984600451774896  hr: 0  min: 9  sec: 24\n",
      "epoch: 7  batch: 51 / 122  loss: 0.06860806295356038  hr: 0  min: 9  sec: 22\n",
      "epoch: 7  batch: 52 / 122  loss: 0.06737185497947323  hr: 0  min: 9  sec: 23\n",
      "epoch: 7  batch: 53 / 122  loss: 0.06730694336957245  hr: 0  min: 9  sec: 22\n",
      "epoch: 7  batch: 54 / 122  loss: 0.0661892804092969  hr: 0  min: 9  sec: 21\n",
      "epoch: 7  batch: 55 / 122  loss: 0.06899829738011415  hr: 0  min: 9  sec: 19\n",
      "epoch: 7  batch: 56 / 122  loss: 0.06789407548162021  hr: 0  min: 9  sec: 16\n",
      "epoch: 7  batch: 57 / 122  loss: 0.07239171171462849  hr: 0  min: 9  sec: 19\n",
      "epoch: 7  batch: 58 / 122  loss: 0.07139353980791979  hr: 0  min: 9  sec: 16\n",
      "epoch: 7  batch: 59 / 122  loss: 0.07634166047229606  hr: 0  min: 9  sec: 14\n",
      "epoch: 7  batch: 60 / 122  loss: 0.07673094036678473  hr: 0  min: 9  sec: 11\n",
      "epoch: 7  batch: 61 / 122  loss: 0.07992081967045049  hr: 0  min: 9  sec: 8\n",
      "epoch: 7  batch: 62 / 122  loss: 0.07902570795868674  hr: 0  min: 9  sec: 6\n",
      "epoch: 7  batch: 63 / 122  loss: 0.07847357026877858  hr: 0  min: 9  sec: 3\n",
      "epoch: 7  batch: 64 / 122  loss: 0.07764476686134003  hr: 0  min: 9  sec: 3\n",
      "epoch: 7  batch: 65 / 122  loss: 0.07651414902737508  hr: 0  min: 9  sec: 1\n",
      "epoch: 7  batch: 66 / 122  loss: 0.07542290284552357  hr: 0  min: 8  sec: 58\n",
      "epoch: 7  batch: 67 / 122  loss: 0.07435179238013033  hr: 0  min: 8  sec: 56\n",
      "epoch: 7  batch: 68 / 122  loss: 0.07333344088607084  hr: 0  min: 8  sec: 54\n",
      "epoch: 7  batch: 69 / 122  loss: 0.08163267761752334  hr: 0  min: 8  sec: 53\n",
      "epoch: 7  batch: 70 / 122  loss: 0.08077348581448729  hr: 0  min: 8  sec: 51\n",
      "epoch: 7  batch: 71 / 122  loss: 0.07970656008905613  hr: 0  min: 8  sec: 49\n",
      "epoch: 7  batch: 72 / 122  loss: 0.07881774595848078  hr: 0  min: 8  sec: 46\n",
      "epoch: 7  batch: 73 / 122  loss: 0.07787769024056217  hr: 0  min: 8  sec: 45\n",
      "epoch: 7  batch: 74 / 122  loss: 0.07708990515046124  hr: 0  min: 8  sec: 44\n",
      "epoch: 7  batch: 75 / 122  loss: 0.07892029624742766  hr: 0  min: 8  sec: 41\n",
      "epoch: 7  batch: 76 / 122  loss: 0.08221895660972223  hr: 0  min: 8  sec: 42\n",
      "epoch: 7  batch: 77 / 122  loss: 0.08120334099119457  hr: 0  min: 8  sec: 44\n",
      "epoch: 7  batch: 78 / 122  loss: 0.08023313482184537  hr: 0  min: 8  sec: 41\n",
      "epoch: 7  batch: 79 / 122  loss: 0.0823486156060207  hr: 0  min: 8  sec: 39\n",
      "epoch: 7  batch: 80 / 122  loss: 0.08146292149613146  hr: 0  min: 8  sec: 36\n",
      "epoch: 7  batch: 81 / 122  loss: 0.08315128705717255  hr: 0  min: 8  sec: 34\n",
      "epoch: 7  batch: 82 / 122  loss: 0.08224785726846809  hr: 0  min: 8  sec: 32\n",
      "epoch: 7  batch: 83 / 122  loss: 0.0813087334278522  hr: 0  min: 8  sec: 30\n",
      "epoch: 7  batch: 84 / 122  loss: 0.08040687045182235  hr: 0  min: 8  sec: 29\n",
      "epoch: 7  batch: 85 / 122  loss: 0.08125089698690263  hr: 0  min: 8  sec: 31\n",
      "epoch: 7  batch: 86 / 122  loss: 0.08034463737398213  hr: 0  min: 8  sec: 31\n",
      "epoch: 7  batch: 87 / 122  loss: 0.0794617874820813  hr: 0  min: 8  sec: 29\n",
      "epoch: 7  batch: 88 / 122  loss: 0.07871864148447374  hr: 0  min: 8  sec: 28\n",
      "epoch: 7  batch: 89 / 122  loss: 0.07787406970118957  hr: 0  min: 8  sec: 28\n",
      "epoch: 7  batch: 90 / 122  loss: 0.08143637004670584  hr: 0  min: 8  sec: 25\n",
      "epoch: 7  batch: 91 / 122  loss: 0.08166328283630639  hr: 0  min: 8  sec: 22\n",
      "epoch: 7  batch: 92 / 122  loss: 0.08112044039475934  hr: 0  min: 8  sec: 19\n",
      "epoch: 7  batch: 93 / 122  loss: 0.08032077432970607  hr: 0  min: 8  sec: 17\n",
      "epoch: 7  batch: 94 / 122  loss: 0.07953944973439533  hr: 0  min: 8  sec: 17\n",
      "epoch: 7  batch: 95 / 122  loss: 0.0787616306303167  hr: 0  min: 8  sec: 15\n",
      "epoch: 7  batch: 96 / 122  loss: 0.07797688083276928  hr: 0  min: 8  sec: 13\n",
      "epoch: 7  batch: 97 / 122  loss: 0.0772399763125427  hr: 0  min: 8  sec: 11\n",
      "epoch: 7  batch: 98 / 122  loss: 0.07689934541300243  hr: 0  min: 8  sec: 12\n",
      "epoch: 7  batch: 99 / 122  loss: 0.07619261683312932  hr: 0  min: 8  sec: 9\n",
      "epoch: 7  batch: 100 / 122  loss: 0.0776955640502274  hr: 0  min: 8  sec: 10\n",
      "Validation Loss after 100 batches: 1.1426163613796234\n",
      "epoch: 7  batch: 101 / 122  loss: 0.07695948426833026  hr: 0  min: 8  sec: 8\n",
      "epoch: 7  batch: 102 / 122  loss: 0.07623530028150509  hr: 0  min: 8  sec: 6\n",
      "epoch: 7  batch: 103 / 122  loss: 0.07891173887256424  hr: 0  min: 8  sec: 6\n",
      "epoch: 7  batch: 104 / 122  loss: 0.07819867123348209  hr: 0  min: 8  sec: 4\n",
      "epoch: 7  batch: 105 / 122  loss: 0.07747523667778643  hr: 0  min: 8  sec: 1\n",
      "epoch: 7  batch: 106 / 122  loss: 0.07676282767997177  hr: 0  min: 7  sec: 58\n",
      "epoch: 7  batch: 107 / 122  loss: 0.07607539445509977  hr: 0  min: 7  sec: 56\n",
      "epoch: 7  batch: 108 / 122  loss: 0.07762159960758355  hr: 0  min: 7  sec: 54\n",
      "epoch: 7  batch: 109 / 122  loss: 0.07819811427333487  hr: 0  min: 7  sec: 53\n",
      "epoch: 7  batch: 110 / 122  loss: 0.07754028935239396  hr: 0  min: 7  sec: 51\n",
      "epoch: 7  batch: 111 / 122  loss: 0.07686639200426168  hr: 0  min: 7  sec: 48\n",
      "epoch: 7  batch: 112 / 122  loss: 0.07619832224112802  hr: 0  min: 7  sec: 46\n",
      "epoch: 7  batch: 113 / 122  loss: 0.07586963838213577  hr: 0  min: 7  sec: 44\n",
      "epoch: 7  batch: 114 / 122  loss: 0.07522377029486131  hr: 0  min: 7  sec: 42\n",
      "epoch: 7  batch: 115 / 122  loss: 0.07459433175744894  hr: 0  min: 7  sec: 40\n",
      "epoch: 7  batch: 116 / 122  loss: 0.07409769273965203  hr: 0  min: 7  sec: 38\n",
      "epoch: 7  batch: 117 / 122  loss: 0.07348736369799282  hr: 0  min: 7  sec: 36\n",
      "epoch: 7  batch: 118 / 122  loss: 0.07445726291694821  hr: 0  min: 7  sec: 35\n",
      "epoch: 7  batch: 119 / 122  loss: 0.07417111146310736  hr: 0  min: 7  sec: 33\n",
      "epoch: 7  batch: 120 / 122  loss: 0.07359252695653898  hr: 0  min: 7  sec: 31\n",
      "epoch: 7  batch: 121 / 122  loss: 0.07300437800393801  hr: 0  min: 7  sec: 29\n",
      "epoch: 7  batch: 122 / 122  loss: 0.07242749168987951  hr: 0  min: 7  sec: 27\n",
      "Learning rate: 4.444444444444444e-06\n",
      "1.1271353974938392\n",
      "epoch: 8  batch: 1 / 122  loss: 0.0037785766180604696  hr: 0  min: 7  sec: 29\n",
      "epoch: 8  batch: 2 / 122  loss: 0.014399234089069068  hr: 0  min: 7  sec: 11\n",
      "epoch: 8  batch: 3 / 122  loss: 0.04060655824529628  hr: 0  min: 6  sec: 43\n",
      "epoch: 8  batch: 4 / 122  loss: 0.03510473103960976  hr: 0  min: 6  sec: 52\n",
      "epoch: 8  batch: 5 / 122  loss: 0.02962754382751882  hr: 0  min: 6  sec: 34\n",
      "epoch: 8  batch: 6 / 122  loss: 0.02602082067945351  hr: 0  min: 6  sec: 35\n",
      "epoch: 8  batch: 7 / 122  loss: 0.022992312741865004  hr: 0  min: 6  sec: 51\n",
      "epoch: 8  batch: 8 / 122  loss: 0.05813069627038203  hr: 0  min: 6  sec: 50\n",
      "epoch: 8  batch: 9 / 122  loss: 0.05294847961825629  hr: 0  min: 6  sec: 51\n",
      "epoch: 8  batch: 10 / 122  loss: 0.04867256523575634  hr: 0  min: 6  sec: 58\n",
      "epoch: 8  batch: 11 / 122  loss: 0.05069132231768559  hr: 0  min: 7  sec: 19\n",
      "epoch: 8  batch: 12 / 122  loss: 0.057228588324505836  hr: 0  min: 7  sec: 11\n",
      "epoch: 8  batch: 13 / 122  loss: 0.05336880370473059  hr: 0  min: 7  sec: 5\n",
      "epoch: 8  batch: 14 / 122  loss: 0.059395527483762374  hr: 0  min: 7  sec: 0\n",
      "epoch: 8  batch: 15 / 122  loss: 0.06844024883272747  hr: 0  min: 6  sec: 54\n",
      "epoch: 8  batch: 16 / 122  loss: 0.06483939797908533  hr: 0  min: 6  sec: 49\n",
      "epoch: 8  batch: 17 / 122  loss: 0.062109121354296803  hr: 0  min: 6  sec: 49\n",
      "epoch: 8  batch: 18 / 122  loss: 0.05914565570290304  hr: 0  min: 6  sec: 50\n",
      "epoch: 8  batch: 19 / 122  loss: 0.06198114844186133  hr: 0  min: 6  sec: 49\n",
      "epoch: 8  batch: 20 / 122  loss: 0.06059357636841014  hr: 0  min: 6  sec: 45\n",
      "epoch: 8  batch: 21 / 122  loss: 0.05898094391228542  hr: 0  min: 6  sec: 56\n",
      "epoch: 8  batch: 22 / 122  loss: 0.06150272625117478  hr: 0  min: 6  sec: 49\n",
      "epoch: 8  batch: 23 / 122  loss: 0.06287343185597463  hr: 0  min: 6  sec: 48\n",
      "epoch: 8  batch: 24 / 122  loss: 0.060840804023124896  hr: 0  min: 6  sec: 43\n",
      "epoch: 8  batch: 25 / 122  loss: 0.059020767556503415  hr: 0  min: 6  sec: 38\n",
      "epoch: 8  batch: 26 / 122  loss: 0.05687798058184294  hr: 0  min: 6  sec: 37\n",
      "epoch: 8  batch: 27 / 122  loss: 0.05506348699607231  hr: 0  min: 6  sec: 39\n",
      "epoch: 8  batch: 28 / 122  loss: 0.05323410143942705  hr: 0  min: 6  sec: 37\n",
      "epoch: 8  batch: 29 / 122  loss: 0.05249801191404976  hr: 0  min: 6  sec: 33\n",
      "epoch: 8  batch: 30 / 122  loss: 0.05087476934616764  hr: 0  min: 6  sec: 30\n",
      "epoch: 8  batch: 31 / 122  loss: 0.04939165313337599  hr: 0  min: 6  sec: 31\n",
      "epoch: 8  batch: 32 / 122  loss: 0.04798744402069133  hr: 0  min: 6  sec: 28\n",
      "epoch: 8  batch: 33 / 122  loss: 0.04688449704906705  hr: 0  min: 6  sec: 26\n",
      "epoch: 8  batch: 34 / 122  loss: 0.046658849603879976  hr: 0  min: 6  sec: 27\n",
      "epoch: 8  batch: 35 / 122  loss: 0.0456494975968131  hr: 0  min: 6  sec: 25\n",
      "epoch: 8  batch: 36 / 122  loss: 0.0494585684629985  hr: 0  min: 6  sec: 21\n",
      "epoch: 8  batch: 37 / 122  loss: 0.048254427223189456  hr: 0  min: 6  sec: 20\n",
      "epoch: 8  batch: 38 / 122  loss: 0.04708035294530227  hr: 0  min: 6  sec: 19\n",
      "epoch: 8  batch: 39 / 122  loss: 0.04598963457064178  hr: 0  min: 6  sec: 16\n",
      "epoch: 8  batch: 40 / 122  loss: 0.04675821816199459  hr: 0  min: 6  sec: 14\n",
      "epoch: 8  batch: 41 / 122  loss: 0.04585500823038562  hr: 0  min: 6  sec: 16\n",
      "epoch: 8  batch: 42 / 122  loss: 0.04483348986555245  hr: 0  min: 6  sec: 11\n",
      "epoch: 8  batch: 43 / 122  loss: 0.043989476504150866  hr: 0  min: 6  sec: 8\n",
      "epoch: 8  batch: 44 / 122  loss: 0.043072893898087466  hr: 0  min: 6  sec: 10\n",
      "epoch: 8  batch: 45 / 122  loss: 0.042197583134596546  hr: 0  min: 6  sec: 7\n",
      "epoch: 8  batch: 46 / 122  loss: 0.041640931269680354  hr: 0  min: 6  sec: 7\n",
      "epoch: 8  batch: 47 / 122  loss: 0.0447536092796462  hr: 0  min: 6  sec: 4\n",
      "epoch: 8  batch: 48 / 122  loss: 0.0496533091791207  hr: 0  min: 6  sec: 2\n",
      "epoch: 8  batch: 49 / 122  loss: 0.05059276362976097  hr: 0  min: 6  sec: 0\n",
      "epoch: 8  batch: 50 / 122  loss: 0.04983076927717775  hr: 0  min: 6  sec: 0\n",
      "epoch: 8  batch: 51 / 122  loss: 0.04891608059680199  hr: 0  min: 5  sec: 58\n",
      "epoch: 8  batch: 52 / 122  loss: 0.04864172597612756  hr: 0  min: 5  sec: 55\n",
      "epoch: 8  batch: 53 / 122  loss: 0.04870814116197234  hr: 0  min: 5  sec: 53\n",
      "epoch: 8  batch: 54 / 122  loss: 0.04788670734140194  hr: 0  min: 5  sec: 51\n",
      "epoch: 8  batch: 55 / 122  loss: 0.04706529013135217  hr: 0  min: 5  sec: 50\n",
      "epoch: 8  batch: 56 / 122  loss: 0.04634265487714272  hr: 0  min: 5  sec: 47\n",
      "epoch: 8  batch: 57 / 122  loss: 0.04581443709729795  hr: 0  min: 5  sec: 44\n",
      "epoch: 8  batch: 58 / 122  loss: 0.04509164883899663  hr: 0  min: 5  sec: 42\n",
      "epoch: 8  batch: 59 / 122  loss: 0.04590094054386146  hr: 0  min: 5  sec: 40\n",
      "epoch: 8  batch: 60 / 122  loss: 0.04524113807128742  hr: 0  min: 5  sec: 37\n",
      "epoch: 8  batch: 61 / 122  loss: 0.04460844683430356  hr: 0  min: 5  sec: 38\n",
      "epoch: 8  batch: 62 / 122  loss: 0.04411811418005175  hr: 0  min: 5  sec: 36\n",
      "epoch: 8  batch: 63 / 122  loss: 0.04355494704999266  hr: 0  min: 5  sec: 34\n",
      "epoch: 8  batch: 64 / 122  loss: 0.04294968323301873  hr: 0  min: 5  sec: 33\n",
      "epoch: 8  batch: 65 / 122  loss: 0.042350924735029154  hr: 0  min: 5  sec: 31\n",
      "epoch: 8  batch: 66 / 122  loss: 0.041765215817246244  hr: 0  min: 5  sec: 30\n",
      "epoch: 8  batch: 67 / 122  loss: 0.04118150494534022  hr: 0  min: 5  sec: 27\n",
      "epoch: 8  batch: 68 / 122  loss: 0.04062011798925917  hr: 0  min: 5  sec: 28\n",
      "epoch: 8  batch: 69 / 122  loss: 0.04134235136291903  hr: 0  min: 5  sec: 26\n",
      "epoch: 8  batch: 70 / 122  loss: 0.04087605876182871  hr: 0  min: 5  sec: 25\n",
      "epoch: 8  batch: 71 / 122  loss: 0.04033757602586083  hr: 0  min: 5  sec: 23\n",
      "epoch: 8  batch: 72 / 122  loss: 0.040464946980743356  hr: 0  min: 5  sec: 20\n",
      "epoch: 8  batch: 73 / 122  loss: 0.039950704779348346  hr: 0  min: 5  sec: 18\n",
      "epoch: 8  batch: 74 / 122  loss: 0.039445416001300956  hr: 0  min: 5  sec: 17\n",
      "epoch: 8  batch: 75 / 122  loss: 0.03897180644174417  hr: 0  min: 5  sec: 16\n",
      "epoch: 8  batch: 76 / 122  loss: 0.03851091710997647  hr: 0  min: 5  sec: 14\n",
      "epoch: 8  batch: 77 / 122  loss: 0.046389355348398935  hr: 0  min: 5  sec: 12\n",
      "epoch: 8  batch: 78 / 122  loss: 0.051854881016203225  hr: 0  min: 5  sec: 9\n",
      "epoch: 8  batch: 79 / 122  loss: 0.051231655604618634  hr: 0  min: 5  sec: 7\n",
      "epoch: 8  batch: 80 / 122  loss: 0.05196202126680873  hr: 0  min: 5  sec: 5\n",
      "epoch: 8  batch: 81 / 122  loss: 0.05195769846416366  hr: 0  min: 5  sec: 3\n",
      "epoch: 8  batch: 82 / 122  loss: 0.05138325121072007  hr: 0  min: 5  sec: 0\n",
      "epoch: 8  batch: 83 / 122  loss: 0.05556613469429045  hr: 0  min: 4  sec: 58\n",
      "epoch: 8  batch: 84 / 122  loss: 0.05547200531388322  hr: 0  min: 4  sec: 57\n",
      "epoch: 8  batch: 85 / 122  loss: 0.05487742156119031  hr: 0  min: 4  sec: 54\n",
      "epoch: 8  batch: 86 / 122  loss: 0.05427192187824741  hr: 0  min: 4  sec: 52\n",
      "epoch: 8  batch: 87 / 122  loss: 0.053940289702009536  hr: 0  min: 4  sec: 50\n",
      "epoch: 8  batch: 88 / 122  loss: 0.05335974876917052  hr: 0  min: 4  sec: 48\n",
      "epoch: 8  batch: 89 / 122  loss: 0.05436067849795303  hr: 0  min: 4  sec: 45\n",
      "epoch: 8  batch: 90 / 122  loss: 0.05382694446792205  hr: 0  min: 4  sec: 44\n",
      "epoch: 8  batch: 91 / 122  loss: 0.05326633503056735  hr: 0  min: 4  sec: 42\n",
      "epoch: 8  batch: 92 / 122  loss: 0.053387135953095785  hr: 0  min: 4  sec: 40\n",
      "epoch: 8  batch: 93 / 122  loss: 0.0528673494049418  hr: 0  min: 4  sec: 38\n",
      "epoch: 8  batch: 94 / 122  loss: 0.05234921465261265  hr: 0  min: 4  sec: 36\n",
      "epoch: 8  batch: 95 / 122  loss: 0.05233210097067058  hr: 0  min: 4  sec: 34\n",
      "epoch: 8  batch: 96 / 122  loss: 0.05181880268840663  hr: 0  min: 4  sec: 34\n",
      "epoch: 8  batch: 97 / 122  loss: 0.05142325290344348  hr: 0  min: 4  sec: 32\n",
      "epoch: 8  batch: 98 / 122  loss: 0.05092986943247747  hr: 0  min: 4  sec: 30\n",
      "epoch: 8  batch: 99 / 122  loss: 0.05056770111321274  hr: 0  min: 4  sec: 29\n",
      "epoch: 8  batch: 100 / 122  loss: 0.05011326688574627  hr: 0  min: 4  sec: 27\n",
      "Validation Loss after 100 batches: 1.1662946552038194\n",
      "epoch: 8  batch: 101 / 122  loss: 0.04963510565916168  hr: 0  min: 4  sec: 25\n",
      "epoch: 8  batch: 102 / 122  loss: 0.049167636086862976  hr: 0  min: 4  sec: 23\n",
      "epoch: 8  batch: 103 / 122  loss: 0.04871095377540451  hr: 0  min: 4  sec: 21\n",
      "epoch: 8  batch: 104 / 122  loss: 0.04832535850157281  hr: 0  min: 4  sec: 18\n",
      "epoch: 8  batch: 105 / 122  loss: 0.04788167379752156  hr: 0  min: 4  sec: 16\n",
      "epoch: 8  batch: 106 / 122  loss: 0.047551099850163564  hr: 0  min: 4  sec: 14\n",
      "epoch: 8  batch: 107 / 122  loss: 0.04712359595578164  hr: 0  min: 4  sec: 12\n",
      "epoch: 8  batch: 108 / 122  loss: 0.04670772189597806  hr: 0  min: 4  sec: 10\n",
      "epoch: 8  batch: 109 / 122  loss: 0.048371509387192035  hr: 0  min: 4  sec: 9\n",
      "epoch: 8  batch: 110 / 122  loss: 0.047954493696505035  hr: 0  min: 4  sec: 7\n",
      "epoch: 8  batch: 111 / 122  loss: 0.04753751989961644  hr: 0  min: 4  sec: 5\n",
      "epoch: 8  batch: 112 / 122  loss: 0.047126984440962714  hr: 0  min: 4  sec: 4\n",
      "epoch: 8  batch: 113 / 122  loss: 0.04672528912964384  hr: 0  min: 4  sec: 2\n",
      "epoch: 8  batch: 114 / 122  loss: 0.04633032360620666  hr: 0  min: 4  sec: 1\n",
      "epoch: 8  batch: 115 / 122  loss: 0.045942406555759194  hr: 0  min: 3  sec: 58\n",
      "epoch: 8  batch: 116 / 122  loss: 0.045574957389456765  hr: 0  min: 3  sec: 57\n",
      "epoch: 8  batch: 117 / 122  loss: 0.04557016426319272  hr: 0  min: 3  sec: 55\n",
      "epoch: 8  batch: 118 / 122  loss: 0.04519804166649642  hr: 0  min: 3  sec: 53\n",
      "epoch: 8  batch: 119 / 122  loss: 0.044833381029292005  hr: 0  min: 3  sec: 51\n",
      "epoch: 8  batch: 120 / 122  loss: 0.048436885221356835  hr: 0  min: 3  sec: 49\n",
      "epoch: 8  batch: 121 / 122  loss: 0.04804954174059359  hr: 0  min: 3  sec: 47\n",
      "epoch: 8  batch: 122 / 122  loss: 0.0476746182167353  hr: 0  min: 3  sec: 45\n",
      "Learning rate: 2.222222222222222e-06\n",
      "1.192286904156208\n",
      "epoch: 9  batch: 1 / 122  loss: 0.015488393604755402  hr: 0  min: 6  sec: 4\n",
      "epoch: 9  batch: 2 / 122  loss: 0.00940216169692576  hr: 0  min: 4  sec: 59\n",
      "epoch: 9  batch: 3 / 122  loss: 0.007108094170689583  hr: 0  min: 4  sec: 22\n",
      "epoch: 9  batch: 4 / 122  loss: 0.006276842439547181  hr: 0  min: 4  sec: 14\n",
      "epoch: 9  batch: 5 / 122  loss: 0.016415389068424702  hr: 0  min: 4  sec: 4\n",
      "epoch: 9  batch: 6 / 122  loss: 0.024511130061000586  hr: 0  min: 3  sec: 50\n",
      "epoch: 9  batch: 7 / 122  loss: 0.02138949484963502  hr: 0  min: 3  sec: 39\n",
      "epoch: 9  batch: 8 / 122  loss: 0.02037696837214753  hr: 0  min: 3  sec: 34\n",
      "epoch: 9  batch: 9 / 122  loss: 0.018353286406232253  hr: 0  min: 3  sec: 30\n",
      "epoch: 9  batch: 10 / 122  loss: 0.01683282954618335  hr: 0  min: 3  sec: 24\n",
      "epoch: 9  batch: 11 / 122  loss: 0.015512683767486702  hr: 0  min: 3  sec: 22\n",
      "epoch: 9  batch: 12 / 122  loss: 0.014659100833038488  hr: 0  min: 3  sec: 20\n",
      "epoch: 9  batch: 13 / 122  loss: 0.013691066961305646  hr: 0  min: 3  sec: 18\n",
      "epoch: 9  batch: 14 / 122  loss: 0.022686027372921153  hr: 0  min: 3  sec: 15\n",
      "epoch: 9  batch: 15 / 122  loss: 0.023053022955233853  hr: 0  min: 3  sec: 12\n",
      "epoch: 9  batch: 16 / 122  loss: 0.021957964680041187  hr: 0  min: 3  sec: 8\n",
      "epoch: 9  batch: 17 / 122  loss: 0.0211924775640535  hr: 0  min: 3  sec: 10\n",
      "epoch: 9  batch: 18 / 122  loss: 0.020205670217466023  hr: 0  min: 3  sec: 9\n",
      "epoch: 9  batch: 19 / 122  loss: 0.019324239882591524  hr: 0  min: 3  sec: 6\n",
      "epoch: 9  batch: 20 / 122  loss: 0.019236356345936657  hr: 0  min: 3  sec: 3\n",
      "epoch: 9  batch: 21 / 122  loss: 0.01846940236698304  hr: 0  min: 3  sec: 2\n",
      "epoch: 9  batch: 22 / 122  loss: 0.01779315584677864  hr: 0  min: 3  sec: 0\n",
      "epoch: 9  batch: 23 / 122  loss: 0.017342507960679737  hr: 0  min: 2  sec: 58\n",
      "epoch: 9  batch: 24 / 122  loss: 0.016739930782932788  hr: 0  min: 2  sec: 57\n",
      "epoch: 9  batch: 25 / 122  loss: 0.01615346855483949  hr: 0  min: 2  sec: 56\n",
      "epoch: 9  batch: 26 / 122  loss: 0.015885891168951415  hr: 0  min: 2  sec: 56\n",
      "epoch: 9  batch: 27 / 122  loss: 0.016270753953398928  hr: 0  min: 2  sec: 54\n",
      "epoch: 9  batch: 28 / 122  loss: 0.015781537579771663  hr: 0  min: 2  sec: 52\n",
      "epoch: 9  batch: 29 / 122  loss: 0.0237602728483235  hr: 0  min: 2  sec: 50\n",
      "epoch: 9  batch: 30 / 122  loss: 0.02306292865735789  hr: 0  min: 2  sec: 47\n",
      "epoch: 9  batch: 31 / 122  loss: 0.022973009490317876  hr: 0  min: 2  sec: 44\n",
      "epoch: 9  batch: 32 / 122  loss: 0.022336751615512185  hr: 0  min: 2  sec: 42\n",
      "epoch: 9  batch: 33 / 122  loss: 0.021839660239603483  hr: 0  min: 2  sec: 40\n",
      "epoch: 9  batch: 34 / 122  loss: 0.023141185164122897  hr: 0  min: 2  sec: 38\n",
      "epoch: 9  batch: 35 / 122  loss: 0.022606938066227097  hr: 0  min: 2  sec: 36\n",
      "epoch: 9  batch: 36 / 122  loss: 0.022048050874016352  hr: 0  min: 2  sec: 35\n",
      "epoch: 9  batch: 37 / 122  loss: 0.021619550053130935  hr: 0  min: 2  sec: 32\n",
      "epoch: 9  batch: 38 / 122  loss: 0.021269180346280336  hr: 0  min: 2  sec: 30\n",
      "epoch: 9  batch: 39 / 122  loss: 0.020788272126362875  hr: 0  min: 2  sec: 29\n",
      "epoch: 9  batch: 40 / 122  loss: 0.0203284619434271  hr: 0  min: 2  sec: 27\n",
      "epoch: 9  batch: 41 / 122  loss: 0.030988283824493607  hr: 0  min: 2  sec: 25\n",
      "epoch: 9  batch: 42 / 122  loss: 0.030600829809416263  hr: 0  min: 2  sec: 24\n",
      "epoch: 9  batch: 43 / 122  loss: 0.030003894331626767  hr: 0  min: 2  sec: 22\n",
      "epoch: 9  batch: 44 / 122  loss: 0.032028842500453306  hr: 0  min: 2  sec: 19\n",
      "epoch: 9  batch: 45 / 122  loss: 0.03137907649183439  hr: 0  min: 2  sec: 17\n",
      "epoch: 9  batch: 46 / 122  loss: 0.03532907008902048  hr: 0  min: 2  sec: 15\n",
      "epoch: 9  batch: 47 / 122  loss: 0.03463670894245994  hr: 0  min: 2  sec: 13\n",
      "epoch: 9  batch: 48 / 122  loss: 0.03398328043112997  hr: 0  min: 2  sec: 12\n",
      "epoch: 9  batch: 49 / 122  loss: 0.03333662839948523  hr: 0  min: 2  sec: 10\n",
      "epoch: 9  batch: 50 / 122  loss: 0.03275934706442058  hr: 0  min: 2  sec: 8\n",
      "epoch: 9  batch: 51 / 122  loss: 0.0322011014830102  hr: 0  min: 2  sec: 6\n",
      "epoch: 9  batch: 52 / 122  loss: 0.03172029323804264  hr: 0  min: 2  sec: 4\n",
      "epoch: 9  batch: 53 / 122  loss: 0.031290355542639516  hr: 0  min: 2  sec: 3\n",
      "epoch: 9  batch: 54 / 122  loss: 0.030827618091953574  hr: 0  min: 2  sec: 1\n",
      "epoch: 9  batch: 55 / 122  loss: 0.030316975856707853  hr: 0  min: 2  sec: 0\n",
      "epoch: 9  batch: 56 / 122  loss: 0.029832616934852143  hr: 0  min: 1  sec: 59\n",
      "epoch: 9  batch: 57 / 122  loss: 0.029354812495671866  hr: 0  min: 1  sec: 57\n",
      "epoch: 9  batch: 58 / 122  loss: 0.02890958995879467  hr: 0  min: 1  sec: 55\n",
      "epoch: 9  batch: 59 / 122  loss: 0.028517722136388392  hr: 0  min: 1  sec: 53\n",
      "epoch: 9  batch: 60 / 122  loss: 0.03297792778660854  hr: 0  min: 1  sec: 51\n",
      "epoch: 9  batch: 61 / 122  loss: 0.03254990285781563  hr: 0  min: 1  sec: 49\n",
      "epoch: 9  batch: 62 / 122  loss: 0.03207331877051582  hr: 0  min: 1  sec: 47\n",
      "epoch: 9  batch: 63 / 122  loss: 0.031650838412580984  hr: 0  min: 1  sec: 46\n",
      "epoch: 9  batch: 64 / 122  loss: 0.031195817075058585  hr: 0  min: 1  sec: 44\n",
      "epoch: 9  batch: 65 / 122  loss: 0.030849801960329597  hr: 0  min: 1  sec: 43\n",
      "epoch: 9  batch: 66 / 122  loss: 0.030412711682870533  hr: 0  min: 1  sec: 41\n",
      "epoch: 9  batch: 67 / 122  loss: 0.03004120098454739  hr: 0  min: 1  sec: 39\n",
      "epoch: 9  batch: 68 / 122  loss: 0.029663089327240252  hr: 0  min: 1  sec: 38\n",
      "epoch: 9  batch: 69 / 122  loss: 0.02926581276351235  hr: 0  min: 1  sec: 36\n",
      "epoch: 9  batch: 70 / 122  loss: 0.02892245937338365  hr: 0  min: 1  sec: 34\n",
      "epoch: 9  batch: 71 / 122  loss: 0.03313191287206407  hr: 0  min: 1  sec: 32\n",
      "epoch: 9  batch: 72 / 122  loss: 0.032704596288062424  hr: 0  min: 1  sec: 31\n",
      "epoch: 9  batch: 73 / 122  loss: 0.03232055984450224  hr: 0  min: 1  sec: 28\n",
      "epoch: 9  batch: 74 / 122  loss: 0.03757051237846246  hr: 0  min: 1  sec: 27\n",
      "epoch: 9  batch: 75 / 122  loss: 0.03715661789290607  hr: 0  min: 1  sec: 25\n",
      "epoch: 9  batch: 76 / 122  loss: 0.03672009391256755  hr: 0  min: 1  sec: 24\n",
      "epoch: 9  batch: 77 / 122  loss: 0.03642195044387761  hr: 0  min: 1  sec: 23\n",
      "epoch: 9  batch: 78 / 122  loss: 0.03600209869611531  hr: 0  min: 1  sec: 21\n",
      "epoch: 9  batch: 79 / 122  loss: 0.03600264354196342  hr: 0  min: 1  sec: 19\n",
      "epoch: 9  batch: 80 / 122  loss: 0.03590952976956032  hr: 0  min: 1  sec: 18\n",
      "epoch: 9  batch: 81 / 122  loss: 0.03550356916255421  hr: 0  min: 1  sec: 16\n",
      "epoch: 9  batch: 82 / 122  loss: 0.035112123331622926  hr: 0  min: 1  sec: 14\n",
      "epoch: 9  batch: 83 / 122  loss: 0.03564558692938502  hr: 0  min: 1  sec: 12\n",
      "epoch: 9  batch: 84 / 122  loss: 0.035259984040056314  hr: 0  min: 1  sec: 10\n",
      "epoch: 9  batch: 85 / 122  loss: 0.034880473146982054  hr: 0  min: 1  sec: 8\n",
      "epoch: 9  batch: 86 / 122  loss: 0.03450745131907075  hr: 0  min: 1  sec: 6\n",
      "epoch: 9  batch: 87 / 122  loss: 0.03427721378018801  hr: 0  min: 1  sec: 5\n",
      "epoch: 9  batch: 88 / 122  loss: 0.034240980484438216  hr: 0  min: 1  sec: 3\n",
      "epoch: 9  batch: 89 / 122  loss: 0.034135114966651024  hr: 0  min: 1  sec: 1\n",
      "epoch: 9  batch: 90 / 122  loss: 0.03381566809904244  hr: 0  min: 0  sec: 59\n",
      "epoch: 9  batch: 91 / 122  loss: 0.033473281003023075  hr: 0  min: 0  sec: 57\n",
      "epoch: 9  batch: 92 / 122  loss: 0.03313565598648932  hr: 0  min: 0  sec: 55\n",
      "epoch: 9  batch: 93 / 122  loss: 0.03286598499874354  hr: 0  min: 0  sec: 53\n",
      "epoch: 9  batch: 94 / 122  loss: 0.03257181488740397  hr: 0  min: 0  sec: 52\n",
      "epoch: 9  batch: 95 / 122  loss: 0.03322420482229638  hr: 0  min: 0  sec: 50\n",
      "epoch: 9  batch: 96 / 122  loss: 0.03323160481765323  hr: 0  min: 0  sec: 48\n",
      "epoch: 9  batch: 97 / 122  loss: 0.03536743675742644  hr: 0  min: 0  sec: 46\n",
      "epoch: 9  batch: 98 / 122  loss: 0.036802915564015964  hr: 0  min: 0  sec: 44\n",
      "epoch: 9  batch: 99 / 122  loss: 0.03653594229906572  hr: 0  min: 0  sec: 42\n",
      "epoch: 9  batch: 100 / 122  loss: 0.03620142844505608  hr: 0  min: 0  sec: 40\n",
      "Validation Loss after 100 batches: 1.2080780863761902\n",
      "epoch: 9  batch: 101 / 122  loss: 0.035862540491939626  hr: 0  min: 0  sec: 38\n",
      "epoch: 9  batch: 102 / 122  loss: 0.03552636475411847  hr: 0  min: 0  sec: 37\n",
      "epoch: 9  batch: 103 / 122  loss: 0.0352005705404665  hr: 0  min: 0  sec: 35\n",
      "epoch: 9  batch: 104 / 122  loss: 0.03487608236789059  hr: 0  min: 0  sec: 33\n",
      "epoch: 9  batch: 105 / 122  loss: 0.03456060446776627  hr: 0  min: 0  sec: 31\n",
      "epoch: 9  batch: 106 / 122  loss: 0.034249144705135445  hr: 0  min: 0  sec: 29\n",
      "epoch: 9  batch: 107 / 122  loss: 0.03394513812338255  hr: 0  min: 0  sec: 27\n",
      "epoch: 9  batch: 108 / 122  loss: 0.03364584923514889  hr: 0  min: 0  sec: 25\n",
      "epoch: 9  batch: 109 / 122  loss: 0.033349872520834274  hr: 0  min: 0  sec: 24\n",
      "epoch: 9  batch: 110 / 122  loss: 0.03306226578583433  hr: 0  min: 0  sec: 22\n",
      "epoch: 9  batch: 111 / 122  loss: 0.03277853488301238  hr: 0  min: 0  sec: 20\n",
      "epoch: 9  batch: 112 / 122  loss: 0.03250081514228701  hr: 0  min: 0  sec: 18\n",
      "epoch: 9  batch: 113 / 122  loss: 0.03225356118395034  hr: 0  min: 0  sec: 16\n",
      "epoch: 9  batch: 114 / 122  loss: 0.0321498979751037  hr: 0  min: 0  sec: 14\n",
      "epoch: 9  batch: 115 / 122  loss: 0.03188366218186591  hr: 0  min: 0  sec: 12\n",
      "epoch: 9  batch: 116 / 122  loss: 0.031632524300848355  hr: 0  min: 0  sec: 11\n",
      "epoch: 9  batch: 117 / 122  loss: 0.03137358576437443  hr: 0  min: 0  sec: 9\n",
      "epoch: 9  batch: 118 / 122  loss: 0.031119832093235485  hr: 0  min: 0  sec: 7\n",
      "epoch: 9  batch: 119 / 122  loss: 0.030904430859847416  hr: 0  min: 0  sec: 5\n",
      "epoch: 9  batch: 120 / 122  loss: 0.030662161550329376  hr: 0  min: 0  sec: 3\n",
      "epoch: 9  batch: 121 / 122  loss: 0.030420146644892832  hr: 0  min: 0  sec: 1\n",
      "epoch: 9  batch: 122 / 122  loss: 0.03019027557456866  hr: 0  min: 0  sec: 0\n",
      "Learning rate: 0.0\n",
      "1.1611193150281907\n",
      "CPU times: total: 11min 26s\n",
      "Wall time: 45min 7s\n"
     ]
    }
   ],
   "source": [
    "%time train_model_ABSA(train_loader, validation_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emink\\AppData\\Local\\Temp\\ipykernel_23364\\1419488891.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path), strict=False)\n"
     ]
    }
   ],
   "source": [
    "model_ABSA = load_model(model_ABSA, 'bert_ABSA4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 55s\n",
      "Wall time: 2min 16s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72       196\n",
      "           1       0.53      0.21      0.30       196\n",
      "           2       0.81      0.94      0.87       727\n",
      "\n",
      "    accuracy                           0.77      1119\n",
      "   macro avg       0.68      0.63      0.63      1119\n",
      "weighted avg       0.74      0.77      0.74      1119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time x, y = test_model_ABSA(test_loader)\n",
    "print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATE + ABSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_ABSA(sentence, aspect, tokenizer):\n",
    "    t1 = tokenizer.tokenize(sentence)\n",
    "    t2 = tokenizer.tokenize(aspect)\n",
    "\n",
    "    word_pieces = ['[cls]']\n",
    "    word_pieces += t1\n",
    "    word_pieces += ['[sep]']\n",
    "    word_pieces += t2\n",
    "\n",
    "    segment_tensor = [0] + [0]*len(t1) + [0] + [1]*len(t2)\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "    segment_tensor = torch.tensor(segment_tensor).to(DEVICE)\n",
    "\n",
    "    print(f\"Tokens: {word_pieces}\")\n",
    "    print(f\"Segment Tensor: {segment_tensor}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ABSA(input_tensor, None, None, segments_tensors=segment_tensor)\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "    \n",
    "    return word_pieces, predictions, outputs\n",
    "#######################################################\n",
    "def predict_model_ATE(sentence, tokenizer):\n",
    "    word_pieces = []\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_pieces += tokens\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_tensor = torch.tensor([ids]).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ATE(input_tensor, None, None)\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "    predictions = predictions[0].tolist()\n",
    "\n",
    "    return word_pieces, predictions, outputs\n",
    "#############################################################\n",
    "# def ATE_ABSA(text):\n",
    "#     terms = []\n",
    "#     word = \"\"\n",
    "    \n",
    "#     # ATE\n",
    "#     x, y, _ = predict_model_ATE(text, tokenizer)\n",
    "#     for i in range(len(y)):\n",
    "#         if y[i] == 1:  # Start of a new term\n",
    "#             if len(word) != 0:\n",
    "#                 terms.append(word.replace(\" ##\", \"\"))\n",
    "#             word = x[i]\n",
    "#         elif y[i] == 2:  # Continuation of the term\n",
    "#             if x[i].startswith(\"##\"):\n",
    "#                 word += x[i][2:]  # Remove the '##' prefix\n",
    "#             else:\n",
    "#                 word += \" \" + x[i]\n",
    "#         else:\n",
    "#             if len(word) != 0:\n",
    "#                 terms.append(word.replace(\" ##\", \"\"))\n",
    "#                 word = \"\"\n",
    "#     if len(word) != 0:\n",
    "#         terms.append(word.replace(\" ##\", \"\"))  # Add the last term\n",
    "    \n",
    "#     print(\"tokens:\", x)\n",
    "#     print(\"ATE:\", terms)\n",
    "\n",
    "#     # ABSA\n",
    "#     if len(terms) != 0:\n",
    "#         for term in terms:\n",
    "#             _, sentiment_class, sentiment_logits = predict_model_ABSA(text, term, tokenizer)\n",
    "\n",
    "####################################################################################\n",
    "# def ATE_ABSA(text):\n",
    "#     terms = []\n",
    "#     current_term = \"\"\n",
    "    \n",
    "#     # ATE: Aspect Term Extraction\n",
    "#     tokens, predictions, _ = predict_model_ATE(text, tokenizer)\n",
    "#     for token, tag in zip(tokens, predictions):\n",
    "#         if tag == 1:  # Start of a new term\n",
    "#             if current_term:  # Append the previous term if it exists\n",
    "#                 terms.append(current_term.strip())  # Finalize the previous term\n",
    "#             current_term = token  # Start a new term\n",
    "#         elif tag == 2:  # Continuation of the term\n",
    "#             if current_term:\n",
    "#                 if token.startswith(\"##\"):\n",
    "#                     current_term += token[2:]  # Append subword without '##'\n",
    "#                 else:\n",
    "#                     current_term += \" \" + token\n",
    "#             else:\n",
    "#                 # Handle unexpected cases where continuation appears without a start\n",
    "#                 current_term = token\n",
    "#         else:  # Tag is 0 (outside)\n",
    "#             if current_term:  # Append the previous term if it exists\n",
    "#                 terms.append(current_term.strip())\n",
    "#                 current_term = \"\"\n",
    "#     if current_term:\n",
    "#         terms.append(current_term.strip())  # Add the last term\n",
    "\n",
    "#     # **Handle Incorrect Subword Splits**\n",
    "#     terms = [\"\".join(term.split(\" ##\")) for term in terms]\n",
    "\n",
    "#     print(\"Tokens:\", tokens)\n",
    "#     print(\"Extracted Aspect Terms (ATE):\", terms)\n",
    "\n",
    "#     # ABSA: Aspect-Based Sentiment Analysis\n",
    "#     results = []\n",
    "#     if terms:\n",
    "#         for term in terms:\n",
    "#             _, sentiment_class, sentiment_logits = predict_model_ABSA(text, term, tokenizer)\n",
    "#             results.append({\n",
    "#                 \"aspect_term\": term,\n",
    "#                 \"sentiment_class\": sentiment_class.item(),\n",
    "#                 \"logits\": sentiment_logits.tolist()\n",
    "#             })\n",
    "    \n",
    "#     # Display Results\n",
    "#     print(\"\\nAspect Terms and Sentiments (ABSA):\")\n",
    "#     for res in results:\n",
    "#         print(f\"Aspect: {res['aspect_term']}, Sentiment Class: {res['sentiment_class']}, Logits: {res['logits']}\")\n",
    "\n",
    "#     return results\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NEW PREDICTON CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "##############################################\n",
    "# 1) PREDICT_MODEL_ABSA (no attention mask)\n",
    "##############################################\n",
    "def predict_model_ABSA(sentence, aspect, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict sentiment class for a given sentence and an extracted aspect term.\n",
    "    We do NOT add an attention mask, per your request.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Tokenize\n",
    "    sentence_tokens = tokenizer.tokenize(sentence)\n",
    "    aspect_tokens   = tokenizer.tokenize(aspect)\n",
    "\n",
    "    # 2) Construct input with two [SEP] tokens:\n",
    "    #    [CLS] + sentence_tokens + [SEP] + aspect_tokens + [SEP]\n",
    "    word_pieces = (\n",
    "        ['[CLS]'] \n",
    "        + sentence_tokens \n",
    "        + ['[SEP]'] \n",
    "        + aspect_tokens \n",
    "        + ['[SEP]']\n",
    "    )\n",
    "    # Segment IDs:\n",
    "    #   - 0 for everything up to the first [SEP]\n",
    "    #   - 1 for aspect tokens + the second [SEP]\n",
    "    segment_ids = (\n",
    "        [0]*(1 + len(sentence_tokens) + 1)\n",
    "        + [1]*(len(aspect_tokens) + 1)\n",
    "    )\n",
    "\n",
    "    # 3) Convert to tensors\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "    input_ids_tensor   = torch.tensor([input_ids]).to(DEVICE)\n",
    "    segment_ids_tensor = torch.tensor([segment_ids]).to(DEVICE)\n",
    "\n",
    "    # Debug prints (optional)\n",
    "    print(\"[ABSA Debug] word_pieces:\", word_pieces)\n",
    "    print(\"[ABSA Debug] segment_ids:\", segment_ids)\n",
    "\n",
    "    # 4) Forward pass (no attention mask)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ABSA(\n",
    "            ids_tensors      = input_ids_tensor,\n",
    "            lable_tensors    = None,\n",
    "            masks_tensors    = None,       # Not adding attention mask\n",
    "            segments_tensors = segment_ids_tensor\n",
    "        )\n",
    "        # outputs shape: [batch_size, 3]\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "\n",
    "    # Print raw logits for debugging\n",
    "    print(\"[ABSA Debug] logits:\", outputs.cpu().numpy())\n",
    "\n",
    "    return word_pieces, predictions, outputs\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 2) PREDICT_MODEL_ATE (no attention mask)\n",
    "#############################################\n",
    "def predict_model_ATE(sentence, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict aspect tags (0,1,2) for each token in the given sentence.\n",
    "    We do NOT add an attention mask, per your request.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Tokenize the sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "    # 2) Convert tokens to IDs\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids_tensor = torch.tensor([input_ids]).to(DEVICE)\n",
    "\n",
    "    # 3) Forward pass (no attention mask)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ATE(\n",
    "            ids_tensors   = input_ids_tensor,\n",
    "            tags_tensors  = None,  # no labels at inference\n",
    "            masks_tensors = None   # skipping attention mask\n",
    "        )\n",
    "        # outputs shape: [batch_size, seq_len, 3]\n",
    "        # we do a max across dim=2 to get predicted tag\n",
    "        _, predictions = torch.max(outputs, dim=2)\n",
    "\n",
    "    # predictions is [1, seq_len], so get the first row\n",
    "    predictions = predictions[0].cpu().tolist()\n",
    "\n",
    "    return tokens, predictions, outputs\n",
    "\n",
    "\n",
    "###########################################\n",
    "# 3) ATE_ABSA: Extract aspects, then ABSA\n",
    "###########################################\n",
    "def ATE_ABSA(text):\n",
    "    \"\"\"\n",
    "    1) Extract aspect terms (ATE) from the text using predict_model_ATE.\n",
    "    2) For each extracted aspect, call predict_model_ABSA for sentiment on the\n",
    "       entire sentence plus that single aspect.\n",
    "    3) Print final results in a user-friendly format:\n",
    "       SENTENCE: ...\n",
    "       - Aspect 'foo': Sentiment = ...\n",
    "       - Aspect 'bar': Sentiment = ...\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure both models are in eval mode for inference\n",
    "    model_ATE.eval()\n",
    "    model_ABSA.eval()\n",
    "\n",
    "    # -----------------------------\n",
    "    # A) Aspect Term Extraction\n",
    "    # -----------------------------\n",
    "    aspect_terms = []\n",
    "    current_term = \"\"\n",
    "\n",
    "    # 1) Predict tokens & tags from ATE\n",
    "    tokens, predicted_tags, _ = predict_model_ATE(text, tokenizer)\n",
    "\n",
    "    # 2) Merge subwords into aspect terms\n",
    "    for token, tag in zip(tokens, predicted_tags):\n",
    "        if tag == 1:  # B-term\n",
    "            if current_term:\n",
    "                aspect_terms.append(current_term.strip())\n",
    "            current_term = token\n",
    "        elif tag == 2:  # I-term\n",
    "            if current_term:\n",
    "                if token.startswith(\"##\"):\n",
    "                    current_term += token[2:]\n",
    "                else:\n",
    "                    current_term += \" \" + token\n",
    "            else:\n",
    "                # If we get '2' without a preceding '1'\n",
    "                current_term = token\n",
    "        else:  # tag == 0 => outside\n",
    "            if current_term:\n",
    "                aspect_terms.append(current_term.strip())\n",
    "                current_term = \"\"\n",
    "\n",
    "    # If leftover aspect term\n",
    "    if current_term:\n",
    "        aspect_terms.append(current_term.strip())\n",
    "\n",
    "    # 3) Clean leftover '##'\n",
    "    aspect_terms = [term.replace(\" ##\", \"\") for term in aspect_terms]\n",
    "\n",
    "    # Debug: Print ATE results\n",
    "    print(\"[ATE Debug] tokens:\", tokens)\n",
    "    print(\"[ATE Debug] predicted_tags:\", predicted_tags)\n",
    "    print(\"[ATE Debug] aspect_terms:\", aspect_terms)\n",
    "\n",
    "    # -----------------------------\n",
    "    # B) Aspect-Based Sentiment\n",
    "    # -----------------------------\n",
    "    results = []\n",
    "\n",
    "    # For each aspect, we do ABSA with full sentence + single aspect\n",
    "    for term in aspect_terms:\n",
    "        word_pieces, pred_class, logits = predict_model_ABSA(text, term, tokenizer)\n",
    "\n",
    "        # Convert logits to Python list for logging\n",
    "        logits_list = logits.squeeze().cpu().numpy().tolist()\n",
    "        \n",
    "        results.append({\n",
    "            \"aspect_term\": term,\n",
    "            \"sentiment_class\": pred_class.item(),  # e.g., 0=neg, 1=neu, 2=pos\n",
    "            \"logits\": logits_list\n",
    "        })\n",
    "\n",
    "    # -----------------------------\n",
    "    # Print final results\n",
    "    # -----------------------------\n",
    "    print(f\"\\nSENTENCE: {text}\")\n",
    "    for r in results:\n",
    "        aspect   = r[\"aspect_term\"]\n",
    "        polarity = r[\"sentiment_class\"]\n",
    "        print(f\" - Aspect '{aspect}': Sentiment = {polarity}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emink\\AppData\\Local\\Temp\\ipykernel_23364\\1419488891.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path), strict=False)\n"
     ]
    }
   ],
   "source": [
    "model_ABSA = load_model(model_ABSA, 'bert_ABSA4.pkl')\n",
    "model_ATE = load_model(model_ATE, 'bert_ATE.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code to randomly slect and test sentences from testing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def test_random_sentences(\n",
    "    csv_path=\"testing.csv\", \n",
    "    num_samples=5\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Reads 'testing.csv' which has:\n",
    "         Tokens (full sentence), \n",
    "         Tags (aspect), \n",
    "         Polarities (gold sentiment).\n",
    "    2) Groups by 'Tokens' so each unique sentence is one group.\n",
    "    3) Randomly picks 'num_samples' distinct sentences from that CSV.\n",
    "    4) For each sentence, calls your ATE_ABSA pipeline.\n",
    "    5) Prints a side-by-side comparison of:\n",
    "         - Aspects & Sentiment (model vs. CSV).\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # A) LOAD & GROUP THE CSV\n",
    "    # -------------------------------\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Group by 'Tokens' => each group is effectively a distinct full sentence\n",
    "    grouped = df.groupby(\"Tokens\")\n",
    "\n",
    "    # Extract all distinct sentences from the CSV\n",
    "    all_sentences = list(grouped.groups.keys())\n",
    "\n",
    "    # If 'num_samples' is bigger than total distinct sentences, clamp it\n",
    "    num_samples = min(num_samples, len(all_sentences))\n",
    "\n",
    "    # Randomly pick 'num_samples' distinct sentences\n",
    "    chosen_sentences = random.sample(all_sentences, k=num_samples)\n",
    "\n",
    "    # -------------------------------\n",
    "    # B) FOR EACH CHOSEN SENTENCE\n",
    "    # -------------------------------\n",
    "    for sentence in chosen_sentences:\n",
    "        # 1) Lookup all aspects from the CSV for this sentence\n",
    "        sub_df = grouped.get_group(sentence)\n",
    "        # e.g. might have:\n",
    "        #   \"But the staff was so horrible to us.\"  staff     negative\n",
    "        #   \"But the staff was so horrible to us.\"  food      positive\n",
    "        # etc.\n",
    "\n",
    "        # Convert them to a list of (aspect, gold_polarity)\n",
    "        gold_aspects = []\n",
    "        for _, row in sub_df.iterrows():\n",
    "            gold_aspects.append((row[\"Tags\"], row[\"Polarities\"]))\n",
    "\n",
    "        # 2) Call your ATE_ABSA pipeline => model aspects & sentiment\n",
    "        #    This function is what you already wrote or the improved version.\n",
    "        #    Must be defined in your code. For example:\n",
    "        model_results = ATE_ABSA(sentence)\n",
    "        # model_results is a list of dicts like:\n",
    "        # [\n",
    "        #   { \"aspect_term\": \"staff\", \"sentiment_class\": 0_or_1_or_2, \"logits\": [...] },\n",
    "        #   ...\n",
    "        # ]\n",
    "\n",
    "        # Convert the model's numeric polarity to text to compare easily\n",
    "        # For example, you might do:\n",
    "        def polarity_to_text(idx):\n",
    "            # Example mapping (adapt if your model uses different labels)\n",
    "            # 0=negative, 1=neutral, 2=positive\n",
    "            if idx == 0:\n",
    "                return \"negative\"\n",
    "            elif idx == 1:\n",
    "                return \"neutral\"\n",
    "            elif idx == 2:\n",
    "                return \"positive\"\n",
    "            return \"???\"\n",
    "\n",
    "        # Build a dict from aspect -> predicted_polarity\n",
    "        # (If model finds multiple aspects, might have duplicates)\n",
    "        model_aspects_dict = {}\n",
    "        for r in model_results:\n",
    "            aspect_term = r[\"aspect_term\"]\n",
    "            polarity_idx = r[\"sentiment_class\"]\n",
    "            polarity_text = polarity_to_text(polarity_idx)\n",
    "            model_aspects_dict[aspect_term] = polarity_text\n",
    "\n",
    "        # 3) Print a side-by-side summary\n",
    "        print(\"==========================================================\")\n",
    "        print(f\"SENTENCE: {sentence}\")\n",
    "        print(\"\\n[CSV file says aspects:]\")\n",
    "        for aspect, gold_pol in gold_aspects:\n",
    "            print(f\"  - Aspect '{aspect}' => Gold Polarity: {gold_pol}\")\n",
    "\n",
    "        print(\"\\n[MODEL found aspects:]\")\n",
    "        if len(model_results) == 0:\n",
    "            print(\"  - (No aspects found)\")\n",
    "        else:\n",
    "            for r in model_results:\n",
    "                # e.g. aspect='staff', pol=2 => 'positive'\n",
    "                aspect_term = r[\"aspect_term\"]\n",
    "                predicted_pol = polarity_to_text(r[\"sentiment_class\"])\n",
    "                print(f\"  - Aspect '{aspect_term}' => Predicted Polarity: {predicted_pol}\")\n",
    "\n",
    "        print(\"==========================================================\\n\")\n",
    "\n",
    "    # Done. We just tested 'num_samples' random sentences from 'testing.csv'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATE Debug] tokens: ['it', 'took', 'them', '15', 'minutes', 'to', 'put', 'water', 'in', 'our', 'glasses', '.']\n",
      "[ATE Debug] predicted_tags: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[ATE Debug] aspect_terms: ['water']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'it', 'took', 'them', '15', 'minutes', 'to', 'put', 'water', 'in', 'our', 'glasses', '.', '[SEP]', 'water', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0276011   0.20085283  1.0429422 ]]\n",
      "\n",
      "SENTENCE: It took them 15 minutes to put water in our glasses.\n",
      " - Aspect 'water': Sentiment = 2\n",
      "==========================================================\n",
      "SENTENCE: It took them 15 minutes to put water in our glasses.\n",
      "\n",
      "[CSV file says aspects:]\n",
      "  - Aspect 'water' => Gold Polarity: neutral\n",
      "\n",
      "[MODEL found aspects:]\n",
      "  - Aspect 'water' => Predicted Polarity: positive\n",
      "==========================================================\n",
      "\n",
      "[ATE Debug] tokens: ['as', 'soon', 'as', 'my', 'father', 'lifted', 'his', 'pen', 'from', 'the', 'check', 'a', 'chef', 'appeared', 'to', 'usher', 'us', 'out', '.']\n",
      "[ATE Debug] predicted_tags: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[ATE Debug] aspect_terms: ['check', 'chef']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'as', 'soon', 'as', 'my', 'father', 'lifted', 'his', 'pen', 'from', 'the', 'check', 'a', 'chef', 'appeared', 'to', 'usher', 'us', 'out', '.', '[SEP]', 'check', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[ 1.4534886   0.05203858 -1.5574646 ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'as', 'soon', 'as', 'my', 'father', 'lifted', 'his', 'pen', 'from', 'the', 'check', 'a', 'chef', 'appeared', 'to', 'usher', 'us', 'out', '.', '[SEP]', 'chef', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[ 1.4846984   0.01075368 -1.5396259 ]]\n",
      "\n",
      "SENTENCE: As soon as my father lifted his pen from the check a chef appeared to usher us out.\n",
      " - Aspect 'check': Sentiment = 0\n",
      " - Aspect 'chef': Sentiment = 0\n",
      "==========================================================\n",
      "SENTENCE: As soon as my father lifted his pen from the check a chef appeared to usher us out.\n",
      "\n",
      "[CSV file says aspects:]\n",
      "  - Aspect 'chef' => Gold Polarity: negative\n",
      "  - Aspect 'check' => Gold Polarity: neutral\n",
      "\n",
      "[MODEL found aspects:]\n",
      "  - Aspect 'check' => Predicted Polarity: negative\n",
      "  - Aspect 'chef' => Predicted Polarity: negative\n",
      "==========================================================\n",
      "\n",
      "[ATE Debug] tokens: ['my', 'fiance', 'took', 'me', 'to', 'sc', '##opa', 'last', 'week', 'for', 'my', 'birthday', 'and', 'i', 'couldn', \"'\", 't', 'believe', 'the', 'food', '.']\n",
      "[ATE Debug] predicted_tags: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[ATE Debug] aspect_terms: ['food']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'my', 'fiance', 'took', 'me', 'to', 'sc', '##opa', 'last', 'week', 'for', 'my', 'birthday', 'and', 'i', 'couldn', \"'\", 't', 'believe', 'the', 'food', '.', '[SEP]', 'food', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-0.9697646   0.09318165  1.0270085 ]]\n",
      "\n",
      "SENTENCE: My fiance took me to Scopa last week for my birthday and I couldn't believe the food.\n",
      " - Aspect 'food': Sentiment = 2\n",
      "==========================================================\n",
      "SENTENCE: My fiance took me to Scopa last week for my birthday and I couldn't believe the food.\n",
      "\n",
      "[CSV file says aspects:]\n",
      "  - Aspect 'food' => Gold Polarity: positive\n",
      "\n",
      "[MODEL found aspects:]\n",
      "  - Aspect 'food' => Predicted Polarity: positive\n",
      "==========================================================\n",
      "\n",
      "[ATE Debug] tokens: ['bal', '##uchi', \"'\", 's', 'has', 'solid', 'food', 'and', 'a', 'nice', 'decor', 'at', 'reasonable', 'prices', '.']\n",
      "[ATE Debug] predicted_tags: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "[ATE Debug] aspect_terms: ['food', 'decor', 'prices']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'bal', '##uchi', \"'\", 's', 'has', 'solid', 'food', 'and', 'a', 'nice', 'decor', 'at', 'reasonable', 'prices', '.', '[SEP]', 'food', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0717934   0.19913511  1.0943848 ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'bal', '##uchi', \"'\", 's', 'has', 'solid', 'food', 'and', 'a', 'nice', 'decor', 'at', 'reasonable', 'prices', '.', '[SEP]', 'decor', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0717802   0.19909026  1.0944092 ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'bal', '##uchi', \"'\", 's', 'has', 'solid', 'food', 'and', 'a', 'nice', 'decor', 'at', 'reasonable', 'prices', '.', '[SEP]', 'prices', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0717432   0.19916448  1.0943434 ]]\n",
      "\n",
      "SENTENCE: Baluchi's has solid food and a nice decor at reasonable prices.\n",
      " - Aspect 'food': Sentiment = 2\n",
      " - Aspect 'decor': Sentiment = 2\n",
      " - Aspect 'prices': Sentiment = 2\n",
      "==========================================================\n",
      "SENTENCE: Baluchi's has solid food and a nice decor at reasonable prices.\n",
      "\n",
      "[CSV file says aspects:]\n",
      "  - Aspect 'food' => Gold Polarity: positive\n",
      "  - Aspect 'decor' => Gold Polarity: positive\n",
      "  - Aspect 'prices' => Gold Polarity: positive\n",
      "\n",
      "[MODEL found aspects:]\n",
      "  - Aspect 'food' => Predicted Polarity: positive\n",
      "  - Aspect 'decor' => Predicted Polarity: positive\n",
      "  - Aspect 'prices' => Predicted Polarity: positive\n",
      "==========================================================\n",
      "\n",
      "[ATE Debug] tokens: ['the', 'pizza', 'is', 'the', 'best', 'if', 'you', 'like', 'thin', 'crust', '##ed', 'pizza', '.']\n",
      "[ATE Debug] predicted_tags: [0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0]\n",
      "[ATE Debug] aspect_terms: ['pizza', 'thin crusted pizza']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'pizza', 'is', 'the', 'best', 'if', 'you', 'like', 'thin', 'crust', '##ed', 'pizza', '.', '[SEP]', 'pizza', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0724463   0.19831562  1.0950933 ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'pizza', 'is', 'the', 'best', 'if', 'you', 'like', 'thin', 'crust', '##ed', 'pizza', '.', '[SEP]', 'thin', 'crust', '##ed', 'pizza', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0718948   0.19873507  1.0943865 ]]\n",
      "\n",
      "SENTENCE: The pizza is the best if you like thin crusted pizza.\n",
      " - Aspect 'pizza': Sentiment = 2\n",
      " - Aspect 'thin crusted pizza': Sentiment = 2\n",
      "==========================================================\n",
      "SENTENCE: The pizza is the best if you like thin crusted pizza.\n",
      "\n",
      "[CSV file says aspects:]\n",
      "  - Aspect 'pizza' => Gold Polarity: positive\n",
      "  - Aspect 'thin crusted pizza' => Gold Polarity: neutral\n",
      "\n",
      "[MODEL found aspects:]\n",
      "  - Aspect 'pizza' => Predicted Polarity: positive\n",
      "  - Aspect 'thin crusted pizza' => Predicted Polarity: positive\n",
      "==========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_random_sentences(csv_path=\"testing.csv\", num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATE Debug] tokens: ['the', 'service', 'was', 'discus', '##ting', '.']\n",
      "[ATE Debug] predicted_tags: [0, 1, 0, 0, 0, 0]\n",
      "[ATE Debug] aspect_terms: ['service']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'service', 'was', 'discus', '##ting', '.', '[SEP]', 'service', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[ 0.35645038  0.8861331  -1.1608526 ]]\n",
      "\n",
      "SENTENCE: the service was discusting.\n",
      " - Aspect 'service': Sentiment = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'aspect_term': 'service',\n",
       "  'sentiment_class': 1,\n",
       "  'logits': [0.3564503788948059, 0.886133074760437, -1.1608525514602661]}]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"the service was discusting.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATE Debug] tokens: ['staff', 'was', 'ex', '##elle', '##nt', '.']\n",
      "[ATE Debug] predicted_tags: [1, 0, 0, 0, 0, 0]\n",
      "[ATE Debug] aspect_terms: ['staff']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'staff', 'was', 'ex', '##elle', '##nt', '.', '[SEP]', 'staff', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0926075   0.17564873  1.1215113 ]]\n",
      "\n",
      "SENTENCE: Staff was exellent.\n",
      " - Aspect 'staff': Sentiment = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'aspect_term': 'staff',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.0926074981689453, 0.1756487339735031, 1.1215113401412964]}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Staff was exellent.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATE Debug] tokens: ['the', 'pasta', 'was', 'delicious', ',', 'but', 'the', 'service', 'was', 'slow', '.']\n",
      "[ATE Debug] predicted_tags: [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[ATE Debug] aspect_terms: ['pasta', 'service']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'pasta', 'was', 'delicious', ',', 'but', 'the', 'service', 'was', 'slow', '.', '[SEP]', 'pasta', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0911393  0.1442051  1.1300265]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'pasta', 'was', 'delicious', ',', 'but', 'the', 'service', 'was', 'slow', '.', '[SEP]', 'service', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.1000597   0.14454973  1.1359445 ]]\n",
      "\n",
      "SENTENCE: The pasta was delicious, but the service was slow.\n",
      " - Aspect 'pasta': Sentiment = 2\n",
      " - Aspect 'service': Sentiment = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'aspect_term': 'pasta',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.091139316558838, 0.14420509338378906, 1.1300264596939087]},\n",
       " {'aspect_term': 'service',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.1000597476959229, 0.14454972743988037, 1.1359444856643677]}]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The pasta was delicious, but the service was slow.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATE Debug] tokens: ['the', 'am', '##bian', '##ce', 'was', 'fantastic', ',', 'but', 'the', 'food', 'was', 'over', '##pr', '##ice', '##d', '.']\n",
      "[ATE Debug] predicted_tags: [0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[ATE Debug] aspect_terms: ['am', '##bian', '##ce', 'food']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'am', '##bian', '##ce', 'was', 'fantastic', ',', 'but', 'the', 'food', 'was', 'over', '##pr', '##ice', '##d', '.', '[SEP]', 'am', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0807803   0.19206576  1.1046661 ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'am', '##bian', '##ce', 'was', 'fantastic', ',', 'but', 'the', 'food', 'was', 'over', '##pr', '##ice', '##d', '.', '[SEP]', '#', '#', 'bi', '##an', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0797855   0.18983082  1.1042734 ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'am', '##bian', '##ce', 'was', 'fantastic', ',', 'but', 'the', 'food', 'was', 'over', '##pr', '##ice', '##d', '.', '[SEP]', '#', '#', 'ce', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.078863    0.18773733  1.104349  ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'am', '##bian', '##ce', 'was', 'fantastic', ',', 'but', 'the', 'food', 'was', 'over', '##pr', '##ice', '##d', '.', '[SEP]', 'food', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0792627   0.19420789  1.1026584 ]]\n",
      "\n",
      "SENTENCE: The ambiance was fantastic, but the food was overpriced.\n",
      " - Aspect 'am': Sentiment = 2\n",
      " - Aspect '##bian': Sentiment = 2\n",
      " - Aspect '##ce': Sentiment = 2\n",
      " - Aspect 'food': Sentiment = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'aspect_term': 'am',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.080780267715454, 0.1920657604932785, 1.1046661138534546]},\n",
       " {'aspect_term': '##bian',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.0797854661941528, 0.18983082473278046, 1.1042734384536743]},\n",
       " {'aspect_term': '##ce',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.0788630247116089, 0.1877373307943344, 1.1043490171432495]},\n",
       " {'aspect_term': 'food',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.0792627334594727, 0.19420789182186127, 1.1026583909988403]}]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The ambiance was fantastic, but the food was overpriced.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATE Debug] tokens: ['the', 'am', '##bian', '##ce', 'was', 'terrible', ',', 'but', 'the', 'food', 'was', 'ex', '##elle', '##nt', '.']\n",
      "[ATE Debug] predicted_tags: [0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[ATE Debug] aspect_terms: ['am', '##bian', '##ce', 'food']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'am', '##bian', '##ce', 'was', 'terrible', ',', 'but', 'the', 'food', 'was', 'ex', '##elle', '##nt', '.', '[SEP]', 'am', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[ 1.9913644 -0.649262  -1.0417598]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'am', '##bian', '##ce', 'was', 'terrible', ',', 'but', 'the', 'food', 'was', 'ex', '##elle', '##nt', '.', '[SEP]', '#', '#', 'bi', '##an', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[ABSA Debug] logits: [[ 1.9827173  -0.63632214 -1.0703382 ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'am', '##bian', '##ce', 'was', 'terrible', ',', 'but', 'the', 'food', 'was', 'ex', '##elle', '##nt', '.', '[SEP]', '#', '#', 'ce', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "[ABSA Debug] logits: [[ 1.985781  -0.6485777 -1.0585136]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'am', '##bian', '##ce', 'was', 'terrible', ',', 'but', 'the', 'food', 'was', 'ex', '##elle', '##nt', '.', '[SEP]', 'food', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[ 1.9983842 -0.6649143 -1.0344505]]\n",
      "\n",
      "SENTENCE: The ambiance was terrible, but the food was exellent.\n",
      " - Aspect 'am': Sentiment = 0\n",
      " - Aspect '##bian': Sentiment = 0\n",
      " - Aspect '##ce': Sentiment = 0\n",
      " - Aspect 'food': Sentiment = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'aspect_term': 'am',\n",
       "  'sentiment_class': 0,\n",
       "  'logits': [1.9913643598556519, -0.649262011051178, -1.0417598485946655]},\n",
       " {'aspect_term': '##bian',\n",
       "  'sentiment_class': 0,\n",
       "  'logits': [1.9827172756195068, -0.6363221406936646, -1.070338249206543]},\n",
       " {'aspect_term': '##ce',\n",
       "  'sentiment_class': 0,\n",
       "  'logits': [1.985780954360962, -0.6485776901245117, -1.0585136413574219]},\n",
       " {'aspect_term': 'food',\n",
       "  'sentiment_class': 0,\n",
       "  'logits': [1.9983842372894287, -0.6649143099784851, -1.0344505310058594]}]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The ambiance was terrible, but the food was exellent.\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATE Debug] tokens: ['the', 'waiter', 'was', 'very', 'friendly', ',', 'and', 'the', 'dessert', '##s', 'were', 'outstanding']\n",
      "[ATE Debug] predicted_tags: [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "[ATE Debug] aspect_terms: ['waiter', 'dessert', '##s']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'waiter', 'was', 'very', 'friendly', ',', 'and', 'the', 'dessert', '##s', 'were', 'outstanding', '[SEP]', 'waiter', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0720174   0.20034967  1.0933613 ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'waiter', 'was', 'very', 'friendly', ',', 'and', 'the', 'dessert', '##s', 'were', 'outstanding', '[SEP]', 'dessert', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0720706   0.20036568  1.0933958 ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'waiter', 'was', 'very', 'friendly', ',', 'and', 'the', 'dessert', '##s', 'were', 'outstanding', '[SEP]', '#', '#', 's', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0723248   0.20028359  1.0936135 ]]\n",
      "\n",
      "SENTENCE: The waiter was very friendly, and the desserts were outstanding\n",
      " - Aspect 'waiter': Sentiment = 2\n",
      " - Aspect 'dessert': Sentiment = 2\n",
      " - Aspect '##s': Sentiment = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'aspect_term': 'waiter',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.0720174312591553, 0.20034967362880707, 1.093361258506775]},\n",
       " {'aspect_term': 'dessert',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.072070598602295, 0.20036567747592926, 1.0933958292007446]},\n",
       " {'aspect_term': '##s',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.0723247528076172, 0.20028358697891235, 1.0936135053634644]}]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"The waiter was very friendly, and the desserts were outstanding\"\n",
    "ATE_ABSA(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATE Debug] tokens: ['the', 'waiter', 'was', 'very', 'friendly', ',', 'and', 'the', 'dessert', 'was', 'discus', '##ting']\n",
      "[ATE Debug] predicted_tags: [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[ATE Debug] aspect_terms: ['waiter', 'dessert']\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'waiter', 'was', 'very', 'friendly', ',', 'and', 'the', 'dessert', 'was', 'discus', '##ting', '[SEP]', 'waiter', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0736802  0.1992836  1.095133 ]]\n",
      "[ABSA Debug] word_pieces: ['[CLS]', 'the', 'waiter', 'was', 'very', 'friendly', ',', 'and', 'the', 'dessert', 'was', 'discus', '##ting', '[SEP]', 'dessert', '[SEP]']\n",
      "[ABSA Debug] segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[ABSA Debug] logits: [[-1.0732858   0.19933473  1.0947171 ]]\n",
      "\n",
      "SENTENCE: The waiter was very friendly, and the dessert was discusting\n",
      " - Aspect 'waiter': Sentiment = 2\n",
      " - Aspect 'dessert': Sentiment = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'aspect_term': 'waiter',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.0736801624298096, 0.19928359985351562, 1.0951329469680786]},\n",
       " {'aspect_term': 'dessert',\n",
       "  'sentiment_class': 2,\n",
       "  'logits': [-1.0732858180999756, 0.19933472573757172, 1.0947171449661255]}]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"The waiter was very friendly, and the dessert was discusting\"\n",
    "ATE_ABSA(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
